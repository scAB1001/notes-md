### Outline
- Read XR Documentation
- Setup Unity software *base*
- Unity XR SDK
- Handtracking then Eye tracking
- Schedule lab times (contingent on machine availability and lab access)
## Varjo Developer Guide
https://developer.varjo.com/docs/get-started/get-started

You can develop for Varjo headsets with the familiar 3D tools you use today to create immersive experiences. These tools can easily be adapted to work with Varjo headsets and human-eye resolution.
### Human Eye Resolution
https://developer.varjo.com/docs/get-started/human-eye-resolution
### Foveated Rendering
Foveated rendering uses eye tracking to render the image in full resolution only in those areas where the user is currently looking. 
- Similar to rendering for other HMDs. 
- The main difference is that applications must submit **four** views instead of two. 
	- Two displays for viewports per eye: a *human-eye-resolution focus* view and a *high-resolution peripheral* view.
- Demonstrates significantly improved performance and frame rate with minimal to no perceived loss in quality. 
- Mimics how the eyes functiom, digesting the greatest detail around the center of our gaze. 
- Foveated rendering is available for all Varjo headsets.
![](https://developer.varjo.com/assets/uploads/Foveation.png)
When using Varjo XR plugins for Unity or Unreal, these features are enabled by default.
To learn more about how the image is rendered to the Varjo HMD, see [Rendering to Varjo headsets](https://developer.varjo.com/docs/native/rendering-to-varjo-headsets).

Varjo XR-4 has no need for two displays per eye as the displays are high enough resolution across the full view.
## Video Pass-through (VPT)
By using VPT instead of optical see-through (OST), Varjo’s [mixed reality device](https://varjo.com/products) completely and convincingly merges real and virtual, **making it the only device to achieve photorealism in mixed reality.**

Enables functionality that OST systems like *HoloLens* or *Magic Leap*.

**Virtual objects can be** **black or opaque and appear as solid as anything in the real world**. Colors are perfectly rendered and appear just as they should. You can also add, omit and adjust colors, shadows and light in the virtual world and the real world.

Furthermore, real-time camera data is used to calculate global illumination models that allow shadows and light to naturally interact with virtual objects in real time – just as they would with real-world objects. **Virtual objects can even illuminate, cast shadows over, or reflect the world around them**. They can also appear as transparent and refract the light from the real world behind them.

Importantly, **Varjo also allows you to see your own body as you interact with virtual objects**, the real world around you, and even your colleagues as you collaborate on mixed reality projects. 
### How does the technology work?
VPT uses cameras to digitize the world in real time. That data is then combined with virtual content inside our GPU and shown to users through our device – completely blurring the lines between real and virtual.

It follows the same general logic as our [Bionic Display™](https://varjo.com/blog/introducing-bionic-display-how-varjo-delivers-human-eye-resolution/). Visual quality and bandwidth are optimized by sampling video streams from each camera with different resolutions and using [eye tracking](https://varjo.com/blog/industrial-strength-eye-tracking-in-varjo) to determine which area of the sensor should be sampled in the highest resolution. So the highest resolution will always be wherever the user is looking at any given time.

Just as the human eye does, Varjo then samples the surrounding areas in lower resolution to make sure the transfer speed (below 10Gb/s) can be handled by modern computers. To put this into context, the human eye sends peripheral images to the brain at just 10 Mb/s.

All this means that **Varjo’s mixed reality runs at around 35 times the resolution of HTC Vive Pro mixed reality**, for example, and each individual pixel is of a much higher quality.
### How VPT mixed reality revolutionizes work in a range of fields
Professionals in engineering, [design](https://varjo.com/use-cases/design-and-visualization), [simulation](https://varjo.com/use-cases/training-and-simulation), and [research](https://varjo.com/use-cases/research) can use Varjo’s mixed reality to **develop and interact with photorealistic 3D models while collaborating with others in real life and real time**. 

We collaborated with [Volvo Cars](https://varjo.com/case-studies/xr-test-drive-with-volvo/) to create a truly ground-breaking demo of **a real car being safely driven while wearing a Varjo headset** and interacting with totally convincing virtual elements – including a virtual moose that the driver needs to avoid. Today, Volvo is using Varjo headsets to design and develop the world’s safest cars. With Varjo’s photorealistic mixed reality and eye tracking, Volvo is now able to do design and safety studies on cars that haven’t even been built yet.

We have also collaborated with [Bohemia Interactive Simulations](https://bisimulations.com/) and [many other simulation companies](https://varjo.com/blog/5-pioneering-virtual-and-mixed-reality-training-solutions/) to create **a hybrid cockpit simulator** where the world outside the cockpit is illustrated with VR and the cockpit is real and enhanced with VPT.
## Eye tracking
Varjo headsets feature the 20/20 Eye Tracker, our integrated eye tracking functionality. You can use eye tracking in your application and log gaze information for analytics. Eye tracking can also be used to interact with content; you can use it to select an object or prompt for additional information simply by looking at it.

For detailed instructions, refer to your engine’s documentation:
- [Native eye tracking documentation](https://developer.varjo.com/docs/native/eye-tracking)
- [Example of using eye tracking with Unity XR SDK](https://developer.varjo.com/docs/unity-xr-sdk/eye-tracking-with-varjo-xr-plugin)
- [Example of using eye tracking with Unreal](https://developer.varjo.com/docs/unreal/ue5/unreal5-examples)

### WHAT IS GAZE?

Gaze starts from the **gaze origin** (the eye) and follows the **gaze vector** (the direction of the gaze). A normalized gaze vector is calculated. It is important to understand this concept in order to process eye tracking data while developing for Varjo headsets.

You can also record gaze data together with a video feed from the headset. For instructions, see [Gaze data logging page](https://developer.varjo.com/docs/get-started/gaze-data-collection).

### EYE TRACKER SPECIFICATIONS

Varjo headsets contain two eye tracking cameras, one for each eye. A combined gaze ray can be computed either using information from two eyes (both eyes tracked; the typical situation) or from one eye (e.g., if the other eye cannot be tracked). With the recommended computer configurations, you can expect a latency of about 20–30ms from pupil change to the eye tracking result being available.

### Varjo XR-3, VR-3 and Aero

- IPD range: 58–72mm
- Gaze camera resolution: 640 x 400 px per camera.
- Gaze tracking frequency: 100 Hz (default for native SDK) or 200 Hz (default for OpenXR and Unity XR SDK)

### Varjo XR-4

- IPD range: 56–72mm
- Gaze camera resolution: 640 x 480 px per camera.
- Gaze tracking frequency: 100 Hz (default for native SDK) or 200 Hz (default for OpenXR and Unity XR SDK)

## DEVELOPING WITH THE 20/20 EYE TRACKER

Before you launch a demo with eye tracking, make sure to enable **Allow eye tracking** in the **System** tab in Varjo Base.

![](https://developer.varjo.com/assets/uploads/eye_tracking.png)

Keep in mind that eye tracking must be recalibrated whenever the headset is taken off and put back on, even if the same person is using it. This is necessary because the headset may not be positioned exactly the same way on a person’s head every time. You can manage the calibration directly from your application.