### Outline
- [x] Read XR Documentation
- [x] Setup Unity XR SDK
- [x] Gain lab access (collec card)
- [ ] Setup *VarjoBaseSetup_4.14.0.exe*
	- [ ] Get admin permissions
- [ ] Implement Hand tracking
- [ ] Implement Eye tracking
## Varjo headset developer setup
On the [Developer tools in Varjo Base](https://developer.varjo.com/docs/get-started/developer-tools-in-varjo-base) page you can find information on how to test your build without the Varjo headset.
### Mixed Reality with Varjo XR plugin
Developing mixed reality applications for the Varjo XR headsets.

- [x] Once set up, the Varjo XR plugin settings can be found in the **Project Settings** dialog. Under the **XR Plug-in Management** tab, select **Varjo** and disable the **Opaque** option. 
- [ ] You can also do this with a script using runtime functions. See [Rendering settings](https://developer.varjo.com/docs/unity-xr-sdk/rendering-settings-in-varjo-xr-plugin) for additional information.
![](https://developer.varjo.com/assets/uploads/unity-opaque.png)

- [ ] To enable/disable rendering the image from the VPT cameras, use the following methods:
```c++
// Start rendering the video see-through image
VarjoMixedReality.StartRender();

// Stop rendering the video see-through image
VarjoMixedReality.StopRender();
```

When enabled, you will see the image from the VPT cameras when your application renders 0 in the color buffer. You can do this either by using a stencil mask or by making the camera clear to 0. For instructions on how to set up a stencil mask, see [Masking](https://developer.varjo.com/docs/unity-xr-sdk/masking-with-varjo-xr-plugin). To make the camera clear to 0, set the camera’s **Clear Flags** to **Solid Color** and set **RGBA(0,0,0,0)** as the background color.
![[clear-camera-unity-xr-settings.png|500]]
#### High Definition Render Pipeline and Universal Render Pipeline
When using HDRP, make sure to use a color buffer format with an alpha channel. 
- [ ] In the **Project Settings** dialog, select the **Quality** tab and under **HDRP settings** select the **HD Render Pipeline Asset**. 
- [ ] Set the **Color Buffer Format** dropdown to **R16G16B16A16**. You will need to do this for all HDRP assets in use.
![](https://developer.varjo.com/assets/uploads/unity-hdrp-color-buffer-format.png)

- [ ] To make the camera clear to 0, set the camera’s **Background Type** to **Color** and set **RGBA(0,0,0,0)** as the background color.
![](https://developer.varjo.com/assets/uploads/unity-vst-camera-settings-hdrp.png)
#### Using mixed reality with Universal Render Pipeline
When using URP, you need to modify the camera settings to be able to write into the alpha channel of the color buffer. 

- [ ] For VPT to work, make sure the camera’s **Post Processing** and **HDR** settings are disabled.
- [ ] To make the camera clear to 0, set the camera’s **Background Type** to **Solid Color** and set **RGBA(0,0,0,0)** as the background color.
![](https://developer.varjo.com/assets/uploads/unity-vst-camera-settings-urp.png)
### Environment reflections
- [ ] Varjo XR Plugin can use the feed from the VPT cameras to create lighting conditions and reflections that match those in your current real world location. 

This is *useful when blending your virtual content with the real world*, making the scene appear more natural and realistic.
![](https://developer.varjo.com/assets/uploads/EnvironmentReflections.PNG)
### Shadow catcher
- [ ] To **cast shadows** on the ground under your virtual objects, add a shadow catcher material to a flat plane set at ground level relative to the XR rig. 
- [ ] For dynamic content, this can be done with the following graph. The graph will only show *real-time* shadows.
![](https://developer.varjo.com/assets/uploads/ShadowCatcher.PNG)
## Multi-app support
Multiple Varjo applications can be ran simultaneously, allowing the image to be constructed using several applications made with different engines or image generators.

The applications are rendered in priority order. Higher-priority is overlaid on top of lower.

Example applications include:
- **Training & Simulation:** Use a specific IG to render terrain while using a Unity or Unreal app for rendering the cockpit, hand-tracking, and mission selection.
- **Design & Research:** Use a specific application to bring 3D objects on top of another virtual scene with Varjo Markers or with custom-made SteamVR applications.
- **Research & Analytics:** Access eye-tracking data while another application is used to render visuals.
### Performance
Be aware that running multiple applications simultaneously can be very taxing on overall system performance. Often, visual quality needs to be toned down to achieve satisfactory performance. Multiple GPUs can be used to improve performance, but keep in mind that this requires specific application support. See to the [Benchmark example](https://developer.varjo.com/docs/native/varjo-sdk-examples#benchmark-example) for a reference implementation.

Note that currently there is no multi-GPU support for Unity or Unreal.
To use multi-app support, please refer to the SDK-specific documentation:
- [Varjo Native SDK](https://developer.varjo.com/docs/native/multi-app-native)
- [Unity XR SDK](https://developer.varjo.com/docs/unity-xr-sdk/multi-app-with-varjo-xr-plugin)
### Using multi-app functionality with Varjo XR plugin
- [ ] To enable multi-app functionality, modify the _Session Priority_ setting in the Varjo-specific **Project Settings**. 

Applications are sorted by their session priority so that higher-priority sessions are overlaid on top of lower-priority ones.

Read more about the _Session Priority_ setting and it’s runtime functions in [Rendering Settings](https://developer.varjo.com/docs/unity-xr-sdk/rendering-settings-in-varjo-xr-plugin).
- [x] To enable transparency for your application, refer to [Mixed Reality with Varjo XR plugin](https://developer.varjo.com/docs/unity-xr-sdk/mixed-reality-with-varjo-xr-plugin).

When developing, **refer to this page** for multi-app use-cases:
- [ ] https://developer.varjo.com/docs/get-started/multi-app-advanced
## Timing
### Varjo system time
Varjo uses *nanoseconds* as a time unit. Varjo timestamps are monotonic, independent of the time-of-day clock, and relative to the epoch that is constant during the execution of the program (Varjo system epoch). 

The current Varjo time can be queried using the method `GetVarjoTimestamp` in the _VarjoTime_ class, which returns the number of nanoseconds that elapsed since the Varjo system epoch.
```c++
// Gets current Varjo system timestamp
long GetVarjoTimestamp();
```
### Varjo system time conversion
Note that the Varjo system time epoch is not equal to e.g. the [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time) or any other global time epoch. However, Varjo timestamps can be converted to be compatible with timestamps from other time sources. 

To do so, Varjo system time and the other time source needs to be synchronized. The clock synchronization procedure measures the offset between the two time sources and the offset would then be used for the timestamp conversion.
### Conversion to C# DateTime object
The method `ConvertVarjoTimestampToDateTime` in the _VarjoTime_ class can be used to convert from the Varjo nanoseconds time to the real time-of-day clock timestamp expressed as the C# [DateTime](https://learn.microsoft.com/en-us/dotnet/api/system.datetime) object.
```c#
// Converts Varjo system timestamp to DateTime object
DateTime? ConvertVarjoTimestampToDateTime(long varjoTimestamp);
```

The result of this conversion depends on the time-of-day clock adjustment of the operating system (e.g. Windows time synchronization with NTP server). 

This conversion is supported for Varjo timestamps that are up to one hour in the past, but not earlier than the time when the Varjo system was started (e.g. the conversion can’t be used with timestamps that were recorded before the last computer reboot or restart of the Varjo services). 

The conversion can also be applied to Varjo timestamps that are in the future relative to the result of `GetVarjoTimestamp`, however, the future time conversion must be used with consideration of a possible upcoming time-of-day clock adjustment that can’t be accounted for at the moment of the conversion.

The conversion can fail (_null_ is returned) if queried for an unsupported Varjo timestamp value, however, the failure is not guaranteed in all possible misuse scenarios.
## Using eye tracking with Varjo XR plugin
### Calibration
To reliably track user’s eye movements, eye tracking must be calibrated first. 
While Varjo headsets support automatic one-dot calibration, we recommended that you request a separate calibration if more accurate eye tracking data is required. 

- [ ] You can request calibration by calling a method in the _VarjoEyeTracking_ class.
```c++
// Requests gaze calibration: "Fast" 5 dot calibration is used by default.
// Returns true if calibration was successfully requested.
bool VarjoEyeTracking.RequestGazeCalibration();

// Requests gaze calibration with specific GazeCalibrationMode: allows to
// choose between "Fast" and "OneDot" calibration.
// Returns true if calibration was successfully requested.
bool VarjoEyeTracking.RequestGazeCalibration(gazeCalibrationMode);
```

It is possible to query quality assessment of current user gaze calibration by calling method `GetGazeCalibrationQuality`.
```c++
// Returns score assesment of currently used gaze calibration
GazeCalibrationQuality VarjoEyeTracking.GetGazeCalibrationQuality();
```

Returned struct `GazeCalibrationQuality` contains score `GazeEyeCalibrationQuality` for left and right eye calibration. These values indicate how well each of the eyes passed through the calibration procedure and how accurate returned gaze data could be for the eye. Possible returned scores are `Invalid` (score not available), `Low`, `Medium` and `High`.
### Accessing eye tracking data
If eye tracking is calibrated, you can access eye tracking data by using either the [XR Input subsystem](https://docs.unity3d.com/ScriptReference/XR.XRInputSubsystem.html) or the methods provided in the _VarjoEyeTracking_ class.

- [ ] With the Input subsystem, the eye tracking data can be retrieved using [Eyes interface](https://docs.unity3d.com/ScriptReference/XR.Eyes.html).

The methods in _VarjoEyeTracking_ allow you to poll the eye tracking data at the full frequency supported by the headset, which is especially important for research scenarios. 

These methods provide eye tracking data frames as [_GazeData_](https://developer.varjo.com/docs/unity-xr-sdk/eye-tracking-with-varjo-xr-plugin#gazedata) structs and estimates for user’s eye properties as [_EyeMeasurements_](https://developer.varjo.com/docs/unity-xr-sdk/eye-tracking-with-varjo-xr-plugin#eyemeasurements):
### GazeData
- `long frameNumber` - A unique identifier of the frame at the time when the data was recorded.
- `long captureTime` - A timestamp, in nanoseconds, of when the video frame was recorded by the eye tracking cameras. It is relative to the [Varjo system time epoch](https://developer.varjo.com/docs/native/introduction-to-varjo-sdk#timing).
- `float focusDistance` - The distance between eye and focus point in meters. Values are between 0 and 2 meters.
- `float focusStability` - Stability of the user’s focus. Value are between 0.0 and 1.0, where 0.0 indicates least stable focus and 1.0 most stable.
- [deprecated] `float leftPupilSize` - Pupil size for the left eye, calculated according to the pupil size range detected by the headset. Values are between 0 and 1.
- [deprecated] `float rightPupilSize` - Pupil size for the right eye, calculated according to the pupil size range detected by the headset. Values are between 0 and 1.

- **GazeStatus** status - A status for eye tracking.
- **GazeRay** gaze - Gaze ray combined from both eyes.
- **GazeEyeStatus** leftStatus - A status for the left eye.
- **GazeRay** left - Gaze ray for the left eye.
- **GazeEyeStatus** rightStatus - A status for the right eye.
- **GazeRay** right - Gaze ray for the right eye.

**GazeStatus** is a value for the eye tracking status of the headset as follows:
- 0 – Data unavailable: User is not wearing the headset or eyes cannot be located
- 1 – User is wearing the headset, but gaze tracking is being calibrated
- 2 – Data is valid

**GazeRay** struct contains data about eye position coordinates in meters [_origin (x, y, z)_] and a normalized direction vector [_forward (x, y, z)_]. Gaze data is given in the left-hand coordinate system and is relative to head pose.

**GazeEyeStatus** is a value for each eye as follows:
- 0 – Eye is not tracked and not visible (e.g., the eye is shut)
- 1 – Eye is visible but not reliably tracked (e.g., during a saccade or blink)
- 2 – Eye is tracked but quality is compromised (e.g., the headset has moved after calibration)
- 3 – Eye is tracked

Gaze data should be polled frequently as all frames older than _500ms_ are discarded. Gaze data is polled using the following methods:
```c++
// Returns the latest gaze data frame. The result may change even within a single Unity engine frame.
GazeData VarjoEyeTracking.GetGaze();

// Writes all gaze data frames since the last call to this method in GazeData list.
// Returns the number of retrieved gaze data frames.
int VarjoEyeTracking.GetGazeList(out gazeDataList);

// Pulls most recent gaze data and eye measurements from the queue.
// Returns the number of retrieved gaze data frames.
int VarjoEyeTracking.GetGazeList(out List<GazeData> gazeData, out List<EyeMeasurements> eyeMeasurements)
```
### EyeMeasurements
- `long frameNumber` - A unique identifier of the frame at the time when the data was recorded.
- `long captureTime` - A timestamp, in nanoseconds, of when the video frame was recorded by the eye tracking cameras. It is relative to the [Varjo system time epoch](https://developer.varjo.com/docs/native/introduction-to-varjo-sdk#timing).
- `float interPupillaryDistanceInMM` - Estimate of user’s inter-pupillary distance measured in millimeters. 0.0 if estimate is not available.
- `float leftPupilIrisDiameterRatio` - Ratio of user’s left pupil diameter estimate to estimated iris diameter. 0.0 if either estimate is not available and ratio cannot be calculated
- `float rightPupilIrisDiameterRatio` - Ratio of user’s right pupil diameter estimate to estimated iris diameter. 0.0 if either estimate is not available and ratio cannot be calculated
- `float leftPupilDiameterInMM` - Estimate of pupil diameter for the left eye in millimeters. If estimate is not available e.g. when pupil is not tracked, value is set to 0.0.
- `float rightPupilDiameterInMM` - Estimate of pupil diameter for the right eye in millimeters. If estimate is not available e.g. when pupil is not tracked, value is set to 0.0.
- `float leftIrisDiameterInMM` - Estimated diameter of left iris in millimeters. Estimation is updated when user puts the headset on his head. Estimate values are valid until user removes headset from his head. 0.0 is returned when estimate is not available.
- `float rightIrisDiameterInMM` - Estimated diameter of right iris in millimeters. Estimation is updated when user puts the headset on his head. Estimate values are valid until user removes headset from his head. 0.0 is returned when estimate is not available.
- `float leftEyeOpenness` - Estimated openness ratio of the left eye. Value 1.0 corresponds to a fully open eye, 0.0 to a fully closed eye.
- `float rightEyeOpenness` - Estimated openness ratio of the right eye. Value 1.0 corresponds to a fully open eye, 0.0 to a fully closed eye.

Eye measurements data can be polled using the following methods:
```c++
// Returns the latest eye measurements. The result may change even within a single Unity engine frame.
EyeMeasurements VarjoEyeTracking.GetEyeMeasurements();

// Pulls most recent gaze data and eye measurements from the queue.
int VarjoEyeTracking.GetGazeList(out List<GazeData> gazeData, out List<EyeMeasurements> eyeMeasurements)
```
*Please note* that **leftPupilIrisDiameterRatio**, **rightPupilIrisDiameterRatio**, **leftPupilDiameterInMM**, **rightPupilDiameterInMM**, **leftIrisDiameterInMM**, **rightIrisDiameterInMM**, **leftEyeOpenness**, **rightEyeOpenness** are experimental features and provided in this release without any accuracy guarantees.
### Data stream options
- [ ] The Varjo XR plugin provides several options for the eye tracking data stream. 

You can choose between:
- smoothed (default, filter type _Standard_) and unfiltered gaze data (filter type _None_)
- and select a gaze output frequency from _100Hz_, _200Hz_ 
- and the _maximum supported frequency_ by connected headset (default). 

See the general [Eye tracking](https://developer.varjo.com/docs/get-started/eye-tracking-with-varjo-headset) documentation for frequencies supported by different headsets.
You can set the options using the following methods:
```c++
// Sets GazeOutputFilterType: None / Standard
// Returns true if output filter type was successfully set.
bool VarjoEyeTracking.SetGazeOutputFilterType(outputFilterType);

// Sets GazeOutputFrequency: MaximumSupported / Frequency100Hz / Frequency200Hz
// Returns true if output frequency was successfully set.
bool VarjoEyeTracking.SetGazeOutputFrequency(outputFrequency);
```

You can also query currently used options using the following methods:
```c++
// Returns currently set GazeOutputFilterType
GazeOutputFilterType VarjoEyeTracking.GetGazeOutputFilterType();

// Returns currently set GazeOutputFrequency
GazeOutputFrequency VarjoEyeTracking.GetGazeOutputFrequency();
```
#### Eye tracking examples
Examples of using eye tracking can be found in the [Eye tracking example](https://github.com/varjocom/VarjoUnityXRPlugin/blob/master/Samples~/HDRP/EyeTracking/Scripts/EyeTrackingExample.cs) of the [Varjo Unity XR Plugin](https://github.com/varjocom/VarjoUnityXRPlugin).
## Using Varjo hand tracking
Varjo hand tracking is **not supported** by Varjo Unity XR SDK. 
- [ ] However, you can use it with Unity OpenXR. See [[#Unity XR SDK Compatibility]] above.
### Using hand tracking with Ultraleap Tracking plugin
Get Ultraleap Tracking plugin from [here.](https://github.com/ultraleap/UnityPlugin)
- [x] https://github.com/ultraleap/UnityPlugin#Installation

Follow the [Ultraleap documentation](https://docs.ultraleap.com/xr-and-tabletop/xr/unity/getting-started/index.html) to enable and use the plugin in your project.
- [ ] https://docs.ultraleap.com/xr-and-tabletop/xr/unity/getting-started/index.html

**Do not** install any Ultraleap drivers (ie. Ultraleap Gemini). All you need is the Ultraleap Unity plugin. The drivers are embedded to Varjo Base installation.

When you start to develop with hand tracking, make sure to define an offset for the hand position. This is necessary because the head tracking point for your headset differs from the hand tracking point for Ultraleap.

The offsets can be defined in the **Leap XR Service Provider** component under **Advanced Options**. 
- [ ] Select **Manual Head Offset** as the **Device Offset Mode** and set the correct values for **Device Offset Y Axis**, **Device Offset Z Axis** and **Device Tilt X Axis**.
![](https://developer.varjo.com/assets/uploads/unity-ultraleap-offset-1.png)

- [ ] Use the following offset for Varjo XR-4:
```c#
Y:       0
Z:       0
X tilt:  0
```
### Set hand tracking offsets automatically
Attach the following component next to the **Leap XR Service Provider** component to handle the offsets automatically depending on the device used.
```c#
using UnityEngine;
using UnityEngine.XR;
using Leap.Unity;

[RequireComponent(typeof(LeapXRServiceProvider))]
public class VarjoHandTrackingOffset : MonoBehaviour
{
	private InputDevice hmd;
	private LeapXRServiceProvider xrServiceProvider;
	
	void Start()
	{
		hmd = InputDevices.GetDeviceAtXRNode(XRNode.Head);
		xrServiceProvider = GetComponent<LeapXRServiceProvider>();

		switch (hmd.name)
		{
		case "XR-4":
				xrServiceProvider.deviceOffsetMode = 
					LeapXRServiceProvider.DeviceOffsetMode.ManualHeadOffset;
				xrServiceProvider.deviceOffsetYAxis = 0f;
				xrServiceProvider.deviceOffsetZAxis = 0f;
				xrServiceProvider.deviceTiltXAxis = 0f;
				break;
		case "XR-3":
		case "VR-3":
				xrServiceProvider.deviceOffsetMode = LeapXRServiceProvider.DeviceOffsetMode.ManualHeadOffset;
				xrServiceProvider.deviceOffsetYAxis = -0.0112f;
				xrServiceProvider.deviceOffsetZAxis = 0.0999f;
				xrServiceProvider.deviceTiltXAxis = 0f;
				break;
		}
	}
}
```
![](https://developer.varjo.com/assets/uploads/unity-ultraleap-offset-2.png)
## Control interpupillary distance (IPD)
### Setting interpupillary distance parameters
- [ ] It is possible to set interpupillary distance (IPD) parameters of the Varjo headset by calling the method `SetInterPupillaryDistanceParameters` in the _VarjoHeadsetIPD_ class.
```c#
// Sets interpupillary distance (IPD) parameters of the headset.
bool SetInterPupillaryDistanceParameters(IPDAdjustmentMode ipdAdjustmentMode,
    float? RequestedPositionInMM = null);
```

- [ ] The requested IPD adjustment mode can be either _Manual_ or _Automatic_.
```c#
// Interpupillary Distance (IPD) Adjustment Mode
public enum IPDAdjustmentMode
{
    Manual = 0,
    Automatic = 1
}
```

With the _Manual_ adjustment mode, the headset can be set to the desired IPD position by supplying the parameter `RequestedPositionInMM` containing a floating point value in millimeters. When using this parameter, the caller should typically set the position value to be equal to the known IPD of the user, even if the the value is out of the supported IPD range of the headset. The headset will then drive the IPD motor to the closest supported position.

*Note that* passing a negative, infinite or not-a-number value via the `RequestedPositionInMM` argument will result in a failure and the function will return `false`. An attempt to set a specific IPD position will also fail if the _Automatic_ adjustment mode is requested.

The accuracy of the _Manual_ IPD adjustment mode via the API is nearly the same as via the user interface (UI) in Varjo Base and thus there is little to no gain in setting a more accurate IPD value via API (e.g. by passing more digits of the decimal fraction) than by setting the value via the UI, given the level of accuracy reached by setting it via the UI.
### Reading interpupillary distance parameters state
- [ ] You can query the currently used IPD adjustment mode by calling the method `GetAdjustmentMode` in the _VarjoHeadsetIPD_ class.
```c#
// Get interpupillary distance adjustment mode
IPDAdjustmentMode GetAdjustmentMode();
```

- [ ] You can read the interpupillary distance set in the headset in millimeters by calling `GetDistance()` from the same _VarjoHeadsetIPD_ class. Returns zero if it is not connected.
```c#
// Get current headset interpupillary distance in millimeters
float GetDistance();
```
## Unity XR plugin examples
The Varjo XR Plugin includes an optional [samples](https://github.com/varjocom/VarjoUnityXRPlugin/tree/master/Samples~) package, which contains example scenes and code for enabling Varjo features for your project. *You are free to use, copy, and modify the code and examples for your own projects.*

While samples are currently provided only for the High Definition Render Pipeline, most of the example scripts also work with other render pipelines. Samples for Unity’s Built-in Render Pipeline and Universal Render Pipeline will be added later.

The samples package includes example scenes for:
- [Controller input](https://github.com/varjocom/VarjoUnityXRPlugin/tree/master/Samples~/HDRP/ControllerInput)
- [Eye tracking](https://github.com/varjocom/VarjoUnityXRPlugin/tree/master/Samples~/HDRP/EyeTracking)
- [Markers](https://github.com/varjocom/VarjoUnityXRPlugin/tree/master/Samples~/HDRP/Markers)
- [Mixed reality (video pass-through)](https://github.com/varjocom/VarjoUnityXRPlugin/tree/master/Samples~/HDRP/MixedReality)
### Importing samples into the project
- [ ] You can import the provided samples into your project using the Package Manager:
- In the Unity main menu, navigate to **Window** > **Package Manager**.
- Select **Varjo XR Plugin** from the list of installed packages.
- Available samples are listed under **Samples**. Select one and click **Import**.

- [ ] Samples are imported into the _Assets_ folder.
![](https://developer.varjo.com/assets/uploads/unity-samples.png)
### Unity XR SDK Known Issues
#### All Render Pipelines
- No XR-4 Varjo Controllers: Update Varjo Unity XR SDK to 3.6.0 or newer in your project or, if you cannot do that, open SteamVR tray when running your application.
- SteamVR Tracker role ‘Held in hand (any)’ does not work with Unity Input System.
#### High Definition Render Pipeline (HDRP)
- Some built-in post process effects cause foveation area to be visible.
- Directional light flare causes foveation area to be visible with HDRI sky.
#### Universal Render Pipeline (URP)
- Unity 6 onwards URP does not render correctly and is not recommended to be used. Caused by invalid camera stereo matrices.
- Unity 2022 LTS onwards URP/Unlit masking materials do not work.
- Post processing causes foveation area to be visible.
- Camera.main.GetStereoProjectionMatrix() returns incorrect matrix.
#### Built-in Render Pipeline
- Real time lights cause foveation area to be visible on materials lit by them.
- Particles cause foveation area to be visible.
## Varjo Base
https://account.varjo.com/downloads/varjoheadsets
#### Showcases
https://account.varjo.com/downloads/showcases
Of interest:
- [ ] https://account.varjo.com/downloads/showcases/varjo_showcases_v4/40_jet_engine/
- [ ] https://account.varjo.com/downloads/showcases/varjo_showcases_v4/40_human_anatomy_controller/
- [ ] https://account.varjo.com/downloads/showcases/varjo_showcases_v4/40_human_anatomy_ultraleap/
## VR Template Quick Start Guide
Unity’s **VR Project Template** provides a starting point for virtual reality development in Unity. The template configures project settings, pre-installs the right packages, and includes various pre-configured Example Assets to demonstrate how to set up a project that is ready for VR. Please refer to the [XR Project Setup](https://docs.unity3d.com/2022.3/Documentation/Manual/xr-create-projects.html) documentation for additional information.

The VR Template uses the following Unity features:
- [OpenXR](https://docs.unity3d.com/Packages/com.unity.xr.openxr@1.14/index.html) - A scalable package that enables cross-platform support for multiple hardware environments.
- [XR Interaction Toolkit](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@3.1/index.html) - A high-level, component-based, interaction system for creating XR experiences. It provides a framework that makes 3D and UI interactions available from Unity input events.
### Using the Sample Scene
The **VR Template** contains a **Scene** named `SampleScene` located in `Assets/Scenes`. This scene’s Hierarchy is pre-configured with GameObjects that allow your application to manage controller and head tracking, locomotion, and interaction with spatial UI and virtual objects.

You can use this scene as a reference, or you can remove the example Assets from it and use it as a starting point for your own Project.
### Sample Scene Hierarchy Overview
![SampleSceneHierarchy](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/sample-scene-hierarchy.png)
### Interactables
There are various objects within the scene that the user is able to interact with. In XRI, they are called interactables. The grab interactable allows a user to pick up a GameObject using either a direct or ray interactor. Grab interactables need a Rigidbody and a Collider to define the interactable area.
![SampleInteractable](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/sample-interactable.png)

In addition, the [Affordance system](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@3.1/manual/affordance-system.html#:%7E:text=The%20XR%20Interactable%20Affordance%20State,subscribed%20to%20this%20particular%20provider.) provides feedback for the user with visual and auditory cues. This requires the use of the [XR Interactable Affordance State Provider](https://docs.unity.cn/Packages/com.unity.xr.interaction.toolkit@3.1/manual/xr-interactable-affordance-state-provider.html) with a specified interactable source. 

For the interactables included in the Sample scene, there is an [Audio Affordance Receiver](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@2.6/api/UnityEngine.XR.Interaction.Toolkit.AffordanceSystem.Receiver.Audio.AudioAffordanceReceiver.html) and a [Color Affordance Receiver](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@2.6/api/UnityEngine.XR.Interaction.Toolkit.AffordanceSystem.Receiver.Rendering.ColorMaterialPropertyAffordanceReceiver.html) already set up.
### Color Affordance Receiver Example
![ColorAffordanceReceiver](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/color-affordance-receiver.png)
[Audio Affordance Receiver Example](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#audio-affordance-receiver-example)
![AudioAffordanceReceiver](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/audio-affordance-receiver.png)

- [ ] These Affordance Receivers are driven by [Affordance System Themes](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@2.6/api/UnityEngine.XR.Interaction.Toolkit.AffordanceSystem.Theme.html) found in Assets > VR Template Assets > Themes.
[Edge Color Affordance Theme Example](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#edge-color-affordance-theme-example)
![EdgeColorAffordanceTheme](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/edge-color-affordance-theme.png)

[Audio Affordance Theme Example](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#audio-affordance-theme-example)
![AudioAffordanceTheme](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/audio-affordance-theme.png)
### Teleportation
Teleportation allows users to move within the Sample Scene without experiencing the discomfort common while walking in VR. [Teleportation Areas](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@3.1/api/UnityEngine.XR.Interaction.Toolkit.Locomotion.Teleportation.TeleportationArea.html) designate specific surfaces for the user to move. The teleportation area is configured with specific colliders from the scene.

[Teleportation Area](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#teleportation-area)
![TeleportArea](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/teleport-area.png)

[Teleport Anchors](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@3.1/api/UnityEngine.XR.Interaction.Toolkit.Locomotion.Teleportation.TeleportationAnchor.html), on the other hand, constrain the user’s view in terms of position and rotation while teleporting. One is already set up for you in the scene.

[Teleportation Anchor](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#teleportation-anchor)
![TeleportAnchor](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/teleport-anchor.png)
### Spatial UI
The Sample Scene contains various world space UI examples which allows both near and far interactions with UI elements such as sliders, dropdown menus, toggles, and buttons. In addition, user’s have the ability to move the canvas in space with the Spatial UI Panel prefab, which affords the ability to grab the canvas by either the header or handle at the bottom of the canvas. Billboarding with the Turn To Face component is on the prefab by default, with the positional transformation of the canvas being determined by the direct/ray interactor.
[Spatial UI Example](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#spatial-ui-example)
![SpatialUI](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/ui-setup.png)
[Project Configuration](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#project-configuration)
Various presets are used to accommodate graphics & rendering requirements for different platforms. By selecting the target platform in the Build Settings, i.e. Windows, Mac, Linux or Android, different URP Config Settings are automatically updated for the user. The presets can be found in `Assets/Settings/Project Configuration`.

- [ ] For Android, the `Android Preset` and `Performance URP Config` assets provide a good baseline of graphics settings for low powered headsets such as the Quest.
![ProjectConfigURPAndroid](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/projectconfig-urp-android.png)

- [ ] For standalone PC VR, the `Standalone Preset` and `Quality URP Config` assets allow for a higher quality experience.
![ProjectConfigURPPCVR](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/projectconfig-urp-pcvr.png)

- [ ] In addition, by selecting the target platform in Build Settings, OpenXR feature groups and interaction profiles are preconfigured. By default, the Android build target updates the settings for Meta Quest support.
![ProjectConfigOpenXRAndroid](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/projectconfig-openxr-metaquest.png)

- [ ] For standalone PC VR, numerous interaction profiles are already configured for the user. If you don't see your platform listed, add your interaction profile manually by clicking the **+** button.
![ProjectConfigOpenXRAndroid](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/images/projectconfig-openxr-standalone.png)
### Graphics APIs
[Graphics APIs](https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#graphics-apis)
The last setting you may need to adjust if you are using the Android build target is the Graphics API. By default, the VR Template uses the Vulkan graphics API for compatibility and performance reasons, including reduced build times. 
 
- [ ] If you are targeting a device that does not fully support the Vulkan API, you may have to add *OpenGLES3* to this list. You can do this by following these steps:
1. Go to **Edit** > **Project Settings**
2. Click **Player** in the left-hand pane
3. Click on the **Android** tab
4. Expand **Other Settings**
5. Locate the **Graphics APIs** list and click the **+** symbol
6. In the drop-down click on **OpenGLES3** to add it to the list
7. Click on **File** > **Save Project** to lock in these changes
## Development without Headset
https://support.varjo.com/hc/en-us/analytics-window

- [x] https://docs.unity3d.com/Packages/com.unity.template.vr@6.1/manual/index.html#audio-affordance-receiver-example
- [x] https://docs.unity3d.com/2022.3/Documentation/Manual/xr-create-projects.html
- [x] https://docs.unity3d.com/Manual/XR.html

## Development with XR-4 Headset and Ultraleap
- [ ] https://github.com/ultraleap/UnityPlugin?tab=readme-ov-file
- [ ] https://docs.ultraleap.com/xr-and-tabletop/xr/unity/getting-started/index.html
- [ ] https://docs.ultraleap.com/hand-tracking/XR-Setup/XR-setup.html