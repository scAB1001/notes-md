# Goal: Understand what fault tolerance in a distributed system (DS) is supported
# Outline and Content:
## 14. Fault Tolerance - *Chapter 8*
The key technique for handling failures is redundancy.
### Dependability, reliability and availability in a DS
Being fault tolerant is strongly related to what are called dependable systems.
Requirements for a **dependable** system: *Availability*, *Reliability*, *Safety* and *Maintainability*.

**(Availability)** The probability the system is operational and ready for use at a _given instant in time_. A highly available system has minimal downtime.

**(QUANTIFYING AVAILABILITY & RELIABILITY)** Key metrics include:
The **availability** $A(t)$ of **a component** in the time interval $[0, t]$ is the *average fraction of time* that the component has been *functioning correctly* during that interval. 
The **long-term availability** $A$ of a component is defined as $A(∞)$.
Likewise, the **reliability** $R(t)$ of a component in the time interval $[0, t]$ is the *conditional probability* that it has been *functioning correctly* during that interval *given* that it was *functioning correctly* at time $T = 0$.
- **Mean Time To Failure** ($MTTF$): The average time until a component fails
- **Mean Time To Repair** ($MTTR$): The average time needed to repair a component
- **Mean Time Between Failures** ($MTBF$) = $MTTF + MTTR$
- General Availability: $A ={ MTTF \over MTBF} = {MTTF \over{MTTF + MTTR}}$. For example, a system with an MTTF of 1000 hours and an MTTR of 10 hours has an availability of `1000 / 1010 ≈ 0.99` (99%).

**(Reliability)** The probability the system operates _continuously without failure_ over a specified _time interval_. A highly reliable system has long periods of uninterrupted service.*In contrast to availability*, reliability is defined in terms of a time interval instead of an instant in time. A highly reliable system is one that will most likely continue to work without interruption during a relatively long period of time.

**(NOTE)** Reliability and availability make sense only if we have an accurate notion of what a *failure* actually is. Availability and reliability measure different things. A system that is down for 1ms every hour has 99.9999% availability but is unreliable (fails frequently). A system shut down for 2 weeks every year is reliable during operation but only 96% available.

**(Safety)** refers to the situation that when a system temporarily fails to operate correctly, no catastrophic event happens. For example, many process-control systems, such as those used for controlling nuclear power plants or sending people into space, are required to provide a high degree of safety. If such control systems temporarily fail for only a very brief moment, the effects could be disastrous. Many examples from the past (and probably many more yet to come) show how hard it is to build safe systems.

**(Maintainability)** refers to how easily a failed system can be re- paired. A highly maintainable system may also show a high degree of availability, especially if failures can be detected and repaired automatically. Automatic failure recovery difficult.
### Terminology
A system is said to *fail* when it cannot meet its promises. In particular, if a distributed system is designed to provide its users with a number of services, the system has failed when one or more of those services cannot be (completely) provided.

| Term                                    | Description                                                              | Example                                              |
| --------------------------------------- | ------------------------------------------------------------------------ | ---------------------------------------------------- |
| Failure                                 | A component is not living up to its<br>specifications.                   | Program crashes.                                     |
| Error                                   | Part of a component (system's state) that can lead to a failure.         | Programming Bug.                                     |
| Fault                                   | The cause of an error.                                                   | The developer.                                       |
| Fault Prevention                        | Prevent the occurrence of a fault.                                       | Thorough software verification and validation.       |
| Fault Tolerance (FT)<br>(**Important**) | Build a component such that it can mask the occurrence of a fault.       | Build each component by two independent programmers. |
| Fault Removal                           | Reduce the presence, number, or seriousness of a fault.                  | Thorough software verification<br>and validation.    |
| Fault Forecasting                       | Estimate current presence, future incidence, and consequences of faults. | Check code quality /<br>programming experience.      |
The developer is the *fault* for the *error*: bug which caused the program to crash in *failure*.
Aim: improve *fault tolerance* -> system can provide its services *even* in the case of faults.
### Failure models

| Type of Failure                           | Description of Server’s Behaviour                                                                                   |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| Crash                                     | Halts, but is working correctly until it halts                                                                      |
| Omission:<br> Receive<br> Send            | Fails to respond to incoming requests<br>Fails to receive incoming messages<br>Fails to send messages (response)    |
| Timing                                    | Response lies outside a specified time interval                                                                     |
| Performance                               | Server responds too late.                                                                                           |
| Response<br> Value<br> State-transition   | Response is incorrect<br>The value of the response is wrong<br>Deviates from correct flow of control (reacts wrong) |
| Arbitrary (or Byzantine)<br>**(Serious)** | May produce arbitrary responses at arbitrary<br>times that is not detected as incorrect. False info.                |
#### Dependability vs Security (Omission versus commission)
Arbitrary failures are sometimes qualified as malicious. It is important to make the following distinction:
- *Omission* failures: a component *fails to take an action* that it *should have* taken
- *Commission* failure: a component *takes an action* that it *should not have* taken
These deliberate failures are often *security* problems. Distinguishing between *deliberate* failures and *unintentional* ones is difficult.
#### Failure Masking by Redundancy
If a system is to be fault tolerant, the best it can do is to try to hide the occurrence of failures from other processes. The key technique for masking faults is to use redundancy:
- **Information** redundancy: Add extra bits to data units so that errors can be recovered when bits are garbled e.g., Hamming code
- **Time** redundancy: Design a system where actions can be performed again if anything went wrong e.g., retransmission request to a server when lacking an expected response
- **Physical** redundancy: add equipment or processes in order to allow one or more components to fail. Often used in DS e.g., extra processes are added to a system so that the system can still function correctly if processes crash.
### Process resilience by Groups
FT can actually be achieved in DS by *protecting* against *process failures*, which is achieved by **replicating processes** into groups. 

organize several identical processes into a group. The key property that all groups have is that when a message is sent to the group itself, all members of the group receive it. In this way, if one process in a group fails, hopefully some other process can take over for it.

Process groups may be dynamic. New groups can be created and old
groups can be destroyed. A process can join a group or leave one during
system operation. A process can be a member of several groups at the same
time. Consequently, mechanisms are needed for managing groups and group
membership.
#### Groups Organisation
An important distinction between different groups has to do with their internal structure. In some groups, all processes are equal. There is no distinctive leader and all decisions are made collectively. In other groups, some kind of hierarchy exists. 
![[process-groups.png]]
**(FLAT GROUP)** symmetrical and has no single point of failure. If one of
the processes crashes, the group simply becomes smaller, but can otherwise
continue. A disadvantage is that decision making is more complicated. For
example, to decide anything, a vote often has to be taken, incurring some
delay and overhead.
**(HIERARCHICAL GROUP)** has the opposite properties. Loss of the coordinator
brings the entire group to a grinding halt, but as long as it is running, it
can make decisions without bothering everyone else. In practice, when the
coordinator in a hierarchical group fails, its role will need to be taken over and
one of the workers is elected as new coordinator.
#### Groups and Failure Masking
A **k-fault tolerant group**: when a group can mask any $k$ concurrent member failures. 
A system is said to be **k-fault tolerant** if it can survive faults in $k$ components and still meet its specifications. $k$ is called **degree of fault tolerance**.

If the components, say processes, fail silently, then having $k + 1$ of them is enough to provide k-fault tolerance. If $k$ of them simply stop, then use the answer from the other one.
With **halting failures** (crash/omission/timing failures): we need a total of **k +1** members as no member will produce an incorrect result, so the result of one member is good enough.

On the other hand, if processes exhibit **arbitrary failures**, continuing to run when faulty and sending out erroneous or random replies, a minimum of $2k + 1$ processes are needed to achieve k-fault tolerance. In the worst case, the $k$ failing processes could accidentally (or even intentionally) generate the same reply. However, the remaining $k + 1$ will also produce the same answer, so the client or voter can just believe the majority.
### Consensus with crash failures
In terms of clients and servers, we have adopted a model in which a potentially very large collection of clients now send commands to a group of processes that jointly behave as a single, highly robust process. To make this work, we need to make an important assumption:

In a **fault-tolerant process group**, each nonfaulty process executes the same commands, in the same order, as every other nonfaulty process

Group members must reach consensus on which command to execute. If failures cannot happen, reaching consensus is easy. For example, using a centralized sequencer that hands out a sequence number to each command that needs to be executed.
#### Crash Failures: Flooding-based Consensus
Assume we have a group of processes P = {P1, . . . , Pn} operating under **fail-stop** failure semantics. We assume that crash failures can be reliably detected among the group members (reliable failure detection). 
A client contacts a group member requesting it to execute a command. Every group member maintains a list of proposed commands: some which it received directly from clients; others which it received from its fellow group members.

The algorithm operates in rounds. In each round, a process $P_i$ sends its list of proposed commands it has seen so far to every other process in $P$. At the end of a round, each process merges all received proposed commands into a new list, from which it then will deterministically select the command to execute, if possible. 

It is important to realize that the selection algorithm is the same for all processes. In other words, if all process have exactly the same list, they will all select the same command to execute (and remove that command from their list). This approach works as long as processes do not fail. Problems start when a process $P_i$ detects, during round $r$, that, say process $P_k$ has crashed. 

**(EXAMPLE)** assume we have a process group of four processes {P1, . . . , P4} and that $P_1$ crashes during round $r$. Also, assume that $P_2$ receives the list of proposed commands from $P_1$ before it crashes, but that $P_3$ and $P_4$ do not (in other words, $P_1$ crashes before it got
a chance to send its list to $P_3$ and $P_4$).
![[crash-ex-1.png]]
- P3 may have detected that P1 crashed, but does not know if P2 received anything, i.e., P3 cannot know if it has the same info as P2 -> cannot make decision (same holds for P4)
- P3 and P4 postpone decision to next round
- P2 makes a decision and broadcast that decision to the others
- P3 and P4 are able to make a decision at round (r+1)
- P3 and P4 decide to execute the same command selected by P2
#### Raft Consensus Algorithm
**Developed for understandability**
- Uses a fairly straightforward **leader-election** algorithm. The current leader operates during the **current term**.
- Every server (typically, five) keeps a **log** of operations, some of which have been committed. *A backup will not vote for a new leader if its own log is more up to date*.
- All committed operations have the same position in the log of each respective server.
-The leader decides which pending op to commit next ⇒ a **primary-backup approach**.
**When submitting an operation**
- A client submits a request for operation $o$
- The leader appends the request $⟨o, t.k⟩$ to its own log (registering the
current term $t$ and length of ), $k$ is the index of o in the leader’s log
- The log is (conceptually) broadcast to the other servers
- The others (conceptually) copy the log and acknowledge the receipt
- When a majority of ACKs arrives, the leader commits $o$.
(**Note**) In practice, only updates are *broadcast*
- At the end, every server has the *same view* and knows about the $c$ committed operations
- Effectively, any information at the backups is *overwritten*
#### Raft: When a Leader Crashes
![[leader-crashes-raft.png]]
New leader election is initiated
• The new leader has the most committed operations in its log
• Any missing commits will eventually be sent to the other backups.
#### The Two Generals Problem
- **Scenario:** Two armies (Generals) must coordinate an attack, but can only communicate via messengers through enemy territory, where messages might be lost.
- **The Dilemma:** General A sends a messenger saying "Attack at dawn." General B receives it and agrees, but to be sure, sends a messenger back: "I agree." General A receives this confirmation, but now _A_ isn't sure _B_ got _A's_ confirmation of _B's_ agreement, so A might not attack, leaving B to attack alone and lose

Achieving perfect, guaranteed agreement (consensus) between two parties over an unreliable communication channel is impossible, even with multiple messages, because one party can never be 100% certain the other received their final confirmation, leading to potential failure if one acts alone. 

In fault tolerance, this shows that systems must settle for probabilistic guarantees, use acknowledgements with timeouts (like TCP), and implement mechanisms (like idempotency) to handle potential duplicate messages and inconsistent states, as complete reliability is unattainable.
![[two-generals-problem.png]]
How Should the Generals Decide?
a) General 1 always attacks, even if no response is received?
- Send lots of messengers to increase probability that one will get through
- If all are captured, general 2 does not know about the attack, so general 1 loses
b) General 1 only attacks if positive response from general 2 is received?
- Now general 1 is safe
- But general 2 knows that general 1 will only attack if general 2's response gets through
- Now general 2 is in the same situation as general 1 in option 1
No common knowledge: the only way of knowing something is to communicate it.
### The Byzantine Generals Problem
A group of Byzantine generals surrounds an enemy city, needing to agree on a coordinated attack or retreat. Communication is via messengers, but some generals might be traitors, sending different messages to different recipients (e.g., "Attack" to one, "Retreat" to another).
**The Goal**: All loyal generals must agree on the same plan (attack or retreat) and execute it together. A split decision leads to defeat.
![[byzantine=generals-problem.png]]
Up to $f$ generals might behave maliciously
- Honest generals don't know who the malicious ones are and the malicious may collude
- Honest generals must agree on plan
- Theorem: need $3f + 1$ generals in total to tolerate $f$ malicious generals (i.e. < 1/3)
- Cryptography (digital signatures) helps, but problem remains hard

DSs achieve consensus despite faulty or malicious nodes (generals) sending conflicting information, requiring honest nodes to agree on a single plan (attack/retreat) for system success.
### Consensus with arbitrary failures
For replicas only subject to *crash failures*, a process group needs to consist of $2k + 1$ servers to survive $k$ crashed members. Here, we assume that a process does not collude with another process and that it is consistent in its messages to others. 

Now, we look at reaching consensus in a fault-tolerant process group in which k members can fail assuming arbitrary failures. We need at least $3k + 1$ members to reach consensus under these failure assumptions.

We consider process groups in which communication between processes is **inconsistent**.
![[arb-consensus-1.png]]
**System model**
• Process group consisting of n members of which one is designated to be the *primary* $P$ and $n - 1$ *backups* $B1, …Bn-1$.
• A client sends a value $v \in {T\text{rue}, F\text{alse}}$ to $P$
• Messages may be lost, but this can be detected
• Messages cannot be corrupted beyond detection
• A receiver of a message can reliably detect its sender.
#### Byzantine Agreement: Requirements
BA1: Every nonfaulty backup process stores the same value
BA2: If the primary is nonfaulty then every nonfaulty backup process stores exactly what the primary had sent.
1. Primary is faulty -> BA1 says that backups may store the same, but different (and thus wrong) value than originally sent by the client.
2. Primary is not faulty -> satisfying BA2 implies that BA1 is satisfied 
#### Why having 3k processes is not enough
**(EXAMPLE)** consider the situation of tolerating the failure of a single process, that is, $k = 1$.
![[arb-consensus-2.png.png]]
*Solution*: $3k+1$
![[arb-consensus-3.png.png.png]]
#### System Models
In real systems, both nodes and networks may be faulty!
- Two generals problem: a model of networks
- Byzantine generals problem: a model of node behaviour

Capture assumptions in a system model consisting of:
- **Network** behaviour (e.g. message loss)
- **Node** behaviour (e.g. crashes)
- **Timing** behaviour (e.g. latency)
Choice of models for each of these parts.
