# Goal: Understand what fault tolerance in a distributed system (DS) is supported
# Outline and Content:
## 14. Fault Tolerance - *Chapter 8*
The key technique for handling failures is redundancy.
### Dependability, reliability and availability in a DS
Being fault tolerant is strongly related to what are called dependable systems.
Requirements for a **dependable** system: *Availability*, *Reliability*, *Safety* and *Maintainability*.

**(Availability)** The probability the system is operational and ready for use at a _given instant in time_. A highly available system has minimal downtime.
**(Reliability)** The probability the system operates _continuously without failure_ over a specified _time interval_ instead of an instance in time. A highly reliable system has long periods of uninterrupted service.
**(Safety)** When a system fails temporarily, it fails in a way that does not cause catastrophic damage (e.g., a nuclear reactor control system failing to a safe state).
**(Maintainability)** The ease and speed with which a failed system can be repaired. High maintainability supports high availability.

**(QUANTIFYING AVAILABILITY & RELIABILITY)** Key metrics include:
The **availability** $A(t)$ of **a component** in the time interval $[0, t]$ is the *average fraction of time* that the component has been *functioning correctly* during that interval. 
The **long-term availability** $A$ of a component is defined as $A(∞)$.
Likewise, the **reliability** $R(t)$ of a component in the time interval $[0, t]$ is the *conditional probability* that it has been *functioning correctly* during that interval *given* that it was *functioning correctly* at time $T = 0$.
- **Mean Time To Failure** ($MTTF$): The average time until a component fails
- **Mean Time To Repair** ($MTTR$): The average time needed to repair a component
- **Mean Time Between Failures** ($MTBF$) = $MTTF + MTTR$
- General Availability: $A ={ MTTF \over MTBF} = {MTTF \over{MTTF + MTTR}}$. For example, a system with an MTTF of 1000 hours and an MTTR of 10 hours has an availability of `1000 / 1010 ≈ 0.99` (99%).

**(NOTE)** Reliability and availability make sense only if we have an accurate notion of what a *failure* actually is. Availability and reliability measure different things. A system that is down for 1ms every hour has 99.9999% availability but is unreliable (fails frequently). A system shut down for 2 weeks every year is reliable during operation but only 96% available.
### Terminology
A system is said to *fail* when it cannot meet its promises. In particular, if a distributed system is designed to provide its users with a number of services, the system has failed when one or more of those services cannot be (completely) provided.

| Term                                    | Description                                                              | Example                                              |
| --------------------------------------- | ------------------------------------------------------------------------ | ---------------------------------------------------- |
| Failure                                 | A component is not living up to its<br>specifications.                   | Program crashes.                                     |
| Error                                   | Part of a component (system's state) that can lead to a failure.         | Programming Bug.                                     |
| Fault                                   | The cause of an error.                                                   | The developer.                                       |
| Fault Prevention                        | Prevent the occurrence of a fault.                                       | Thorough software verification and validation.       |
| Fault Tolerance (FT)<br>(**Important**) | Build a component such that it can mask the occurrence of a fault.       | Build each component by two independent programmers. |
| Fault Removal                           | Reduce the presence, number, or seriousness of a fault.                  | Thorough software verification<br>and validation.    |
| Fault Forecasting                       | Estimate current presence, future incidence, and consequences of faults. | Check code quality /<br>programming experience.      |
The developer is the *fault* for the *error*: bug which caused the program to crash in *failure*.
Aim: improve *fault tolerance* -> system can provide its services *even* in the case of faults.
### Failure models

| Type of Failure                           | Description of Server’s Behaviour                                                                                              |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| Crash                                     | Halts, but is working correctly until it halts                                                                                 |
| Omission:<br> Receive<br> Send            | Fails to respond to incoming requests<br>Fails to receive incoming messages<br>Fails to send messages (response)               |
| Timing                                    | Response lies outside a specified time interval (timeout)                                                                      |
| Performance                               | Server responds too late.                                                                                                      |
| Response<br> Value<br> State-transition   | Response is incorrect e.g. 2+2=5<br>The value of the response is wrong<br>Deviates from correct flow of control (reacts wrong) |
| Arbitrary (or Byzantine)<br>**(Serious)** | May produce arbitrary responses at arbitrary<br>times that is not detected as incorrect. False info.                           |
#### Dependability vs Security (Omission versus Commission)
Arbitrary failures are sometimes qualified as malicious. 
- *Omission* failures: a component *fails to take an action* that it *should have* taken
- *Commission* failure: a component *takes an action* that it *should not have* taken. Malicious (Byzantine) failures are often commission failures.
These deliberate failures are often *security* problems. Distinguishing between *deliberate* failures and *unintentional* ones is difficult.
#### Failure Masking by Redundancy
If a system is to be fault tolerant, the best it can do is to try to hide the occurrence of failures from other processes. The key technique for masking faults is to use redundancy:
- **Information** redundancy: Add extra bits to data units so that errors can be recovered when bits are garbled e.g., Hamming code
- **Time** redundancy: Design a system where actions can be performed again if anything went wrong e.g., retransmission request to a server when lacking an expected response
- **Physical** redundancy: add equipment or processes in order to allow one or more components to fail. Often used in DS e.g., extra processes are added to a system so that the system can still function correctly if processes crash.
### Process resilience by Groups
FT can actually be achieved in DS by *protecting* against *process failures*, which is achieved by **replicating processes** into groups. 

Organize several identical processes into a group. For all groups: when a message is sent to the group itself, all members of the group receive it. In this way, if one process in a group fails, hopefully some other process can take over for it.

Process groups may be dynamic. New groups can be created and old groups can be destroyed. A process can join a group or leave one during system operation. A process can be a member of several groups at the same time. Consequently, mechanisms are needed for managing groups and group membership.
#### Groups Organisation
![[process-groups.png]]
**(FLAT GROUP)** All processes are equal (symmetric). More robust (no single point of failure) but decision-making (e.g., voting) is slower and more complex.
**(HIERARCHICAL GROUP)** One process acts as the coordinator (leader). Efficient for decision-making but the coordinator is a single point of failure; if it crashes, a new leader must be elected.
#### Groups and Failure Masking
A system is **k-fault tolerant** if it can survive `k` component failures and still meet its specification. The required group size depends on the failure type:

- For **Halting or Crash/Omission Failures (fail-stop):** You need **k + 1** processes. If `k` fail, the one remaining correct process provides the answer.
- For **Arbitrary (Byzantine) Failures:** You need at least **2k + 1** processes. In the worst case, `k` malicious processes could send the same bad answer. You need a majority (`k+1`) of correct processes to out-vote them and reach a consensus.
### Consensus with crash failures
For a replicated service to work correctly, all non-faulty replicas must **agree (reach consensus)** on the order and content of client commands. With only crash failures, this is simpler.

In a **fault-tolerant process group**, each nonfaulty process executes the same commands, in the same order, as every other nonfaulty process
#### Crash Failures: Flooding-based Consensus
A simple algorithm where processes exchange lists of proposed commands in rounds. Each process merges received lists and uses a **deterministic function** (same for all) to select the next command to execute.  
![[crash-ex-1.png]]This approach works as long as processes do not fail. Problems start when a process $P_i$ detects, during round $r$, that, say process $P_k$ has crashed. 
- **Challenge:** If a process crashes mid-round, some members may have received its proposal and others not. Processes must wait an extra round to ensure they all have the same information before deciding, ensuring consistency.

**(EXAMPLE)** assume we have a process group of four processes {P1, . . . , P4} and that $P_1$ crashes during round $r$. Also, assume that $P_2$ receives the list of proposed commands from $P_1$ before it crashes, but that $P_3$ and $P_4$ do not (in other words, $P_1$ crashes before it got
a chance to send its list to $P_3$ and $P_4$).
![[crash-ex-1.png]]
- P3 may have detected that P1 crashed, but does not know if P2 received anything, i.e., P3 cannot know if it has the same info as P2 -> cannot make decision (same holds for P4)
- P3 and P4 postpone decision to next round
- P2 makes a decision and broadcast that decision to the others
- P3 and P4 are able to make a decision at round (r+1)
- P3 and P4 decide to execute the same command selected by P2
#### Raft Consensus Algorithm
**A popular, understandable algorithm for managing a replicated log.
- Uses a fairly straightforward **leader-election** algorithm. The current leader operates during the **current term**.
- Every server (typically, five) keeps a **log** of operations, some of which have been committed. *A backup will not vote for a new leader if its own log is more up to date*.
- All committed operations have the same position in the log of each respective server.
- The leader decides which pending op to commit next ⇒ a **primary-backup approach**.
- 
**When submitting an operation**
- A client submits a request for operation $o$
- The leader appends the request $⟨o, t.k⟩$ to its own log (registering the current term $t$ and length of ), $k$ is the index of o in the leader’s log
- The log is (conceptually) broadcast to the other servers
- The others (conceptually) copy the log and acknowledge the receipt
- When a majority of ACKs arrives, the leader commits $o$.

**(EXAMPLE)**
1. A **leader** is elected for a **term**. All client commands go to the leader.
2. The leader appends the command to its log and replicates it to **followers**.
3. Once a **majority** of followers acknowledge, leader **commits** the entry and notifies them.
4. This ensures all servers have the same log of committed commands.  
![[leader-crashes-raft.png]]
- **Leader Failure:** If the leader crashes, a new election is held. The new leader must have the most up-to-date log to ensure consistency. Any missing commits will eventually be sent to the other backups.

(**Note**) In practice, only updates are *broadcast*
- At the end, every server has the *same view* and knows about the $c$ committed operations
- Effectively, any information at the backups is *overwritten*
#### The Two Generals Problem
This thought experiment illustrates the fundamental difficulty of achieving **perfect reliability** over an unreliable communication channel.  
![[two-generals-problem.png]]

- **Scenario:** Two armies must coordinate an attack by sending messengers through enemy lines. Messengers can be captured.
- **Dilemma:** No matter how many acknowledgements are sent, the sender can never be _100% certain_ the final acknowledgement was received, creating an infinite regress of uncertainty.
- **Implication:** In practice, distributed systems use timeouts and probabilistic guarantees (e.g., TCP retransmission) rather than seeking perfect certainty.
**How Should the Generals Decide?**
a) General 1 always attacks, even if no response is received?
- Send lots of messengers to increase probability that one will get through
- If all are captured, general 2 does not know about the attack, so general 1 loses
b) General 1 only attacks if positive response from general 2 is received?
- Now general 1 is safe
- But general 2 knows that general 1 will only attack if general 2's response gets through
- Now general 2 is in the same situation as general 1 in option 1
No common knowledge: the only way of knowing something is to communicate it.
### The Byzantine Generals Problem
This is the consensus problem in the presence of **arbitrary (Byzantine) failures**, where components may behave maliciously and send conflicting information.  
![[byzantine=generals-problem.png]]

- **Goal:** All loyal (non-faulty) generals must agree on a common plan (e.g., Attack or Retreat) despite the presence of `f` traitors who may lie and send contradictory messages.
- **Famous Result (Lamport et al.):** To tolerate `f` Byzantine failures, the system must have **at least `3f + 1`** total components. This means **less than one-third** can be faulty. Cryptography (digital signatures) helps but doesn't change this fundamental bound.
### Consensus with arbitrary failures
For replicas only subject to *crash failures*, a process group needs to consist of $2k + 1$ servers to survive $k$ crashed members. Here, we assume that a process does not collude with another process and that it is consistent in its messages to others. 

Now, we look at reaching consensus in a fault-tolerant process group in which k members can fail assuming arbitrary failures. We need at least $3k + 1$ members to reach consensus under these failure assumptions.

We consider process groups in which communication between processes is **inconsistent**.
![[arb-consensus-1.png]]
**System model**
• Process group consisting of n members of which one is designated to be the *primary* $P$ and $n - 1$ *backups* $B1, …Bn-1$.
• A client sends a value $v \in {T\text{rue}, F\text{alse}}$ to $P$
• Messages may be lost, but this can be detected
• Messages cannot be corrupted beyond detection
• A receiver of a message can reliably detect its sender.
#### Byzantine Agreement: Requirements
BA1: Every nonfaulty backup process stores the same value
BA2: If the primary is nonfaulty then every nonfaulty backup process stores exactly what the primary had sent.
1. Primary is faulty -> BA1 says that backups may store the same, but different (and thus wrong) value than originally sent by the client.
2. Primary is not faulty -> satisfying BA2 implies that BA1 is satisfied 
#### Why having 3k processes is not enough
**(EXAMPLE)** consider the situation of tolerating the failure of a single process, that is, $k = 1$.
![[arb-consensus-2.png.png]]
*Solution*: $3k+1$
- **Example (Why 4 is needed for f=1):** With 3 generals (1 faulty), the faulty one can tell different lies to the two loyal ones, leaving each loyal general with a different view and no way to determine the truth, as they cannot form a majority (2 out of 3) for a single value.  
![[arb-consensus-3.png.png.png]]
- **Solution (With 4 for f=1):** With 4 generals (1 faulty), the 3 loyal generals can exchange messages. Even if the traitor tells different stories, the 3 loyal ones can compare notes and agree on the majority value, achieving consensus.
#### System Models
In real systems, both nodes and networks may be faulty!
- Two generals problem: a model of networks
- Byzantine generals problem: a model of node behaviour

Capture assumptions in a system model consisting of:
- **Network** behaviour (e.g. message loss)
- **Node** behaviour (e.g. crashes)
- **Timing** behaviour (e.g. latency)
Choice of models for each of these parts.
