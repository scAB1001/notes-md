Goals: understand principles behind consistency and replication
Outline:
## 12. Consistency and Replication Pt.1 - *Chapter 7*
### Introduction to the problem

### Data-centric consistency models
content:
## 12. Consistency and Replication Pt.1 - *Chapter 7*
### Introduction to the problem
 An important issue in distributed systems is the
**replication** of data
– Example: multiple copies of the same file
• Data are generally replicated to
– **Enhance reliability**: what if one replica crashes?
– **Improve performance**: an increasing number of processes
need to access data managed by a single server
• One of the major problems is **keeping replicas consistent**
– This means that when one copy is updated we need to
ensure that the other copies are updated as well
– otherwise the replicas will no longer be the same
![[replica-db-ex.png]]
#### Performance and Scalability
> To keep replicas consistent, we need to ensure that all **conflicting** operations are done in the same order everywhere.

From the world of transactions, conflicting operations (ops) are:
- Read-write conflict: a read and write op act concurrently.
- Write-write conflict: two concurrent write ops

The issue is that you cannot guarantee global ordering on these conflicting ops as it is costly, affecting scalability:
*Solution:* Weaken consistency requirements so that global synch can be avoided.
##### Reasons for replication
There are two primary reasons for replicating data. First, data are replicated
to increase the reliability of a system. If a file system has been replicated
it may be possible to continue working after one replica crashes by simply
switching to one of the other replicas. Also, by maintaining multiple copies,
it becomes possible to provide better protection against corrupted data. For
example, imagine there are three copies of a file and every read and write
operation is performed on each copy. We can safeguard ourselves against a
single, failing write operation, by considering the value that is returned by at
least two copies as being the correct one.
The other reason for replicating data is performance. Replication for
performance is important when a distributed system needs to scale in terms of
size or in terms of the geographical area it covers. Scaling with respect to size
occurs, for example, when an increasing number of processes needs to access
data that are managed by a single server. In that case, performance can be
improved by replicating the server and subsequently dividing the workload
among the processes accessing the data.
Scaling with respect to a geographical area may also require replication.
The basic idea is that by placing a copy of data in proximity of the process
using them, the time to access the data decreases. As a consequence, the
performance as perceived by that process increases. This example also illus-
trates that the benefits of replication for performance may be hard to evaluate.
Although a client process may perceive better performance, it may also be the
case that more network bandwidth is now consumed keeping all replicas up
to date.
If replication helps to improve reliability and performance, who could be
against it? Unfortunately, there is a price to be paid when data are replicated.
The problem with replication is that having multiple copies may lead to
consistency problems. Whenever a copy is modified, that copy becomes
different from the rest. Consequently, modifications have to be carried out on
all copies to ensure consistency. Exactly when and how those modifications
need to be carried out determines the price of replication.
To understand the problem, consider improving access times to Web pages.
If no special measures are taken, fetching a page from a remote Web server
may sometimes even take seconds to complete. To improve performance,
Web browsers often locally store a copy of a previously fetched Web page
(i.e., they cache a Web page). If a user requires that page again, the browser
automatically returns the local copy. The access time as perceived by the user
is excellent. However, if the user always wants to have the latest version of
a page, he may be in for bad luck. The problem is that if the page has been
modified in the meantime, modifications will not have been propagated to
cached copies, making those copies out-of-date.
One solution to the problem of returning a stale copy to the user is to
forbid the browser to keep local copies in the first place, effectively letting the
server be fully in charge of replication. However, this solution may still lead
to poor access times if no replica is placed near the user. Another solution is
to let the Web server invalidate or update each cached copy, but this requires
that the server keeps track of all caches and sending them messages. This,
in turn, may degrade the overall performance of the server. We return to
performance versus scalability issues below.
##### Replication as scaling technique
Replication and caching for performance are widely applied as scaling tech-
niques. Scalability issues generally appear in the form of performance prob-
lems. Placing copies of data close to the processes using them can improve
performance through reduction of access time and thus solve scalability prob-
lems.
A possible trade-off that needs to be made is that keeping copies up to date
may require more network bandwidth. Consider a process P that accesses
a local replica N times per second, whereas the replica itself is updated M
times per second. Assume that an update completely refreshes the previous
version of the local replica. If N  M, that is, the access-to-update ratio is
very low, we have the situation where many updated versions of the local
replica will never be accessed by P, rendering the network communication
for those versions useless. In this case, it may have been better not to install
a local replica close to P, or to apply a different strategy for updating the
replica.
A more serious problem, however, is that keeping multiple copies con-
sistent may itself be subject to serious scalability problems. Intuitively, a
collection of copies is consistent when the copies are always the same. This
means that a read operation performed at any copy will always return the
same result. Consequently, when an update operation is performed on one
copy, the update should be propagated to all copies before a subsequent
operation takes place, no matter at which copy that operation is initiated or
performed.
This type of consistency is sometimes informally (and imprecisely) re-
ferred to as tight consistency as provided by what is also called synchronous
replication. (In Section 7.2, we will provide precise definitions of consistency
and introduce a range of consistency models.) The key idea is that an update
is performed at all copies as a single atomic operation, or transaction. Unfor-
tunately, implementing atomicity involving a large number of replicas that
may be widely dispersed across a large-scale network is inherently difficult
when operations are also required to complete quickly.
Difficulties come from the fact that we need to synchronize all replicas. In
essence, this means that all replicas first need to reach agreement on when
exactly an update is to be performed locally. For example, replicas may need
to decide on a global ordering of operations using Lamport timestamps, or
let a coordinator assign such an order. Global synchronization simply takes
a lot of communication time, especially when replicas are spread across a
wide-area network.
We are now faced with a dilemma. On the one hand, scalability problems
can be alleviated by applying replication and caching, leading to improved per-
formance. On the other hand, to keep all copies consistent generally requires
global synchronization, which is inherently costly in terms of performance.
The cure may be worse than the disease.
In many cases, the only real solution is to relax the consistency constraints.
In other words, if we can relax the requirement that updates need to be
executed as atomic operations, we may be able to avoid (instantaneous) global
synchronizations, and may thus gain performance. The price paid is that
copies may not always be the same everywhere. As it turns out, to what
extent consistency can be relaxed depends highly on the access and update
patterns of the replicated data, as well as on the purpose for which those data
are used.
There are a range of consistency models and many different ways to
implement models through what are called distribution and consistency
protocols.
### Data-centric consistency models
> A consistency model is a contract between a (distributed) data store and processes where the data store specifies precisely what the results of read and write ops are in the presence of concurrency. 

A data store is a distributed collection of storages:
![[data-store-ex.png]]
#### Continuous Consistency