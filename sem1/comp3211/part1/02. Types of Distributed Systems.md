Monday 06/10/2025

---
# Original
#### Module Objectives
Three types of distributed systems
1. High performance distributed computing systems
2. Distributed information systems
3. Distributed systems for pervasive computing
### 1. High Performance Distributed Computing System (HPDCS)
High-performance distributed computing started with parallel computing
- Multiprocessor and multi-core versus multi-computer 
#### Distributed Shared Memory systems 
Multiprocessors are relatively easy to program in comparison to multi-computers, yet have problems when increasing the number of processors (or cores). 

**Problem**
Performance of distributed shared memory could never compete with that of multiprocessors, and failed to meet the expectations of programmers. 
- It has been widely abandoned by now.
- Roughly speaking, one can make a distinction between two subgroups of DS for high-performance computing tasks.

**Solution** 
Try to implement a shared-memory model on top of a multi-computer. 
Example through virtual-memory techniques. 
- Map all main-memory pages (from different processors) into one single virtual address space. 
- If a process at processor A addresses a page P located at processor B, the OS at A traps and fetches P from B, just as it would if P had been located on local disk. 
#### Cluster Computing
Cluster computing systems became popular when the price/performance ratio of personal computers and workstations improved. 

- It then became financially and technically attractive to build a supercomputer using off-the-shelf technology by simply hooking up a collection of relatively simple computers in a high-speed network. 
- In virtually all cases, cluster computing is used for parallel programming in which a single (compute intensive) program is run in parallel on multiple machines.
##### Linux-based Beowulf clusters
![[cluster-computing-system-ex.png]]
- Each cluster consists of a collection of compute nodes that are controlled and accessed by means of a single master node. 
	- This node typically handles the allocation of nodes to a particular parallel program, maintains a batch queue of submitted jobs, and provides an interface for the users of the system. 
	- As such, the master node actually runs the middleware needed for the execution of programs and management of the cluster, while the compute nodes are equipped with a standard OS extended with typical middleware functions for communication, storage, fault tolerance, and so on. 
	- Apart from the master node, the compute nodes are thus seen to be highly identical.
##### MOSIX system [Amar et al., 2004] 
MOSIX takes a more symmetric approach; it attempts to provide a single-system image of a cluster, meaning that to a process a cluster computer offers the ultimate distribution transparency by appearing to be a single computer. 

As we mentioned, providing such an image under all circumstances is impossible. 
With MOSIX, the high degree of transparency is provided by allowing processes to dynamically and preemptively migrate between the nodes that make up the cluster. 

Process migration allows a user to start an application on any node (referred to as the home node), after which it can transparently move to other nodes, for example, to make efficient use of resources.

However, several modern cluster computers have been moving away from these symmetric architectures to more hybrid solutions in which the middleware is functionally partitioned across different nodes, as explained by En- gelmann et al. [2007]. 

The advantage of such a separation is obvious: 
- having compute nodes with dedicated, lightweight operating systems will most likely provide optimal performance for compute-intensive applications. 
- Likewise, storage functionality can most likely be optimally handled by other specially configured nodes such as file and directory servers. 
- The same holds for other dedicated middleware services, including job management, database services, and perhaps general Internet access to external services.
##### Summary
- Essentially a group of high-end systems connected
- through a Local Area Network (LAN)
- Homogeneous: same OS, near-identical hardware
- Single managing node running the OS
#### Grid Computing
Comprising DSs that are often constructed as a federation of computer systems, where each system may fall under a different administrative domain, and may be very different when it comes to hardware, software, and deployed network technology.

We've noted the emerging trend of a more hybrid architecture whereby nodes are configured specifically for certain tasks. This is even more prevelant in grid computing. 

A key issue in a grid-computing system is that resources from different organizations are brought together to allow the collaboration of a group of people from different institutions, forming a federation of systems. This becomes a form of virtual organization. 
- The processes belonging to the same virtual organization have access rights to the resources that are provided to that organization. 
- Typically, resources consist of compute servers (including supercomputers, possibly implemented as cluster computers), storage facilities, and databases. 
- In addition, special networked devices such as telescopes, sensors, etc., can be provided as well.

Given its nature, much of the software for realizing grid computing evolves around providing access to resources from different administrative domains, and to only those users and applications that belong to a specific virtual organization. For this reason, focus is often on architectural issues.
##### Proposed Architecture for grid computing systems Foster et al. [2001]
![[grid-computing-systems-proposed-arch-ex.png]]
The architecture consists of four layers. 
###### Fabric layer (0)
The lowest layer provides interfaces to local resources at a specific site. 
Note: _These interfaces are tailored to allow sharing of resources within a virtual organization._ 

Typically, they will provide functions for querying the state and capabilities of a resource, along with functions for actual resource management (e.g., locking resources). 
###### Connectivity layer (1)
Consists of communication protocols for supporting grid transactions that span the usage of multiple resources. 
- For example, protocols are needed to transfer data between resources, or access remote ones.
- In addition, this layer contains security protocols to authenticate users and resources. 
Note: _in many cases human users are not authenticated; instead, programs acting on behalf of the users are authenticated._ 

In this sense, delegating rights from a user to programs is an important function that needs to be supported in the connectivity layer. 
###### Resource layer (1)
Responsible for managing a single resource. 
It uses the functions provided by the connectivity layer and calls directly the interfaces made available by the fabric layer. 

For example, this layer will offer functions for obtaining configuration information on a specific resource, or, in general, to perform specific operations such as creating a process or reading data. 

This layer is responsible for access control, and hence will rely on the authentication performed as part of the connectivity layer.
###### Collective layer (2)
It deals with handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on. 

Unlike the connectivity and resource layer, each consisting of a relatively small, standard collection of protocols, the collective layer may consist of many different protocols reflecting the broad spectrum of services it may offer to a virtual organization. 
###### Application layer
Consists of the applications that operate within a virtual organization and which make use of the grid computing environment. 

Typically the collective, connectivity, and resource layer form the heart of what could be called a **grid middleware layer**. 

These layers jointly provide access to and management of resources that are potentially dispersed across multiple sites.
##### Open Grid Services Architecture (OGSA) [Foster et al., 2006].
An important observation from a middleware perspective is that in grid computing the notion of a site (or administrative unit) is common. 

This prevalence is emphasized by the gradual shift toward a service-oriented architecture in which sites offer access to the various layers through a collection of Web services [Joseph et al., 2004]. 

This leads to the definition of an alternative architecture known as the Open Grid Services Architecture (OGSA). OGSA is based upon the original ideas as formulated by Foster et al. [2001], yet having gone through a standardization process makes it complex, to say the least. 
OGSA implementations generally follow Web service standards. 
#### Cloud Computing
From the perspective of grid computing, a next logical step is to simply outsource the entire infrastructure that is needed for compute-intensive applications. 

In essence, this is what cloud computing is all about: 
- providing the facilities to dynamically construct an infrastructure and compose what is needed from available services. 

Unlike grid computing, which is strongly associated with high-performance computing, cloud computing is much more than just providing lots of resources.
##### Definitions
Cloud computing is an information technology infrastructure in which computing resources are virtualised and accessed as a service.

Following Vaquero et al. [2008], cloud computing is characterized by an easily usable and accessible pool of _virtualized_ resources. 
- Which and how resources are used can be configured dynamically, providing the basis for scalability: 
	- if more work needs to be done, a customer can simply acquire more resources. 
	- The link to utility computing is formed by the fact that cloud computing is generally based on a pay-per-use model in which guarantees are offered by means of customized service level agreements (SLAs).

The next step: lots of nodes from everywhere
- Flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources
- Enable communities “Virtual Organisations” to share geographically distributed resources as they pursue common goals
##### Utility Computing
While researchers were pondering on how to organize computational grids that were easily accessible, organizations in charge of running data centers were facing the problem of opening up their resources to customers. 

This lead to the concept of utility computing by which a customer could upload tasks to a data center and be charged on a per-resource basis. This then formed the basis of cloud computing
##### Organisation
The organization of clouds (adapted from Zhang et al. [2010]). Four layers.
![[the-organisation-of-clouds.png]]

Distinction between four layers:
1. **Hardware**
	The lowest layer is formed by the means to manage the necessary hardware: processors, routers, but also power and cooling systems. It is generally implemented at data centers.
	- Processors, routers, power and cooling systems.
	- Customers normally never get to see these
2. **Infrastructure** 
	Provides customers an infrastructure consisting of virtual storage and computing resources.
	- Deploys virtualisation techniques. 
	- Evolves around allocating and managing virtual storage devices and virtual servers
3. **Platform** 
	One could argue that the platform layer provides to a cloud-computing customer what an operating system provides to application developers, namely the means to easily develop and deploy applications that need to run in a cloud. 
	- In practice, an application developer is offered a vendor-specific API, which includes calls to uploading and executing a program in that vendor’s cloud. 
	- In a sense, this is comparable  to the Unix exec family of system calls, which take an executable file as parameter and pass it to the operating system to be executed. 
	- Also like operating systems, the platform layer provides higher-level abstractions for storage and such. 
		- The Amazon S3 storage system [Murty, 2008] is offered to the application developer in the form of an API allowing (locally created) files to be organized and stored in buckets. 
		- A bucket is somewhat comparable to a directory. By storing a file in a bucket, that file is automatically uploaded to the Amazon cloud.
4. **Application** 
	Actual applications, such as office suites (text processors, spreadsheet applications, presentation applications) run here. 
	- It is important to realize that these applica- tions are again executed in the vendor’s cloud.
	- Comparable to the suite of apps shipped with OSes.
##### How these layers are provided
Cloud-computing providers offer these layers to their customers through various interfaces (including command-line tools, programming interfaces, and Web interfaces), leading to three different types of services:
- **Infrastructure-as-a-Service** (IaaS) covering the hardware and infrastructure layer
- **Platform-as-a-Service** (PaaS) covering the platform layer
- **Software-as-a-Service** (SaaS) in which their applications are covered

As of now, making use of clouds is relatively easy, and we discuss in later chapters more concrete examples of interfaces to cloud providers. 
As a consequence, cloud computing as a means for outsourcing local computing infrastructures has become a serious option for many enterprises. However, there are still a number of serious obstacles including provider lock-in, security and privacy issues, and dependency on the availability of services, to mention a few (see also Armbrust et al. [2010]). 

Also, because the details on how specific cloud computations are actually carried out are generally hidden, and even perhaps unknown or unpredictable, meeting performance demands may be impossible to arrange in advance. 
On top of this, Li et al. [2010] have shown that different providers may easily show very different performance profiles. 

Cloud computing is no longer a hype, and certainly a serious alternative to maintaining huge local infrastructures, yet there is still a lot of room for improvement.
### 2. Distributed Information Systems (DIS)
##### Context: Integrating applications
Many of the existing middleware solutions are the result of working with an infrastructure in which it was easier to integrate applications into an enterprise-wide information system [Alonso et al., 2004; Bernstein, 1996; Hohpe and Woolf, 2004]. 

##### Situation: organisations confronted with many networked applications, but achieving interoperability was painful.
We can distinguish several levels at which integration can take place. In many cases, a networked application simply consists of a server running that application (often including a database) and making it available to remote programs, called **clients**. 
- Such clients send a request to the server for executing a specific operation, after which a response is sent back. 
- Integration at the lowest level allows clients to wrap a number of requests, possibly for different servers, into a single larger request and have it executed as a distributed transaction.

The key idea is that all, or none of the requests are executed. 
##### Basic approach: a networked application is one that runs on a server making its services available to remote clients. 
Simple integration: clients combine requests for (different) applications; send that off; collect responses, and present a coherent result to the user. 

As applications became more sophisticated and were gradually separated into independent components (notably distinguishing database components from processing components), it became clear that integration should also take place by letting applications communicate directly with each other. 

This has now lead to a huge industry that concentrates on **Enterprise Application Integration (EAI)**.
##### Example: Distributed Transaction Processing
Context: Database applications where operations are carried out in the form of transactions.
- In a mail system, there might be primitives to send, receive, and forward mail. 
- In an accounting system, they might be quite different. READ and WRITE are typical examples, however. Ordinary statements, procedure calls, and so on, are also allowed inside a transaction. 
- In particular, **remote procedure calls (RPCs)**, that is, procedure calls to remote servers, are often also encapsulated in a transaction, leading to what is known as a **transactional RPC**.
##### Transaction primitives

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
##### All or Nothing Property
`BEGIN_TRANSACTION` and `END_TRANSACTION` are used to delimit the scope of a transaction. 
- The operations between them form the body of the transaction. 
- The characteristic feature of a transaction is _either all of these operations are executed or none are executed_. 
	- These may be system calls, library procedures, or bracketing statements in a language, depending on the implementation. 
- This all-or-nothing property of transactions is one of the four characteristic properties that transactions have. 

More specifically, transactions adhere to the so-called ACID properties:
	**Atomic**: happens indivisibly (seemingly)
	**Consistent**: does not violate system in-variants
	**Isolated**: not mutual interference between concurrent transactions
	**Durable**: commit means changes are permanent

In DS, transactions are often constructed as a number of **subtransactions**, jointly forming a nested transaction. 
- The top-level transaction may fork off children that run in parallel with one another, on different machines, to gain performance or simplify programming. 
- Each of these children may also execute one or more subtransactions, or fork off its own children. 
##### Subtransaction issues
Imagine that a transaction starts several subtransactions in parallel, and one of these commits, making its results visible to the parent transaction. 
- After further computation, the parent aborts, restoring the entire system to the state it had before the top-level transaction started. 
- Consequently, the results of the subtransaction that committed must nevertheless be undone. Thus the permanence referred to above applies only to top-level transactions.

Since transactions can be nested arbitrarily deep, considerable administration is needed to get everything right. The semantics are clear, however. 
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire system for it to manipulate_ as it wishes. 
	- If it aborts, its private universe just vanishes, as if it had never existed. 
	- If it commits, its private universe replaces the parent’s universe. 
- Thus if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- Likewise, if an enclosing (higher level) transaction aborts, all its underlying subtransactions have to be aborted as well. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.
##### TPM: Transaction Processing Monitor
In the early days of enterprise middleware systems, the component that handled distributed (or nested) transactions formed the core for integrating applications at the server or database level. 

In many cases, _the data involved in a transaction is distributed across several servers_. 
A TP Monitor is responsible for coordinating the execution of a transaction and the commitment of subtransactions following a standard protocol known as **distributed

Nested transactions are important in DS, naturally providing a way of distributing a transaction across multiple machines, following a logical division of the work of the original transaction. 

For example, a transaction for planning a trip by which three different flights need to be reserved can be logically split up into three subtransactions. 
- Each of these subtransactions can be managed separately and independently of the other two.
![[tpm-in-a-ds.png]]
An important observation is that applications wanting to coordinate several subtransactions into a single transaction did not have to implement this coordination themselves. 

By simply making use of a TP monitor, this coordination was done for them. 
This is exactly where middleware comes into play: 
- it implements services that are useful for many applications avoiding that such services have to be reimplemented over and over again by application developers.
##### Middleware and EAI
Several types of communication middleware exist. 

With remote procedure calls (RPC), an application component can effectively send a request to another application component by doing a local procedure call, which results in the request being packaged as a message and sent to the callee. 

Likewise, the result will be sent back and returned to the application as the result of the procedure call. 
As the popularity of object technology increased, techniques were developed to allow calls to remote objects, leading to what is known as **remote method invocations (RMI)**. An RMI is _essentially the same as an RPC, except that it operates on objects instead of functions_.

Middleware offers communication facilities for application integration (EAIs)
![[middleware-in-eai.png]]

**Remote Procedure Call (RPC):** Requests are sent through local procedure call, packaged as message, processed, responded through message, and result returned as return from call.

RPC and RMI have the _disadvantage_ that the caller and callee both need to be up and running at the time of communication. 

In addition, they need to know **exactly** how to refer to each other. This tight coupling is often experienced as a serious drawback, and has lead to what is known as **message-oriented middleware (MOM)**. 

In this case, applications send messages to logical contact points, often described by means of a subject. 
- Likewise, applications can indicate their interest for a specific type of message, after which the communication middleware will take care that those messages are delivered to those applications. 
- These so-called **publish/subscribe systems** form an important and expanding class of distributed systems.
- Messages are sent to logical contact point (published), and forwarded to subscribed applications.
### 3. Distributed Pervasive Systems (DPS)
Emerging next-generation of distributed systems in which nodes are small, mobile, and often embedded in a larger system, characterised by the fact that the system naturally blends into the user's environment

What makes them unique in comparison to the computing and information systems described so far, is that the separation between users and system components is much more blurred. 

There is often no single dedicated interface, such as a screen/keyboard combination. 
Instead, a pervasive system is often equipped with many **sensors** that pick up various aspects of a user’s behavior. Likewise, it may have a myriad of **actuators** to provide information and feedback, often even purposefully aiming to _steer_ behavior.


- Often coined as the Internet of Things (IoT)
- We often need to deal with the intricacies of wireless and mobile communication, will require special solutions to make a pervasive system as transparent or unobtrusive as possible.
##### Three (overlapping) sub-types:
1. **Ubiquitous computing systems**: pervasive and continuously present, i.e., there is a continuous interaction between system and user.
2. **Mobile computing systems**: pervasive, but emphasis is on the fact that devices are inherently mobile.
3. **Sensor (and actuator) networks**: pervasive, with emphasis on the actual (collaborative) sensing and actuation of the environment.
#### Ubiquitous computing systems
This DPS system is pervasive and continuously present. 
- The latter means that a user will be continuously interacting with the system, often not even being aware that interaction is taking place. 
- Poslad [2009] describes the core requirements for a ubiquitous computing system roughly as follows: 
	1. (**Distribution**) Devices are networked, distributed, and accessible in a transparent manner 
	2. (**Interaction**) Interaction between users and devices is highly unobtrusive 
	3. (**Context awareness**) The system is aware of a user’s context in order to optimize interaction 
	4. (**Autonomy**) Devices operate autonomously without human interven- tion, and are thus highly self-managed 
	5. (**Intelligence**) The system as a whole can handle a wide range of dy- namic actions and interactions 

Let us briefly consider these requirements from a distributed-systems perspective.
###### Distribution
As mentioned, a ubiquitous computing system is an example of a distributed system: the devices and other computers forming the nodes of a system are simply networked and work together to form the illusion of a single coherent system. 

Distribution also comes naturally: 
- there will be devices close to users (such as sensors and actuators), connected to computers hidden from view and perhaps even operating remotely in a cloud. 
- Most, if not all, of the requirements regarding distribution transparency mentioned, should therefore hold.
###### Interaction
When it comes to interaction with users, ubiquitous computing systems differ a lot in comparison to the systems we have been discussing so far; much of the interaction by humans will be **implicit**, with an implicit action being defined as _one that is not primarily aimed to interact with a computerized system but which such a system understands as input_ [Schmidt, 2000]. 

In other words, a user could be mostly unaware of the fact that input is being provided to a computer system, seemingly _hiding interfaces_.

A simple example is where the settings of a car’s driver’s seat, steering wheel, and mirrors is fully personalized. 
- If Bob takes a seat, the system will recognize that it is dealing with Bob and subsequently makes the appropriate adjustments. 
- The same happens when Alice uses the car, while an unknown user will be steered toward making his or her own adjustments (to be remembered for later). 

This example already illustrates an important role of sensors in ubiquitous computing, namely as input devices that are used to identify a situation (a specific person apparently wanting to drive), whose input analysis leads to actions (making adjustments). 

In turn, the actions may lead to natural reactions, for example that Bob slightly changes the seat settings. The system will have to take all (implicit and explicit) actions by the user into account and react accordingly.
###### Context awareness
Reacting to the sensory input, but also the explicit input from users by taking the context in which interactions take place into account. 

Context awareness is described by Dey and Abowd [2000] as _any information that can be used to characterize the situation of entities (i.e., whether a person, place or object) that are considered relevant to the interaction between a user and an application, including the user and the application themselves_. 

In practice, context is often characterized by location, identity, time, and activity: 
- the where, who, when, and what. 
- A system will need to have the necessary (sensory) input to determine one or several of these context types. 
- Raw data as collected by various sensors is lifted to a level of abstraction that can be used by applications. 

A concrete example is detecting where a person is, for example in terms of GPS coordinates, and subsequently mapping that information to an actual location, such as the corner of a street, or a specific shop or other known facility. 

The question is where this processing of sensory input takes place: 
- is all data collected at a central server connected to a database with detailed information on a city, or is it the user’s smartphone where the mapping is done? 
- Clearly, there are trade-offs to be considered.

When it comes to combining flexibility and potential distribution, so-called shared data spaces in which processes are decoupled in time and space are attractive, yet, suffer from scalability problems. 
###### Autonomy
An important aspect of most ubiquitous computing systems is that explicit systems management has been reduced to a minimum. 

In a ubiquitous computing environment there is simply no room for a systems administrator to keep everything up and running. 
- As a consequence, the system as a whole should be able to act autonomously, and automatically react to changes. 

This requires a myriad of techniques; to give a few simple examples, think of the following:
- **Address allocation**
	- In order for networked devices to communicate, they need an IP address. Addresses can be allocated automatically using protocols like the Dynamic Host Configuration Protocol (DHCP) [Droms, 1997] (which requires a server) or Zeroconf [Guttman, 2001]. 
- **Adding devices**
	- It should be easy to add devices to an existing system. A step towards automatic configuration is realized by the Universal Plug and Play Protocol (UPnP) [UPnP Forum, 2008].
	- Using UPnP, devices can discover each other and make sure that they can set up communication channels between them. 
- **Automatic updates
	- Many devices in a ubiquitous computing system should be able to regularly check through the Internet if their software should be updated. 
	- If so, they can download new versions of their components and ideally continue where they left off.
Manual intervention is to be kept to a minimum.
###### Intelligence
FinallyPoslad [2009] mentions that ubiquitous computing systems often use methods and techniques from the field of artificial intelligence. 

In many cases, a wide range of advanced algorithms and models need to be deployed to handle incomplete input, quickly react to a changing environment, handle unexpected events, and so on. 

The extent to which this can or should be done in a distributed fashion is crucial from the perspective of distributed systems. Unfortunately, distributed solutions for many problems in the field of artificial intelligence are yet to be found, meaning that there may be a natural tension between the first requirement of networked and distributed devices, and advanced distributed information processing.
#### Mobile Computing Systems
Mobility often forms an important component of pervasive systems, and many, if not all aspects that we have just discussed also apply to mobile computing. There are several issues that set mobile computing aside to pervasive systems in general (see also Adelstein et al. [2005] and Tarkoma and Kangasharju [2009]). 

First, the devices that form part of a (distributed) mobile system may vary widely. Typically, mobile computing is now done with devices such as smartphones and tablet computers. 

However, completely different types of devices are now using the Internet Protocol (IP) to communicate, placing mobile computing in a different perspective. 
- Such devices include remote controls, pagers, active badges, car equi ment, various GPS-enabled devices, and so on. 
- A characteristic feature of al  these devices is that they use wireless communication. Mobile implies wireless so it seems (although there are exceptions to the rules).

Second, in mobile computing the location of a device is assumed to change over time. 
A changing location has its effects on many issues. For example, if the location of a device changes regularly, so will perhaps the services that are locally available. 
- As a consequence, we may need to pay special attention to dynamically discovering services, but also letting services announce their presence. 
- In a similar vein, we often also want to know where a device actually is and may need the actual geographical coordinates of a device such as in tracking and tracing applications, but it may also require that we are able to simply detect its network position (as in mobile IP [Perkins, 2010; Perkins et al., 2011]. 

Changing locations also has a profound effect on communication. To illustrate, consider a (wireless) mobile ad hoc network, generally abbreviated as a MANET. 
- Suppose that two devices in a MANET have discovered each other in the sense that they know each other’s network address. 
- How do we route messages between the two? 
	- Static routes are generally not sustainable as nodes along the routing path can easily move out of their neighbor’s range, invalidating the path. 
- For large MANETs, using a priority set-up paths is not a viable option. 

What we are dealing with here are so-called disruption-tolerant networks: 
- networks in which connectivity between two nodes can simply not be guaranteed. 
- Getting a message from one node to another may then be problematic, to say the least.

The trick in such cases, is not to attempt to set up a communication path from the source to the destination, but to rely on two principles. 
- First, using special flooding-based techniques will allow a message to gradually spread through a part of the network, to eventually reach the destination. 
	- Obviously, any type of flooding will impose redundant communication, but this may be the price we have to pay. 
- Second, in a disruption-tolerant network, we let an intermediate node store a received message until it encounters another node to which it can pass it on. 
	- In other words, a node becomes a temporary carrier of a message, as sketched in (See Figure). Eventually, the message should reach its destination.

_Passing messages in a (mobile) disruption-tolerant network_
![[passing-messages-disruption-tolerant-network.png]]

It is not difficult to imagine that selectively passing messages to encountered nodes may help to ensure efficient delivery. For example, if nodes are known to belong to a certain class, and the source and destination belong to the same class, we may decide to pass messages only among nodes in that class. Likewise, it may prove efficient to pass messages only to well-connected nodes, that is, nodes who have been in range of many other nodes in the recent past. An overview is provided by Spyropoulos et al. [2010].
#### Sensor networks 
They are more than just a collection of input devices; instead, sensor nodes often collaborate to efficiently process the sensed data in an application-specific manner, making them very different from, for example, traditional computer networks. 

Akyildiz et al. [2002] and Akyildiz et al. [2005] provide an overview from a networking perspective. 
A more systems-oriented introduction to sensor networks is given by Zhao and Guibas [2004], but also Karl and Willig [2005] will show to be useful. 

A sensor network generally consists of tens to hundreds or thousands of relatively small nodes, each equipped with one or more sensing devices. In addition, nodes can often act as actuators [Akyildiz and Kasimoglu, 2004], a typical example being the automatic activation of sprinklers when a fire has been detected. 

Many sensor networks use wireless communication, and the nodes are often battery powered. 
- Their limited resources, restricted communication capabilities, and constrained power consumption demand that efficiency is high on the list of design criteria. 
- When zooming into an individual node, we see that, conceptually, they do not differ a lot from “normal” computers: 
	- above the hardware there is a software layer akin to what traditional operating systems offer, including low-level network access, access to sensors and actuators, memory management, and so on. 
- Normally, support for specific services is included, such as localization, local storage (think of additional flash devices), and convenient communication facilities such as messaging and routing. 
- However, similar to other networked computer systems, additional support is needed to effectively deploy sensor network applications. In distributed systems, this takes the form of middleware. 

For sensor networks, instead of looking at middleware, it is better to see what kind of programming support is provided, which has been extensively surveyed by Mottola and Picco [2011]. 
- One typical aspect in programming support is the scope provided by communication primitives. 
- This scope can vary between addressing the physical neighborhood of a node, and providing primitives for systemwide communication. In addition, it may also be possible to address a specific group of nodes. 
	- Likewise, computations may be restricted to an individual node, a group of nodes, or affect all nodes. To illustrate, Welsh and Mainland [2004] use so-called abstract regions allowing a node to identify a neighborhood from where it can, for example, gather information:
	```cpp
	region = k_nearest_region.create(8);
	reading = get_sensor_reading();
	region.putvar(reading_key, reading);
	max_id = region.reduce(OP_MAXID, reading_key);
	```
In line 1, a node first creates a region of its eight nearest neighbors, after which it fetches a value from its sensor(s). This reading is subsequently written to the previously defined region to be defined using the key `reading_key`. In line 4, the node checks whose sensor reading in the defined region was the largest, which is returned in the variable `max_id`.

As another related example, consider a sensor network as implementing a distributed database, which is, according to Mottola and Picco [2011], one of four possible ways of accessing data. 
- This database view is quite common and easy to understand when realizing that many sensor networks are deployed for measurement and surveillance applications [Bonnet et al., 2002]. 
- In these cases, an operator would like to extract information from (a part of) the network by simply issuing queries 

To organize a sensor network as a distributed database, there are essentially two extremes:
- First, sensors do not cooperate but simply send their data to a centralized database located at the operator’s site. 
- The other extreme is to forward queries to relevant sensors and to let each compute an answer, requiring the operator to aggregate the responses. 
- Neither of these solutions is very attractive. 
##### Organizing a sensor network database, while storing and processing data (a) only at the operator’s site or (b) only at the sensors.
![[sensor-network-db-on-site-vs-at-sensors.png]]
(a) The first one requires that sensors send all their measured data through the network, which may waste network resources and energy. 
(b) The second solution may also be wasteful as it discards the aggregation capabilities of sensors which would allow much less data to be returned to the operator. 

What is needed are facilities for in-network data processing, similar to the previous example of abstract regions. In-network processing can be done in numerous ways. 
- One obvious one is to forward a query to all sensor nodes along a tree encompassing all nodes and to subsequently aggregate the results as they are propagated back to the root, where the initiator is located. 
- Aggregation will take place where two or more branches of the tree come together. As simple as this scheme may sound, it introduces difficult questions: 
	- How do we (dynamically) set up an efficient tree in a sensor network?
	- How does aggregation of results take place? Can it be controlled?
	- What happens when network links fail? 
- These questions have been partly addressed in TinyDB, which implements a declarative (database) interface to wireless sensor networks [Madden et al., 2005]. 
- In essence, TinyDB can use any tree-based routing algorithm.

An intermediate node will collect and aggregate the results from its children, along with its own findings, and send that toward the root. 
- To make matters efficient, queries span a period of time allowing for careful scheduling of operations so that network resources and energy are optimally consumed. 

However, when queries can be initiated from different points in the net- work, using single-rooted trees such as in TinyDB may not be efficient enough. 
- As an alternative, sensor networks may be equipped with special nodes where results are forwarded to, as well as the queries related to those results. 

To give a simple example, queries and results related to temperature readings may be collected at a different location than those related to humidity measurements. 
- This approach corresponds directly to the notion of publish/subscribe systems.

**Features**
- A myriad of different mobile devices (smartphones, tablets, GPS devices, remote controls)
- Mobile implies that a device's location is expected to change over time, change of local services, reachability

Keyword: **discovery**
- Communication may become more difficult: no stable route, (possibly) no guaranteed connectivity 
- disruption-tolerant networking.

**Bottom line**
Mobile devices set up connections to stationary servers, essentially bringing mobile computing in the position of clients of cloud-based services.
- Mobile cloud computing
- Mobile edge computing
# Old
## 2. Types of Distributed Systems Pt.2 - *Chapter 1*
There are three types of DS that serve distinct purposes.
### 1. High performance distributed computing systems (HPDCS)
#### Distributed Shared Memory systems
High-performance distributed computing started with parallel computing
- Multiprocessor and multi-core versus multi-computer 
**(CONTEXT)** Distributed Shared Memory systems vs Multiprocessors
- Multiprocessors are easy to program in comparison to multi-computers, yet have problems when increasing the number of processors (or cores). 
- Distributed shared memory could never compete with that of multiprocessors, and failed to meet the expectations of programmers; they have been widely abandoned now.
**(RESULT)**  A shared-memory model on top of a multi-computer via virtual-memory techniques. 
- Map all main-memory pages (from different processors) into one single virtual address space. 
- If a process at processor A addresses a page P located at processor B, the OS at A traps and fetches P from B, just as it would if P had been located on local disk. 
#### Cluster Computing
Cluster computing systems became popular when the price/performance ratio of personal computers and workstations improved. It made economical sense to build a supercomputer using off-the-shelf technology by simply hooking up a collection of relatively simple computers in a high-speed network. 
- Mostly used for parallel programming in which a single (compute intensive) program is run in parallel on multiple machines.
- Essentially a group of high-end systems connected through a Local Area Network (LAN)
- Homogeneous: same OS, near-identical hardware
- Single managing node running the OS
**(EXAMPLE)** Linux-based Beowulf clusters
![[cluster-computing-system-ex.png\|400]]
Each cluster is a collection of mostly-identical COMPUTE nodes that are controlled and accessed by means of a single MASTER node. 
- This node handles the allocation of nodes to a particular parallel program, maintains a batch queue of submitted jobs, and provides an interface for the users of the sys. 
- As such, the master node actually runs the *middleware* needed for the execution of programs and management of the cluster, while the compute nodes are equipped with a standard OS extended with typical middleware functions for communication, storage, fault tolerance, etc.
#### Grid Computing
The next step along in development is it have lots of nodes from any and everywhere.
These DSs are often constructed as a federation of computer systems, each may fall under a different admin domain: hardware, software and network technology could differ.
- Enables flexible, secure and coordinated resource sharing among dynamic collections of individuals, institutions, and resources.
- Enable communities "Virtual orgs" to share geographically distributed resources.

There is an emerging trend of a more hybrid architecture whereby nodes are configured specifically for certain tasks. This is even more prevalent in grid computing. 
- The processes belonging to the same virtual org have access rights to the resources that are provided to that org. 
- Typically, resources consist of compute servers (including supercomputers, possibly implemented as cluster computers), storage facilities, and dbs. 
- In addition, special networked devices such as telescopes, sensors, etc., can be provided.

Most grid-computing software focuses on managing access to different admin domains, and apps to users and virtual orgs. This outlines the architectural issues.
![[grid-computing-systems-proposed-arch-ex.png\|200]]
Typically the collective, connectivity, and resource layer form the heart of what could be called a GRID MIDDLEWARE LAYER, jointly providing access to and management of resources that are potentially dispersed across multiple sites; they act as a site (or admin) unit.
	This prevalence is emphasised by the gradual shift toward a service-oriented architecture in 
which sites offer access to the various layers through a collection of Web services.
This defines an alternative architecture: Open Grid Services Architecture (OGSA), which is based upon a standardised version of the original architecture that follows Web service standards. 
#### Cloud Computing
From the perspective of grid computing, a next logical step is to simply outsource the entire infrastructure that is needed for compute-intensive apps. 
> (1) Cloud computing is an information technology infrastructure in which computing resources are *virtualised* and accessed as a *service*.

Provides the facilities to dynamically construct an infrastructure and compose what is needed from available services. Different to grid computing, which is associated with HPC.
> (2) Cloud computing is characterised by an easily usable and accessible pool of _virtualised_ resources. 

Usage of resources can be configured dynamically, providing the basis for scalability:
- if more work needs to be done, a customer can simply acquire more resources. 
- This links to utility computing because cloud computing is generally based on a pay-per-use model in which guarantees are offered by means of customised service level agreements (SLAs).
**(CONTEXT)** Researchers wanted to organise computational grids that were easily accessible and orgs running data centers wanted to open-up their resources to customers. 
	This lead to UTILITY computing: a customer could upload tasks to a data centre and be
charged on a per-resource basis, forming the basis of cloud computing.

The cloud has four layers:
![[the-organisation-of-clouds.png\|400]]
- Generally implemented at data centres.
- Processors, routers, power and cooling systems- invisible to customers
**(2) Infrastructure**: Provides customer a mix of virtual storage and computing resources.
- Deploys virtualisation techniques. 
- Evolves around allocating and managing virtual storage devices and virtual servers
**(3) Platform**: Provides higher-level abstractions for storage and such. 
- An OS to app devs: easy to develop and deploy apps that need to run in a cloud. 
- In practice, an app developer is offered a vendor-specific API, which includes calls to uploading and executing a program in that vendor’s cloud. 
**(4) app**: apps themselves run here, e.g., text processors, presentation apps.
- It is important to realise that these apps are executed in the VENDOR’s cloud.
- Comparable to the suite of apps shipped with OSes.

**(PROVISION)** These layers are provided through various interfaces (including CLI tools, programming and Web interfaces), leading to three different types of services:
- **Infrastructure-as-a-Service** (IaaS) covering the hardware and infrastructure layer
- **Platform-as-a-Service** (PaaS) covering the platform layer
- **Software-as-a-Service** (SaaS) in which their apps are covered
Cloud computing outsources local computing infrastructures but still has its obstacles including *provider lock-in*, *security* and *privacy* issues, and *dependency on the availability* of services.
### 2. Distributed information systems
#### Context
Many of the existing middleware solutions are the result of working with an infrastructure in which it was easier to integrate apps into an enterprise-wide information sys. 
Organisations had *many* *networked apps* but struggled to achieve interoperability.

**(INTEGRATION)** A networked app simply of a server running that app (often including a db) and making it available to remote programs, called *clients*. 
- Clients send a request to the server for executing a specific operation and a response returned. 
- Integration at the lowest level allows clients to wrap a number of requests, possibly for different apps or servers, into a single larger request and have it executed as a distributed transaction.
- The key idea is that all, or none of the requests are executed. 
- As apps became more sophisticated and were separated into independent components (notably distinguishing db components from processing components), apps needed to communicate directly with each other, leading to *Enterprise app Integration (EAI)*
#### Transactions
Database apps where operations are carried out in the form of TRANSACTIONS.
- In a mail sys, there might be primitives to send, receive, and forward mail. 
- In an accounting sys, they might be quite different. READ and WRITE are typical examples, however. Ordinary statements, procedure calls, and so on, are also allowed inside a transaction. 
- In particular, **remote procedure calls (RPCs)**, that is, procedure calls to remote servers, are often also encapsulated in a transaction, leading to what is known as a **transactional RPC**.

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
The operations between them form the body of the transaction. 
- The characteristic feature of a transaction is _either all of these operations are executed or none are executed_. 
	- These may be sys calls, library procedures, or bracketing statements in a language, depending on the implementation. 
- This all-or-nothing property of transactions is one of the four characteristic properties that transactions have. 

More specifically, transactions adhere to the so-called ACID properties:
**Atomic**: happens indivisibly (seemingly)
**Consistent**: does not violate sys in-variants
**Isolated**: not mutual interference between concurrent transactions
**Durable**: commit means changes are permanent

**(SUB-TRANSACTION)** join to form a nested transaction. 
- The top-level transaction may fork off children that run in parallel with one another, on different machines, to gain performance or simplify programming. 
- Each child may also execute one or more sub-transactions, or fork off its own children. 
**(ISSUES)** Imagine a transaction starts several subs in parallel, and one commits, making its results visible to the parent transaction. 
- After further computation, the parent aborts, restoring the entire sys to the state it had before the top-level transaction started. 
- Consequently, the results of the sub that committed must be undone. Thus, the permanence referred to above applies only to top-level transactions.

Since transactions can be nested arbitrarily deep, considerable administration is needed to get everything right. The semantics are clear, however. 
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire sys for it to manipulate_ as it wishes. 
	- If it aborts, its private universe just vanishes, as if it had never existed. 
	- If it commits, its private universe replaces the parent’s universe. 
- Thus if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- Likewise, if an enclosing (higher level) transaction aborts, all its underlying subtransactions have to be aborted as well. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.
**(TRANSACTION PROCESSING MONITOR)**  The *data* involved in a transaction is *distributed across several servers*. 
A TP Monitor is responsible for coordinating the execution of a transaction and the commitment of subtransactions following a standard protocol known as distributed.

Nested transactions are important in DS, naturally providing a way of distributing a transaction across multiple machines, following a logical division of the work of the original transaction. 

For example, a transaction for planning a trip by which three different flights need to be reserved can be logically split up into three subtransactions. 
- Each of these subtransactions can be managed separately and independently of the other two.
![[tpm-in-a-ds.png\|400]]
Apps wanting to coordinate several subtransactions into a single transaction did not have to implement this coordination themselves because a TP monitor does this for them. 
This is exactly where MIDDLEWARE comes into play: 
- it implements services that are useful for many apps avoiding that such services have to be reimplemented over and over again by app developers.

**(Middleware and EAI)** Several types of communication middleware exist. 
With remote procedure calls (RPC), an app component can effectively send a request to another app component by doing a local procedure call, which results in the request being packaged as a message and sent to the callee. 

The result will be sent back and returned to the app as the result of the procedure call. 
Later, techniques were developed to allow calls to remote objects, leading to what is known as **remote method invocations (RMI)**. An RMI is _essentially the same as an RPC, except that it operates on objects instead of functions_.

Middleware offers communication facilities for app integration (EAIs)
![[middleware-in-eai.png\|300]]
**Remote Procedure Call (RPC):** Requests are sent through local procedure call, packaged as message, processed, responded through message, and result returned as return from call.

RPC and RMI have the _disadvantage_ that the caller and callee both need to be up and running at the time of communication. 

In addition, they need to know **exactly** how to refer to each other. This **tight coupling** is often experienced as a serious drawback, and has lead to what is known as **message-oriented middleware (MOM)**. 

Here, apps send messages to logical contact points, described by means of a subject. 
- Likewise, apps can indicate their interest for a specific type of message, after which the communication middleware will take care that those messages are delivered to those apps. 
- These so-called **publish/subscribe systems** form an important and expanding class of DS.
- Messages are sent to logical contact point (published), and forwarded to subscribed apps.
### 3. Distributed Systems for Pervasive computing (DPS)
Next for DS where nodes are small, mobile, and often embedded in a larger sys, characterised by the fact that the sys *naturally blends into the user's env*. Coined as the IoT.
- We often need to deal with the intricacies of wireless and mobile communication, will require special solutions to make a pervasive sys as transparent or unobtrusive as possible.

They're unique because the separation between users and sys components is more blurred. 
There is often no single dedicated interface, such as a screen/keyboard combination. 
Instead, a **pervasive** sys is often equipped with many **sensors** that pick up various aspects of a user’s behaviour. Likewise, it may have a myriad of **actuators** to provide information and feedback, often even purposefully aiming to _steer_ behaviour.
#### Ubiquitous Computing Systems (UCS)
Pervasive and continuously present, i.e., a continuous interaction between sys and user often with the user being unaware that the interaction is happening.
Core requirements for a UCS roughly as follows: 
1. (**Distribution**) Devices are networked, distributed, and accessible in a transparent manner 
2. (**Interaction**) Interaction between users and devices is highly unobtrusive, hiding interfaces.
Much of the interaction by humans will be **implicit**: _one that is not primarily aimed to interact with a computerised sys but which such a sys understands as input_.
**(Ex)** The settings of a car’s driver’s seat, steering wheel, and mirrors is fully personalised. 
If Bob takes a seat, the sys will recognise that it is dealing with Bob and subsequently makes the appropriate adjustments. 
3. (**Context awareness**) The sys is aware of a user’s context in order to optimise interaction.
_any information that can be used to characterise the situation of entities (i.e., whether a person, place or object) that are considered relevant to the interaction between a user and an app, including the user and the app themselves_. 

In practice, context is often characterised by location, identity, time, and activity: 
- the where, who, when, and what. 
- A sys will need to have the necessary (sensory) input to determine one or several of these context types. 
- Raw data as collected by various sensors is lifted to a level of abstraction that can be used by apps. 

A concrete example is detecting where a person is, for example in terms of GPS coordinates, and subsequently mapping that information to an actual location, such as the corner of a street, or a specific shop or other known facility. 

The question is where this processing of sensory input takes place: 
- is all data collected at a central server connected to a db with detailed information on a city, or is it the user’s smartphone where the mapping is done? 
- When it comes to combining flexibility and potential distribution, so-called shared data spaces in which processes are decoupled in time and space are nice, yet, suffer from scalability problems. 
4. (**Autonomy**) Devices operate autonomously without human interven- tion, and are thus highly self-managed 
In a UCS env, the sys admin cannot keep everything up and running. 
The sys should act autonomously, and automatically react to changes using many techniques:
- **Address allocation**
	- In order for networked devices to communicate, they need an IP address. Addresses can be allocated automatically using protocols like the Dynamic Host Configuration Protocol (DHCP) (which requires a server) or Zeroconf. 
- **Adding devices**
	- It should be easy to add devices to an existing sys. A step towards automatic configuration is realised by the Universal Plug and Play Protocol (UPnP).
	- Using UPnP, devices can discover each other and make sure that they can set up communication channels between them. 
- **Automatic updates
	- Many devices in a UCS should be able to regularly check the Internet software updates
	- Manual intervention is to be kept to a minimum
4. (**Intelligence**) The sys as a whole can handle a wide range of dynamic actions and interactions.
UCS often use methods and techniques from the field of artificial intelligence. 
In many cases, a wide range of advanced algorithms and models need to be deployed to handle incomplete input, quickly react to a changing environment, handle unexpected events, and so on. 
#### Mobile computing systems 
Pervasive, but emphasises that devices are inherently mobile.
A DS where devices can change physical location and network attachment points while maintaining connectivity, often forming a key part of **pervasive computing**.

* **Device Heterogeneity:** Encompasses a wide range of **IP-enabled devices** beyond smartphones/tablets (e.g., IoT sensors, wearables, vehicular systems).
* **Wireless Communication:** The primary enabler of mobility (*Mobile typically implies wireless*).
* **Dynamic Location:** A device's **network location** (IP address/point of attachment) and **physical location** change over time. This is the central challenge.
![[mobile-comp-ex.png\|200]]
**A. Service Discovery & Availability**
* **Problem:** As devices move, the set of locally available services changes.
* **Required Mechanism:** **Dynamic service discovery** (services announce presence, clients discover them). This is crucial for context-aware apps.
*   *Example:* Your smartphone automatically discovering and connecting to a nearby wireless printer when you enter an office, but not seeing it at home.
**B. Location Management**
*   **Geographical Location:** Needed for **tracking/tracing** (e.g., GPS in a delivery vehicle).
*   **Network Location:** Needed for routing. Solved by protocols like **Mobile IP**, which separates a device's permanent **home address** from its temporary **care-of address**, allowing seamless movement between networks.
**C. Communication in Dynamic Networks: MANETs & DTNs**
*   **MANET (Mobile Ad-hoc Network):** A self-configuring network of mobile devices connected wirelessly without fixed infrastructure.
    *   **Core Routing Problem:** **Static routes fail** as nodes move. Requires **proactive** (e.g., DSDV) or **reactive** (e.g., AODV) routing protocols.
*   **DTN (Disruption/Disconnection-Tolerant Network):** Assumes **intermittent connectivity** and no guaranteed end-to-end path.

DTN Routing Principles (Store-Carry-Forward).
When a continuous path cannot be established, DTNs use a **store-carry-forward** paradigm:
1.  **Store:** A node receives and **stores** a message.
2.  **Carry:** The node physically **carries** the message as it moves.
3.  **Forward:** When it finds another node, it may **forward** the message based on a chosen strategy.
_Passing messages in a (mobile) disruption-tolerant network_
![[passing-messages-disruption-tolerant-network.png\|300]]
**Routing Strategies:**
*   **Flooding-Based (e.g., Epidemic Routing):** Messages spread redundantly. High delivery chance but high resource cost.
*   **Selective Forwarding:** More efficient. Decisions based on:
    *   **Utility/Probability:** Forward to nodes with higher historical probability of contacting the destination.
    *   **Social Patterns:** Forward to nodes belonging to the same "community" (**social-based routing**).
    *   **Connectivity:** Forward to **well-connected nodes** (central hubs in the mobility pattern).
*   ***Technical Example:** A wildlife tracker on a zebra (Node A) needs to send data to the ranger station (Node D). There's no direct path. The zebra moves and encounters a vulture's tracker (Node B). Using a **utility-based** protocol, Node A forwards the data to Node B. The vulture flies and later lands near a fixed sensor node (Node C) connected to the station, finally delivering the data. The network tolerated long delays and disconnections.*
Mobile computing shifts the design focus from stable connectivity and fixed locations to **mobility management, discovery, and tolerant routing** in the face of constant change and potential disconnection. Mobile devices set up connections to stationary servers, bringing mobile computing in the position of clients of cloud-based services.
#### Sensor (and actuator) networks: 
Pervasive, with emphasis on the actual (collaborative) sensing and actuation of the env.
Here is the content reformatted as requested, with headings prepended to the paragraphs and the "Features" section integrated.

**(SENSOR NETWORKS)** They are more than just a collection of input devices; instead, sensor nodes often collaborate to efficiently process the sensed data in an app-specific manner, making them very different from, for example, traditional computer networks. A sensor network generally consists of tens to hundreds or thousands of relatively small nodes, each equipped with one or more sensing devices. In addition, nodes can often act as actuators, a typical example being the automatic activation of sprinklers when a fire has been detected.

**(KEY CHARACTERISTICS)** Many sensor networks use wireless communication, and the nodes are often battery powered. Their limited resources, restricted communication capabilities, and constrained power consumption demand that efficiency is high on the list of design criteria.

**(NODE ARCHITECTURE)** When zooming into an individual node, we see that, conceptually, they do not differ a lot from “normal” computers: above the hardware there is a software layer akin to what traditional operating systems offer, including low-level network access, access to sensors and actuators, and memory management. Normally, support for specific services is included, such as localisation, local storage (think of additional flash devices), and convenient communication facilities such as messaging and routing. However, similar to other networked computer systems, additional support is needed to effectively deploy sensor network apps. In DSs, this takes the form of middleware.

**(PROGRAMMING MODELS & COMMUNICATION SCOPE)** For sensor networks, instead of looking at middleware, it is better to see what kind of programming support is provided. One typical aspect in programming support is the scope provided by communication primitives. This scope can vary between addressing the physical neighbourhood of a node, and providing primitives for sys-wide communication. In addition, it may also be possible to address a specific group of nodes. Likewise, computations may be restricted to an individual node, a group of nodes, or affect all nodes.
*   *Example (Abstract Regions):* A node can define a region of its eight nearest neighbours. It can write a sensor reading to that region, and then perform an operation like finding which node in the region has the maximum reading, enabling local, collaborative processing.

**(DATABASE VIEW & DATA ACCESS)** As another related example, consider a sensor network as implementing a distributed db. This view is common as many networks are deployed for measurement and surveillance, where an operator extracts information by issuing queries.

**(DATA PROCESSING ARCHITECTURES)** To organise a sensor network as a distributed db, there are essentially two extremes:
1.  **Centralised Processing:** Sensors send all raw data to a central operator's site. This wastes network resources and energy.
2.  **Purely Edge Processing:** Queries are sent to sensors, each computes an answer, and the operator aggregates all responses. This wastes the opportunity for sensors to aggregate data locally, reducing the volume of traffic.

**(IN-NETWORK PROCESSING)** What is needed are facilities for **in-network data processing**. One common method is to use a **routing tree**. A query is propagated down the tree to all nodes, and results are **aggregated** (e.g., summed, averaged, filtered) at intermediate nodes as they are passed back up to the root (the operator). This conserves energy and bandwidth.
*   *Example (TinyDB):* Implements a declarative SQL-like interface. It uses a tree-based routing algorithm where parent nodes schedule data collection from children and aggregate results before forwarding them upward.

**(PUBLISH/SUBSCRIBE MODEL)** An alternative to a single, query-specific tree is a **publish/subscribe** model. Nodes publish specific types of data (e.g., temperature readings) which are forwarded to designated **broker nodes**. Queries for that data type are then sent to the corresponding broker. This decouples data producers from consumers.

**(FEATURES)** Sensor networks are characterised by:
*   **Collaborative Processing:** Nodes work together to process data.
*   **Severe Resource Constraints:** Limited battery, CPU, memory, and communication bandwidth.
*   **Application-Specific Design:** The network architecture is tightly coupled to its sensing task.
*   **Wireless Communication & Ad-hoc Deployment.**
*   **In-Network Processing:** Computation occurs within the network to reduce data volume and save energy.
*   **Diverse Programming Models:** Including the db abstraction (e.g., TinyDB) and region-based programming.
**(BOTTOM LINE)** While mobile devices often act as clients to cloud services (**mobile cloud/edge computing**), sensor networks represent a different paradigm: a massively distributed, collaborative, and deeply embedded sys where processing is pushed into the network itself to overcome severe resource limitations and achieve scalability.
![[cloud-edge-continuum.png\|400]]
**(WHAT IS GOOGLE SPANNER?)** Google Spanner is a globally distributed, strongly consistent, relational db service. Its unique achievement is combining the horizontal scalability of NoSQL systems with **Strong ACID transactions** and an SQL interface across data centers worldwide. It was first described in 2012 and is used internally for services like Google Ads, Gmail, and Google Photos before being offered as Cloud Spanner

**(CORE INNOVATIONS)** It works by automatically sharding data across many Paxos-based replication groups for availability. Its most significant innovation is **TrueTime**, a globally synchronised clock API that uses GPS and atomic clocks to bound clock uncertainty across servers. TrueTime is the key that enables its powerful consistency properties without sacrificing performance globally.

**(CONSISTENCY PROPERTIES)** By default, Spanner provides **External Consistency**, which is the strictest isolation level for transaction-processing systems.
It has two formal guarantees:
- **Strict Serializability**: The sys behaves as if all transactions executed one at a time in some order (**serializability**), and that order respects real-time: if transaction T2 starts _after_ T1 commits, T2 must appear to execute _after_ T1 in the serial order.
- **External Consistency**: This adds a real-time ordering guarantee for transactions _observed_ to commit in sequence. If T2's commit process begins after T1's commit _returns successfully_, T2 is guaranteed to have a later commit timestamp than T1. This prevents anomalies like seeing a withdrawal before a deposit that visibly completed earlier. External consistency is a stronger property than both linearizability (for single operations) and basic strong consistency

**(HANDLING ACID TRANSACTIONS)** Spanner provides full ACID guarantees globally
This is achieved through a specific architecture:
- **Atomicity & Durability**: Uses a **Paxos-based synchronous replication** across every data shard. A write must be accepted by a majority of replicas before it is committed, ensuring durability. The two-phase commit protocol coordinates commits across different shards involved in a transaction
- **Isolation & Consistency**: Primarily uses **Multi-Version Concurrency Control (MVCC)**. Writes create new, timestamped versions of data
- TrueTime assigns globally meaningful, monotonically increasing timestamps to commits, establishing the externally consistent serial order. **Read-write transactions** (for reads and writes) use locking and get external consistency. **Read-only transactions** are executed at a chosen timestamp (system-chosen for strong reads, past for stale reads) without acquiring locks, reading a consistent snapshot.
- **Alternative Isolation Level**: Spanner also offers **Repeatable Read (Snapshot) Isolation**, which can improve performance for read-heavy, low-conflict workloads. Unlike the default, it is susceptible to **write skew**, so apps must use locking reads (`FOR UPDATE`) for correctness in critical sections.
# New

## 2. Types of Distributed Systems Pt.2 - *Chapter 1*
There are three types of DS that serve distinct purposes.
### 1. High performance distributed computing systems (HPDCS)
These systems focus on delivering massive computational power for complex tasks like scientific simulations and large-scale data processing.
#### Distributed Shared Memory systems
Early HPDCS aimed to mimic the programming ease of multiprocessors (shared memory) across multiple independent computers.
- **Concept:** Create a single, virtual shared address space across physically separate machines using virtual-memory page-faulting techniques. If a process accesses a "page" located on another machine, the system fetches it transparently.
- **Outcome:** This model largely failed to compete with true multiprocessors on performance and did not meet programmer expectations, leading to its decline.
#### Cluster Computing
Cluster computing became popular by connecting many **off-the-shelf, commodity computers** with a high-speed network to create a cost-effective supercomputer.
- **Characteristics:** **Homogeneous** hardware/OS, controlled by a single **master node**, used primarily for **parallel programming** (running a single program split across many nodes).
- **Architecture:** A **master node** manages job scheduling, a batch queue, and user access. **Compute nodes** run the actual workloads. The master runs the coordinating **middleware**.  
![[cluster-computing-system-ex.png|400]]
- **Example:** **Linux-based Beowulf clusters**.
#### Grid Computing
Grid computing federates resources (computers, storage, special devices) from **different administrative domains** (e.g., universities, labs) into a **Virtual Organization (VO)**.
- **Goal:** Enable flexible, secure, coordinated resource sharing across institutional boundaries.
- **Challenge:** Managing heterogeneity and differing policies.
- **Architecture:** Built around a layered **Grid Middleware** that provides services for resource access, security, and management across sites. It evolved towards **service-oriented architectures** like the **Open Grid Services Architecture (OGSA)** based on web services.  
![[grid-computing-systems-proposed-arch-ex.png|200]]
#### Cloud Computing
Cloud computing is the logical evolution, outsourcing the entire infrastructure and providing resources as an on-demand, pay-per-use **utility**.

>**(1)** An IT infrastructure where computing resources are **virtualised** and accessed as a **service**.  
>**(2)** Characterised by an easily usable pool of **virtualised resources** that can be **dynamically reconfigured (elastic)** and accessed via a **pay-per-use** model with **Service Level Agreements (SLAs)**.

**(THE FOUR LAYERS OF A CLOUD)**  
![[the-organisation-of-clouds.png|400]]
1. **Hardware:** The physical data center (servers, routers, cooling) – invisible to users.
2. **Infrastructure:** Virtualised compute and storage resources (e.g., virtual machines). Provides **Infrastructure-as-a-Service (IaaS)**.
3. **Platform:** Higher-level abstractions, APIs, and services for developers to build and deploy applications. Provides **Platform-as-a-Service (PaaS)**.
4. **Application:** The actual software applications running in the cloud (e.g., Gmail, Office 365). Provides **Software-as-a-Service (SaaS)**.
**(OBSTACLES)** Include **vendor lock-in**, **security/privacy** concerns, and **dependency** on provider availability.
### 2. Distributed information systems
These systems focus on integrating applications and data across an enterprise, evolving from simple client-server models to complex, interoperable middleware.

**(CONTEXT)** Organisations had many independent, networked applications ("islands of automation") that needed to work together, leading to **Enterprise Application Integration (EAI)**. The goal was to achieve **interoperability** between different apps and their data.
#### Transactions
A fundamental concept for ensuring reliable operations across multiple components is the **transaction**.
> A sequence of operations (e.g., `READ`, `WRITE`, `BEGIN_TRANSACTION`) that must **all succeed or all fail** (the "all-or-nothing" property).
 
**ACID Properties:** Transactions guarantee:
- **Atomicity:** Indivisible execution.
- **Consistency:** Leaves the system in a valid state.
- **Isolation:** Concurrent transactions don't interfere.
- **Durability:** Committed changes are permanent.

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
**(SUB-TRANSACTIONS & NESTING)** Transactions can be nested to distribute work across machines. A top-level transaction can spawn parallel subtransactions. If a parent aborts, **all** its committed children must also be undone—**permanence** applies only to **top-level commits**. This provides a clean model for distributed work.
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire sys for it to manipulate_ as it wishes. 
- If it aborts, its private universe vanishes. 
- If it commits, its private universe replaces the parent’s universe. 
- Thus, if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.

**(TRANSACTION PROCESSING (TP) MONITORS)** When transaction data is distributed across servers, a **TP Monitor** is the middleware component that coordinates the execution. It ensures the ACID properties are maintained using protocols (like two-phase commit) across all participating servers.  
![[tpm-in-a-ds.png|400]]
- **Role:** The TP monitor relieves application developers from implementing complex coordination logic.

**(MIDDLEWARE FOR EAI)** Several middleware communication paradigms facilitate integration:  
![[middleware-in-eai.png|300]]
- **Remote Procedure Call (RPC) / Remote Method Invocation (RMI):** Allow a component to call a *procedure*/*method* on a remote component as if it were local. Leads to **tight coupling** (both parties must be running and know each other's exact location). Procedure calls to remote servers that are encapsulated in a transaction, are **transactional RPCs**.
- **Message-Oriented Middleware (MOM) / Publish-Subscribe:** Provides **loose coupling**. Producers **publish** messages to a logical topic/queue. Consumers **subscribe** to topics of interest. The middleware ensures delivery. This decouples components in time and space.
#### Coupling in Distributed Systems: Space vXs. Time

| Coupling Dimension               | Type      | Description                                                                                                                                               | System Characteristic                                                                                                       | Example                                                                                                                                                                                                          | Typical Architecture/Pattern                                            |
| :------------------------------- | :-------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------- |
| **Space (Referential) Coupling** | **Tight** | Components must have **explicit knowledge of each other's identity/location** (e.g., IP address, object reference) to communicate.                        | Components are **directly dependent** on each other's specific interfaces and network addresses. Changes require updates.   | **Client-Server RPC/RMI:** A client must know the server's exact endpoint. **Direct socket communication.**                                                                                                      | Traditional 2/3-Tier, RPC, RMI                                          |
|                                  | **Loose** | Components communicate **indirectly** and need **no explicit knowledge** of each other's identity. They refer to a logical identifier, topic, or channel. | Components are **independent**. The system can add, remove, or replace components without disrupting others.                | **Publish/Subscribe:** A publisher sends a message to a "news" topic; subscribers listen to that topic without knowing the publisher. **Message Queues:** A producer sends to a queue; a consumer takes from it. | Message-Oriented Middleware (MOM), Event-Driven, Shared Data Space      |
| **Time Coupling**                | **Tight** | The sender and receiver **must both be active and available at the same time** for communication to succeed. Communication is **synchronous**.            | Communication is **blocking**. System is sensitive to latency and failures of either party.                                 | **Phone Call, HTTP Request/Response, Synchronous RPC:** The caller blocks and waits for an immediate reply.                                                                                                      | Request-Reply, Synchronous Client-Server                                |
|                                  | **Loose** | The sender and receiver **do not need to be active simultaneously**. Messages can be **intermediated and stored**.                                        | Communication is **asynchronous** and **non-blocking**. Systems are more resilient to temporary failures and variable load. | **Email, Message Queue, Persistent Pub/Sub:** A user sends an email; the recipient reads it hours later.                                                                                                         | Message Queuing, Asynchronous Messaging, Store-and-Forward (e.g., DTNs) |
**(INTERSECTION)** of these two dimensions creates four classic coordination models:

|                     | Temporally Tight                                                                                                                                                                                                    | Temporally Loose                                                                                                                                                                                                                                                        |
| :------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Spatially Tight** | **1. Direct Communication**<br>• **Example:** Phone call, synchronous RPC.<br>• **Model:** Tightest coupling. Common in basic client-server architectures.                                                          | **2. Mailbox Coordination**<br>• **Example:** Email, traditional MPI send/receive to a known process.<br>• **Model:** Referentially coupled (need the mailbox ID), but decoupled in time.                                                                               |
| **Spatially Loose** | **3. Event-Based Coordination**<br>• **Example:** **Publish-Subscribe** with immediate delivery (subscriber must be running).<br>• **Model:** **Referentially decoupled** (use topics), but **temporally coupled**. | **4. Shared Data Space**<br>• **Example:** **Tuple spaces** (e.g., Linda), distributed blackboards, some persistent pub/sub.<br>• **Model:** **Fully decoupled.** Processes communicate by reading/writing to a shared, persistent space without knowing of each other. |
**(TRADE-OFFS)**
* **Tight Coupling (Space & Time):** Leads to systems that are often **simpler to reason about** but are **less flexible, scalable, and fault-tolerant**. They have clear dependencies but single points of failure.
* **Loose Coupling (Space & Time):** Leads to systems that are **more scalable, resilient, and flexible** (components can evolve independently) but are **more complex to design, debug, and ensure consistency** in. This is the dominant trend for large-scale, modern distributed systems (microservices, event-driven architectures, serverless).

The evolution of DS design often involves moving from the **top-left quadrant (Direct Communication)** -> **bottom-right quadrant (Shared Data Space)** to achieve greater scalability and resilience, accepting the complexity that comes with loose coupling.
### 3. Distributed Systems for Pervasive computing (DPS)
These systems are characterized by small, mobile, and embedded devices that blend naturally into the user's environment, often interacting implicitly. The **Internet of Things (IoT)** is a key part of this domain.
#### Ubiquitous Computing Systems (UCS)
Systems that are continuously present and interact unobtrusively with users.
**Core Requirements:**
1. **Distribution:** Networked, accessible devices.
2. **Interaction:** Highly unobtrusive, often **implicit** (the system infers intent from user actions, like adjusting a car seat).
3. **Context Awareness:** The system is aware of the user's situation (**where, who, when, what**) using sensor data (e.g., mapping GPS to a shop location).
4. **Autonomy:** Self-managed with minimal human intervention (e.g., auto IP address allocation via DHCP, device discovery via UPnP, automatic software updates).
5. **Intelligence:** Uses AI techniques to handle dynamic, unpredictable environments and incomplete input.
#### Mobile computing systems
A subset of DPS where devices change location and network attachment (mobile).
- **Key Challenges:** **Dynamic Location**, **Service Discovery**, **Intermittent Connectivity**.
**Networks:**
- **MANETs (Mobile Ad-hoc Networks):** Self-forming wireless networks without infrastructure; require dynamic routing protocols.
- **DTNs (Disruption-Tolerant Networks):** Assume no continuous path; use **store-carry-forward** routing, where messages are physically carried by mobile nodes until they can be forwarded.  
![[passing-messages-disruption-tolerant-network.png|300]]
- **Example Protocol:** **Mobile IP** allows a device to keep a permanent "home address" while using a temporary "care-of address" when roaming.
**(ROUTING STRATEGIES)**
*   **Flooding-Based (e.g., Epidemic Routing):** Messages spread redundantly. High delivery chance but high resource cost.
*   **Selective Forwarding:** More efficient. Decisions based on:
    *   **Utility/Probability:** Forward to nodes with higher historical probability of contacting the destination.
    *   **Social Patterns:** Forward to nodes belonging to the same "community" (**social-based routing**).
    *   **Connectivity:** Forward to **well-connected nodes** (central hubs in the mobility pattern).
* **Technical Example:** A wildlife tracker on a zebra (Node A) needs to send data to the ranger station (Node D). There's no direct path. The zebra moves and encounters a vulture's tracker (Node B). Using a **utility-based** protocol, Node A forwards the data to Node B. The vulture flies and later lands near a fixed sensor node (Node C) connected to the station, finally delivering the data. The network tolerated long delays and disconnections.
#### Sensor (and actuator) networks
Massive networks of small, resource-constrained devices for collaborative sensing and actuation.

- **Key Characteristics:** Severe **resource constraints** (battery, CPU, bandwidth), **application-specific design**, and **collaborative in-network processing** to save energy.
    
- **Programming Models:** Include **database abstractions** (e.g., TinyDB, where queries are aggregated up a tree) and **region-based programming** (nodes collaborate with neighbours).
    
- **Architecture Shift:** Represents a move from centralized cloud processing to **edge intelligence**, where data is processed within the network itself.  
    ![[cloud-edge-continuum.png|400]]
    

**(GOOGLE SPANNER - A CASE STUDY IN GLOBAL CONSISTENCY)**  
Spanner is a globally distributed database that uniquely combines **SQL, ACID transactions, and horizontal scalability**.

- **Core Innovation:** **TrueTime**, a globally synchronized clock API using atomic clocks/GPS, which enables **external consistency**.
    
- **Consistency Model:** Provides **Strict Serializability** (transactions appear serialized) and **External Consistency** (the serial order respects real-time observed commits).
    
- **ACID Implementation:** Uses **synchronous Paxos-based replication** for durability and **Multi-Version Concurrency Control (MVCC)** with TrueTime timestamps for isolation.
    
- **Trade-off:** Also offers a **Repeatable Read** isolation level for performance, which is susceptible to **write skew** and requires careful application design.
    

**Check of Understanding**

> **Question:** Compare and contrast **Cluster Computing**, **Grid Computing**, and **Cloud Computing** across three dimensions: (1) Primary ownership/control of resources, (2) Degree of hardware/software homogeneity, and (3) The main economic model for users. Provide a real-world use case that would be a _poor_ fit for Grid Computing but a _good_ fit for Cloud Computing, and explain why.
> 
> **Answer:**
> 
> |Dimension|Cluster Computing|Grid Computing|Cloud Computing|
> |---|---|---|---|
> |**Ownership/Control**|Single organization (e.g., a university lab).|Multiple independent organizations (a Virtual Organization).|Third-party provider (e.g., AWS, Google).|
> |**Homogeneity**|High. Uses identical, commodity hardware and software.|Low. Heterogeneous resources from different admin domains.|High (within a provider's data centers). Hidden from user by virtualization.|
> |**Economic Model**|Capital expenditure (buying the cluster). Often grant-funded.|Resource sharing/bartering within the VO.|Operational expenditure (pay-per-use utility).|
> 
> **Use Case:** A startup developing a new mobile app that needs to handle unpredictable, rapid growth in user traffic.
> 
> - **Poor for Grid:** A Grid is ill-suited because the startup has no established partnerships in a research VO to "share" resources. Grids are designed for coordinated, pre-planned resource sharing among trusted partners, not for dynamic, commercial, on-demand scaling.
>     
> - **Good for Cloud:** The Cloud is perfect because the startup can use **IaaS/PaaS** to instantly provision resources, scale elastically with user demand, and only pay for what they use. This aligns with the **pay-per-use** model and removes the need for large capital investment or managing physical hardware.
>

Usage of resources can be configured dynamically, providing the basis for scalability:
- if more work needs to be done, a customer can simply acquire more resources. 
- This links to utility computing because cloud computing is generally based on a pay-per-use model in which guarantees are offered by means of customised service level agreements (SLAs).
**(CONTEXT)** Researchers wanted to organise computational grids that were easily accessible and orgs running data centers wanted to open-up their resources to customers. 
	This lead to UTILITY computing: a customer could upload tasks to a data centre and be
charged on a per-resource basis, forming the basis of cloud computing.

The cloud has four layers:
![[the-organisation-of-clouds.png\|400]]
- Generally implemented at data centres.
- Processors, routers, power and cooling systems- invisible to customers
**(2) Infrastructure**: Provides customer a mix of virtual storage and computing resources.
- Deploys virtualisation techniques. 
- Evolves around allocating and managing virtual storage devices and virtual servers
**(3) Platform**: Provides higher-level abstractions for storage and such. 
- An OS to app devs: easy to develop and deploy apps that need to run in a cloud. 
- In practice, an app developer is offered a vendor-specific API, which includes calls to uploading and executing a program in that vendor’s cloud. 
**(4) app**: apps themselves run here, e.g., text processors, presentation apps.
- It is important to realise that these apps are executed in the VENDOR’s cloud.
- Comparable to the suite of apps shipped with OSes.

1. **Hardware:** The physical data center (servers, routers, cooling) – invisible to users. Provides **Infrastructure-as-a-Service (IaaS)**.
2. **Infrastructure:** Virtualised compute and storage resources (e.g., virtual machines). Provides **IaaS** as well.
3. **Platform:** Higher-level abstractions, APIs, and services for developers to build and deploy applications. Provides **Platform-as-a-Service (PaaS)**.
4. **Application:** The actual software applications running in the cloud (e.g., Gmail, Office 365). Provides **Software-as-a-Service (SaaS)**.
    

**(OBSTACLES)** Include **vendor lock-in**, **security/privacy** concerns, and **dependency** on provider availability.
### 2. Distributed information systems
#### Context
Many of the existing middleware solutions are the result of working with an infrastructure in which it was easier to integrate apps into an enterprise-wide information sys. 
Organisations had *many* *networked apps* but struggled to achieve interoperability.

**(INTEGRATION)** A networked app simply of a server running that app (often including a db) and making it available to remote programs, called *clients*. 
- Clients send a request to the server for executing a specific operation and a response returned. 
- Integration at the lowest level allows clients to wrap a number of requests, possibly for different apps or servers, into a single larger request and have it executed as a distributed transaction.
- The key idea is that all, or none of the requests are executed. 
- As apps became more sophisticated and were separated into independent components (notably distinguishing db components from processing components), apps needed to communicate directly with each other, leading to *Enterprise app Integration (EAI)*
#### Transactions
Database apps where operations are carried out in the form of TRANSACTIONS.
- In a mail sys, there might be primitives to send, receive, and forward mail. 
- In an accounting sys, they might be quite different. READ and WRITE are typical examples, however. Ordinary statements, procedure calls, and so on, are also allowed inside a transaction. 
- In particular, **remote procedure calls (RPCs)**, that is, procedure calls to remote servers, are often also encapsulated in a transaction, leading to what is known as a **transactional RPC**.

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
The operations between them form the body of the transaction. 
- The characteristic feature of a transaction is _either all of these operations are executed or none are executed_. 
	- These may be sys calls, library procedures, or bracketing statements in a language, depending on the implementation. 
- This all-or-nothing property of transactions is one of the four characteristic properties that transactions have. 

More specifically, transactions adhere to the so-called ACID properties:
**Atomic**: happens indivisibly (seemingly)
**Consistent**: does not violate sys in-variants
**Isolated**: not mutual interference between concurrent transactions
**Durable**: commit means changes are permanent

**(SUB-TRANSACTION)** join to form a nested transaction. 
- The top-level transaction may fork off children that run in parallel with one another, on different machines, to gain performance or simplify programming. 
- Each child may also execute one or more sub-transactions, or fork off its own children. 
**(ISSUES)** Imagine a transaction starts several subs in parallel, and one commits, making its results visible to the parent transaction. 
- After further computation, the parent aborts, restoring the entire sys to the state it had before the top-level transaction started. 
- Consequently, the results of the sub that committed must be undone. Thus, the permanence referred to above applies only to top-level transactions.

Since transactions can be nested arbitrarily deep, considerable administration is needed to get everything right. The semantics are clear, however. 
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire sys for it to manipulate_ as it wishes. 
	- If it aborts, its private universe just vanishes, as if it had never existed. 
	- If it commits, its private universe replaces the parent’s universe. 
- Thus if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- Likewise, if an enclosing (higher level) transaction aborts, all its underlying subtransactions have to be aborted as well. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.
**(TRANSACTION PROCESSING MONITOR)**  The *data* involved in a transaction is *distributed across several servers*. 
A TP Monitor is responsible for coordinating the execution of a transaction and the commitment of subtransactions following a standard protocol known as distributed.

Nested transactions are important in DS, naturally providing a way of distributing a transaction across multiple machines, following a logical division of the work of the original transaction. 

For example, a transaction for planning a trip by which three different flights need to be reserved can be logically split up into three subtransactions. 
- Each of these subtransactions can be managed separately and independently of the other two.
![[tpm-in-a-ds.png\|400]]
Apps wanting to coordinate several subtransactions into a single transaction did not have to implement this coordination themselves because a TP monitor does this for them. 
This is exactly where MIDDLEWARE comes into play: 
- it implements services that are useful for many apps avoiding that such services have to be reimplemented over and over again by app developers.

**(Middleware and EAI)** Several types of communication middleware exist. 
With remote procedure calls (RPC), an app component can effectively send a request to another app component by doing a local procedure call, which results in the request being packaged as a message and sent to the callee. 

The result will be sent back and returned to the app as the result of the procedure call. 
Later, techniques were developed to allow calls to remote objects, leading to what is known as **remote method invocations (RMI)**. An RMI is _essentially the same as an RPC, except that it operates on objects instead of functions_.

Middleware offers communication facilities for app integration (EAIs)
![[middleware-in-eai.png\|300]]
**Remote Procedure Call (RPC):** Requests are sent through local procedure call, packaged as message, processed, responded through message, and result returned as return from call.

RPC and RMI have the _disadvantage_ that the caller and callee both need to be up and running at the time of communication. 

In addition, they need to know **exactly** how to refer to each other. This **tight coupling** is often experienced as a serious drawback, and has lead to what is known as **message-oriented middleware (MOM)**. 

Here, apps send messages to logical contact points, described by means of a subject. 
- Likewise, apps can indicate their interest for a specific type of message, after which the communication middleware will take care that those messages are delivered to those apps. 
- These so-called **publish/subscribe systems** form an important and expanding class of DS.
- Messages are sent to logical contact point (published), and forwarded to subscribed apps.
### 3. Distributed Systems for Pervasive computing (DPS)
Next for DS where nodes are small, mobile, and often embedded in a larger sys, characterised by the fact that the sys *naturally blends into the user's env*. Coined as the IoT.
- We often need to deal with the intricacies of wireless and mobile communication, will require special solutions to make a pervasive sys as transparent or unobtrusive as possible.

They're unique because the separation between users and sys components is more blurred. 
There is often no single dedicated interface, such as a screen/keyboard combination. 
Instead, a **pervasive** sys is often equipped with many **sensors** that pick up various aspects of a user’s behaviour. Likewise, it may have a myriad of **actuators** to provide information and feedback, often even purposefully aiming to _steer_ behaviour.
#### Ubiquitous Computing Systems (UCS)
Pervasive and continuously present, i.e., a continuous interaction between sys and user often with the user being unaware that the interaction is happening.
Core requirements for a UCS roughly as follows: 
1. (**Distribution**) Devices are networked, distributed, and accessible in a transparent manner 
2. (**Interaction**) Interaction between users and devices is highly unobtrusive, hiding interfaces.
Much of the interaction by humans will be **implicit**: _one that is not primarily aimed to interact with a computerised sys but which such a sys understands as input_.
**(Ex)** The settings of a car’s driver’s seat, steering wheel, and mirrors is fully personalised. 
If Bob takes a seat, the sys will recognise that it is dealing with Bob and subsequently makes the appropriate adjustments. 
3. (**Context awareness**) The sys is aware of a user’s context in order to optimise interaction.
_any information that can be used to characterise the situation of entities (i.e., whether a person, place or object) that are considered relevant to the interaction between a user and an app, including the user and the app themselves_. 

In practice, context is often characterised by location, identity, time, and activity: 
- the where, who, when, and what. 
- A sys will need to have the necessary (sensory) input to determine one or several of these context types. 
- Raw data as collected by various sensors is lifted to a level of abstraction that can be used by apps. 

A concrete example is detecting where a person is, for example in terms of GPS coordinates, and subsequently mapping that information to an actual location, such as the corner of a street, or a specific shop or other known facility. 

The question is where this processing of sensory input takes place: 
- is all data collected at a central server connected to a db with detailed information on a city, or is it the user’s smartphone where the mapping is done? 
- When it comes to combining flexibility and potential distribution, so-called shared data spaces in which processes are decoupled in time and space are nice, yet, suffer from scalability problems. 
4. (**Autonomy**) Devices operate autonomously without human interven- tion, and are thus highly self-managed 
In a UCS env, the sys admin cannot keep everything up and running. 
The sys should act autonomously, and automatically react to changes using many techniques:
- **Address allocation**
	- In order for networked devices to communicate, they need an IP address. Addresses can be allocated automatically using protocols like the Dynamic Host Configuration Protocol (DHCP) (which requires a server) or Zeroconf. 
- **Adding devices**
	- It should be easy to add devices to an existing sys. A step towards automatic configuration is realised by the Universal Plug and Play Protocol (UPnP).
	- Using UPnP, devices can discover each other and make sure that they can set up communication channels between them. 
- **Automatic updates
	- Many devices in a UCS should be able to regularly check the Internet software updates
	- Manual intervention is to be kept to a minimum
4. (**Intelligence**) The sys as a whole can handle a wide range of dynamic actions and interactions.
UCS often use methods and techniques from the field of artificial intelligence. 
In many cases, a wide range of advanced algorithms and models need to be deployed to handle incomplete input, quickly react to a changing environment, handle unexpected events, and so on. 
#### Mobile computing systems 
Pervasive, but emphasises that devices are inherently mobile.
A DS where devices can change physical location and network attachment points while maintaining connectivity, often forming a key part of **pervasive computing**.

* **Device Heterogeneity:** Encompasses a wide range of **IP-enabled devices** beyond smartphones/tablets (e.g., IoT sensors, wearables, vehicular systems).
* **Wireless Communication:** The primary enabler of mobility (*Mobile typically implies wireless*).
* **Dynamic Location:** A device's **network location** (IP address/point of attachment) and **physical location** change over time. This is the central challenge.
![[mobile-comp-ex.png\|200]]
**A. Service Discovery & Availability**
* **Problem:** As devices move, the set of locally available services changes.
* **Required Mechanism:** **Dynamic service discovery** (services announce presence, clients discover them). This is crucial for context-aware apps.
*   *Example:* Your smartphone automatically discovering and connecting to a nearby wireless printer when you enter an office, but not seeing it at home.
**B. Location Management**
*   **Geographical Location:** Needed for **tracking/tracing** (e.g., GPS in a delivery vehicle).
*   **Network Location:** Needed for routing. Solved by protocols like **Mobile IP**, which separates a device's permanent **home address** from its temporary **care-of address**, allowing seamless movement between networks.
**C. Communication in Dynamic Networks: MANETs & DTNs**
*   **MANET (Mobile Ad-hoc Network):** A self-configuring network of mobile devices connected wirelessly without fixed infrastructure.
    *   **Core Routing Problem:** **Static routes fail** as nodes move. Requires **proactive** (e.g., DSDV) or **reactive** (e.g., AODV) routing protocols.
*   **DTN (Disruption/Disconnection-Tolerant Network):** Assumes **intermittent connectivity** and no guaranteed end-to-end path.

DTN Routing Principles (Store-Carry-Forward).
When a continuous path cannot be established, DTNs use a **store-carry-forward** paradigm:
1.  **Store:** A node receives and **stores** a message.
2.  **Carry:** The node physically **carries** the message as it moves.
3.  **Forward:** When it finds another node, it may **forward** the message based on a chosen strategy.
_Passing messages in a (mobile) disruption-tolerant network_
![[passing-messages-disruption-tolerant-network.png\|300]]
**Routing Strategies:**
*   **Flooding-Based (e.g., Epidemic Routing):** Messages spread redundantly. High delivery chance but high resource cost.
*   **Selective Forwarding:** More efficient. Decisions based on:
    *   **Utility/Probability:** Forward to nodes with higher historical probability of contacting the destination.
    *   **Social Patterns:** Forward to nodes belonging to the same "community" (**social-based routing**).
    *   **Connectivity:** Forward to **well-connected nodes** (central hubs in the mobility pattern).
*   ***Technical Example:** A wildlife tracker on a zebra (Node A) needs to send data to the ranger station (Node D). There's no direct path. The zebra moves and encounters a vulture's tracker (Node B). Using a **utility-based** protocol, Node A forwards the data to Node B. The vulture flies and later lands near a fixed sensor node (Node C) connected to the station, finally delivering the data. The network tolerated long delays and disconnections.*
Mobile computing shifts the design focus from stable connectivity and fixed locations to **mobility management, discovery, and tolerant routing** in the face of constant change and potential disconnection. Mobile devices set up connections to stationary servers, bringing mobile computing in the position of clients of cloud-based services.
#### Sensor (and actuator) networks: 
Pervasive, with emphasis on the actual (collaborative) sensing and actuation of the env.
Here is the content reformatted as requested, with headings prepended to the paragraphs and the "Features" section integrated.

**(SENSOR NETWORKS)** They are more than just a collection of input devices; instead, sensor nodes often collaborate to efficiently process the sensed data in an app-specific manner, making them very different from, for example, traditional computer networks. A sensor network generally consists of tens to hundreds or thousands of relatively small nodes, each equipped with one or more sensing devices. In addition, nodes can often act as actuators, a typical example being the automatic activation of sprinklers when a fire has been detected.

**(KEY CHARACTERISTICS)** Many sensor networks use wireless communication, and the nodes are often battery powered. Their limited resources, restricted communication capabilities, and constrained power consumption demand that efficiency is high on the list of design criteria.

**(NODE ARCHITECTURE)** When zooming into an individual node, we see that, conceptually, they do not differ a lot from “normal” computers: above the hardware there is a software layer akin to what traditional operating systems offer, including low-level network access, access to sensors and actuators, and memory management. Normally, support for specific services is included, such as localisation, local storage (think of additional flash devices), and convenient communication facilities such as messaging and routing. However, similar to other networked computer systems, additional support is needed to effectively deploy sensor network apps. In DSs, this takes the form of middleware.

**(PROGRAMMING MODELS & COMMUNICATION SCOPE)** For sensor networks, instead of looking at middleware, it is better to see what kind of programming support is provided. One typical aspect in programming support is the scope provided by communication primitives. This scope can vary between addressing the physical neighbourhood of a node, and providing primitives for sys-wide communication. In addition, it may also be possible to address a specific group of nodes. Likewise, computations may be restricted to an individual node, a group of nodes, or affect all nodes.
*   *Example (Abstract Regions):* A node can define a region of its eight nearest neighbours. It can write a sensor reading to that region, and then perform an operation like finding which node in the region has the maximum reading, enabling local, collaborative processing.

**(DATABASE VIEW & DATA ACCESS)** As another related example, consider a sensor network as implementing a distributed db. This view is common as many networks are deployed for measurement and surveillance, where an operator extracts information by issuing queries.

**(DATA PROCESSING ARCHITECTURES)** To organise a sensor network as a distributed db, there are essentially two extremes:
1.  **Centralised Processing:** Sensors send all raw data to a central operator's site. This wastes network resources and energy.
2.  **Purely Edge Processing:** Queries are sent to sensors, each computes an answer, and the operator aggregates all responses. This wastes the opportunity for sensors to aggregate data locally, reducing the volume of traffic.

**(IN-NETWORK PROCESSING)** What is needed are facilities for **in-network data processing**. One common method is to use a **routing tree**. A query is propagated down the tree to all nodes, and results are **aggregated** (e.g., summed, averaged, filtered) at intermediate nodes as they are passed back up to the root (the operator). This conserves energy and bandwidth.
*   *Example (TinyDB):* Implements a declarative SQL-like interface. It uses a tree-based routing algorithm where parent nodes schedule data collection from children and aggregate results before forwarding them upward.

**(PUBLISH/SUBSCRIBE MODEL)** An alternative to a single, query-specific tree is a **publish/subscribe** model. Nodes publish specific types of data (e.g., temperature readings) which are forwarded to designated **broker nodes**. Queries for that data type are then sent to the corresponding broker. This decouples data producers from consumers.

**(FEATURES)** Sensor networks are characterised by:
*   **Collaborative Processing:** Nodes work together to process data.
*   **Severe Resource Constraints:** Limited battery, CPU, memory, and communication bandwidth.
*   **Application-Specific Design:** The network architecture is tightly coupled to its sensing task.
*   **Wireless Communication & Ad-hoc Deployment.**
*   **In-Network Processing:** Computation occurs within the network to reduce data volume and save energy.
*   **Diverse Programming Models:** Including the db abstraction (e.g., TinyDB) and region-based programming.
**(BOTTOM LINE)** While mobile devices often act as clients to cloud services (**mobile cloud/edge computing**), sensor networks represent a different paradigm: a massively distributed, collaborative, and deeply embedded sys where processing is pushed into the network itself to overcome severe resource limitations and achieve scalability.
![[cloud-edge-continuum.png\|400]]
**(WHAT IS GOOGLE SPANNER?)** Google Spanner is a globally distributed, strongly consistent, relational db service. Its unique achievement is combining the horizontal scalability of NoSQL systems with **Strong ACID transactions** and an SQL interface across data centers worldwide. It was first described in 2012 and is used internally for services like Google Ads, Gmail, and Google Photos before being offered as Cloud Spanner

**(CORE INNOVATIONS)** It works by automatically sharding data across many Paxos-based replication groups for availability. Its most significant innovation is **TrueTime**, a globally synchronised clock API that uses GPS and atomic clocks to bound clock uncertainty across servers. TrueTime is the key that enables its powerful consistency properties without sacrificing performance globally.

**(CONSISTENCY PROPERTIES)** By default, Spanner provides **External Consistency**, which is the strictest isolation level for transaction-processing systems.
It has two formal guarantees:
- **Strict Serializability**: The sys behaves as if all transactions executed one at a time in some order (**serializability**), and that order respects real-time: if transaction T2 starts _after_ T1 commits, T2 must appear to execute _after_ T1 in the serial order.
- **External Consistency**: This adds a real-time ordering guarantee for transactions _observed_ to commit in sequence. If T2's commit process begins after T1's commit _returns successfully_, T2 is guaranteed to have a later commit timestamp than T1. This prevents anomalies like seeing a withdrawal before a deposit that visibly completed earlier. External consistency is a stronger property than both linearizability (for single operations) and basic strong consistency

**(HANDLING ACID TRANSACTIONS)** Spanner provides full ACID guarantees globally
This is achieved through a specific architecture:
- **Atomicity & Durability**: Uses a **Paxos-based synchronous replication** across every data shard. A write must be accepted by a majority of replicas before it is committed, ensuring durability. The two-phase commit protocol coordinates commits across different shards involved in a transaction
- **Isolation & Consistency**: Primarily uses **Multi-Version Concurrency Control (MVCC)**. Writes create new, timestamped versions of data
- TrueTime assigns globally meaningful, monotonically increasing timestamps to commits, establishing the externally consistent serial order. **Read-write transactions** (for reads and writes) use locking and get external consistency. **Read-only transactions** are executed at a chosen timestamp (system-chosen for strong reads, past for stale reads) without acquiring locks, reading a consistent snapshot.
- **Alternative Isolation Level**: Spanner also offers **Repeatable Read (Snapshot) Isolation**, which can improve performance for read-heavy, low-conflict workloads. Unlike the default, it is susceptible to **write skew**, so apps must use locking reads (`FOR UPDATE`) for correctness in critical sections.