Friday 03/10/2025

---
What is Data?
Objective: Predict the survival of the passengers aboard RMS Titanic.
Dataset: Titanic DataSet
- provides the data on all the passengers who were aboard the RMS Titanic when it sank on 15 April 1912 after colliding with an iceberg in the North Atlantic ocean.
- a combination of variables based on personal characteristics such as age, class of ticket and gender, and tests one’s classification skills.
- 891 rows and 12 columns
### What is Data?
“A datum is a single measurement of something on a scale that is understandable to both the recorder and the reader. Data are multiple such measurements.”
#### Where does it come from?
 • Internal sources: already collected by or is part of the overall data
collection of your organization.
For example: business-centric data that is available in the organization
data base to record day to day operations; scientific or experimental
data.
• Existing External Sources: available in ready to read format from an
outside source for free or for a fee.
For example: public government databases, stock market data, Yelp
reviews, [your favorite sport]-reference.
• External Sources Requiring Collection Efforts: available from external
source but acquisition requires special processing.
For example: data appearing only in print form, or data on websites.
How to get data generated, published or hosted online:
• API (Application Programming Interface): using a prebuilt set of
functions developed by a company to access their services. Often
pay to use. For example: Google Map API, Facebook API, Twitter
API
• RSS (Rich Site Summary): summarizes frequently updated online
content in standard format. Free to read if the site has one. For
example: news-related sites, blogs
• Web scraping: using software, scripts or by-hand extracting data
from what is displayed on a page or what is contained in the HTML
file (often in tables).
#### Web Scraping
• Why do it? Older government or smaller news sites might not have
APIs for accessing data, or publish RSS feeds or have databases for
download. Or, you don’t want to pay to use the API or the database.
• How do you do it?
• Should you do it?
– You just want to explore: Are you violating their terms of service?
Privacy concerns for website and their clients?
– You want to publish your analysis or product: Do they have an API or
fee that you are bypassing? Are they willing to share this data? Are
you violating their terms of service? Are there privacy concerns?
#### What type of data? Key Data Properties
##### Structure -- the “shape” of a data file
- e.g. Tab separated values
- A common table file format.
- Records are delimited by a newline: '\n', "\r\n"
- Fields are delimited by '\t' (tab)
- pd.read_csv: Need to specify delimiter='\t'
- ![[non-rectangular-data.png]]
##### Granularity -- how fine/coarse is each datum
Singular people as data-points vs groups
##### Scope --how (in)complete is the data
Will my data be enough to answer my question?
- Example: I am interested in studying crime in California but
- I only have Berkeley crime data.
- Solution: collect more data/change research question

Is my data too broad?
- Example: I am interested in student grades for Algorithms but have student grades for all Algorithms classes (grad + undergrad).
- Solution: Filtering ⇒ Implications on sample?

If the data is a sample I may have poor coverage after filtering.

 Does my data cover the right time frame? Which brings us to Temporality
- Temporality -- how is the data situated in time
###### Data changes – when was the data collected/last updated?
Periodicity — Is there periodicity? Diurnal (24-hr) patterns?
What is the meaning of the time and date fields? A few options:
- When the “event” happened?
- When the data was collected or was entered into the system?
- Date the data was copied into a database? (look for many matching timestamps)
##### Faithfulness -- how well does the data capture “reality”
Does my data contain unrealistic or “incorrect” values?
- Dates in the future for events in the past
- Locations that don’t exist
- Negative counts
- Misspellings of names
- Large outliers

Does my data violate obvious dependencies?
- E.g., age and birthday don’t match
- Was the data entered by hand?
- Spelling errors, fields shifted …

Did the form require all fields or provide default values?
- Are there obvious signs of data falsification?
- Repeated names, fake looking email addresses, repeated use of uncommon names or fields.
Duplicated Records or Fields Identify and eliminate (use primary key).
Time Zone Inconsistencies Convert to a common timezone (e.g., UTC)
Spelling Errors Apply corrections or drop records not in a dictionary
Units not specified or consistent Infer units, check values are in reasonable ranges for data
Truncated data Early Microsoft Excel limits: 65536 Rows, 255 Columns
● Be aware of consequences in
analysis when using data with
inconsistencies.
● Understand the potential
implications for how data were
collected
Examples
" “
0, -1
999, 12345
NaN: “Not a Number”
1970, 2000
NaN
Null
Missing
#### What type of data? 
What kind of values are in your data (data types)?
Simple or atomic:
- Numeric: integers, floats
- Boolean: binary or true false values
- Strings: sequence of symbols
A. Drop records with missing values
● Probably most common
● Caution: check for biases induced by dropped values
○ Missing or corrupt records might be related to something of
interest
B. Keep as NaN
C. Imputation/Interpolation: Inferring missing values
● Average Imputation: replace with an average value
○ Which average? Often use closest related subgroup mean.
● Hot deck imputation: replace with a random value
● Regression imputation: replace with a predicted value, using some model
● Multiple imputation: replace with multiple random values.
Choice affects bias and uncertainty
quantification (large statistics literature)
Essential question: why are the
records missing?
#### How is the data stored? 
How is your data represented and stored (data format)?
- Tabular Data: a dataset that is a two-dimensional table, where each row typically represents a single data record, and each column represents one type of measurement (csv, dat, xlsx, etc.).
- Structured Data: each data record is presented in a form of a [possibly complex and multi-tiered] dictionary (json, xml, etc.)
- Semi-structured Data: not all records are represented by the same set of keys or some 
- data records are not represented using the key-value pair structure
#### What kind of values are in your data (data types)? Compound,
composed of a bunch of atomic types:
• Date and time: compound value with a specific structure
• Lists: a list is a sequence of values
• Dictionaries: A dictionary is a collection of key-value pairs, a pair
of values x : y where x is usually a string called the key
representing the “name” of the entry, and y is a value of any type.
Example: Student record: what are x and y?
• First: Kevin
• Last: Rader
• Classes: [CS-109A, STAT139]

Example: Student record: what are x and y?
- First and Last name: Kevin Rader
- Classes: [CS-109A, STAT139]
### Common Issues
A few good generic questions to ask are as follows:
● How big is the dataset?
● Is this the entire dataset?
● Is this data representative enough? For example, maybe data was only collected for a subset of users.
● Are there likely to be gross outliers or extraordinary sources of noise? For example, 99% of the traffic from
a web server might be a single denial‐of‐service attack.
● Might there be artificial data inserted into the dataset? This happens a lot in industrial settings.
● Are there any fields that are unique identifiers? These are the fields you
might use for joining between datasets, etc.
● Are the supposedly unique identifiers actually unique? What does it mean if they aren’t?
● If there are two datasets A and B that need to be joined, what does it mean if something in A doesn’t
matching anything in B?
● When data entries are blank, where does that come from?
● How common are blank entries?
#### Common issues with data:
• Missing values: how do we fill in?
• Wrong values: how can we detect and correct?
• Messy format
• Not usable: the data cannot answer the question posed
Sometimes your data comes in multiple files:
● Often data will reference other pieces of data.
● Alternatively, you will collect multiple pieces of related data.
Use pd.merge to join data on keys.
Primary key: the column or set of columns in a table that uniquely
determine the values in the remaining columns
● Primary keys are unique
● Examples: SSN, ProductIDs, …
Foreign keys: the column or set of columns that reference
primary keys in other tables
#### Common causes of messy data are:
• Column headers are values, not variable names
• Variables are stored in both rows and columns
• Multiple variables are stored in one column/entry
• Multiple types of experimental units stored in same table
In general, we want each file to correspond to a dataset, each column
to represent a single variable and each row to represent a single
observation.
We want to tabularize the data.
### Hierarchy of data-categories for a variable
![[data-types.png]]Variable->[Quantitative, Qualitative(categorical)]
Quantitative (intervals have meaning)->[Continuous, Discrete]
Qualitative->[Ordinal, Nominal]
Continuous: Could be measured to arbitrary precision.
E.g. • Price
• Temperature
Discrete: Finite possible values
E.g. • Number of
siblings
• Yrs of education
Ordinal: Categories w/ordered levels; no consistent meaning to difference
E.g. • Preferences
• Level of
education
Nominal: Categories w/ no specific ordering.
E.g. • Political
Affiliation
• Student lD

Examples for each with Justitfication: **TODO**
Many variables do not sit neatly in one of these categories!
Example here would these sits?
1 CO2 level (ppm)
2 Number of siblings
3 GPA
4 Income bracket
(low, med, high)
5 Race/Ethnicity
6 Shirt Size (S,M,L)
7 Yelp Star Rating
### Paper review
### **Summary: A Practical Guide to Characterising Data and Investigating Data Quality**

This guide provides a systematic, task-based framework for the **data profiling** stage of a data science project, which the authors note often consumes 50-90% of project time. It aims to move profiling from an *ad-hoc art* to a more **rigorous, consistent, and reproducible** process.

### **Core Components:**

1.  **A Six-Step Workflow:** A recommended order to avoid rework.
    1.  **Look at your data** (initial sanity check).
    2.  **Watch out for special values** (e.g., codes for missing data).
    3.  **Check for missing data** (coverage, duplicates).
    4.  **Check each variable** individually (format, distribution, plausibility).
    5.  **Check combinations of variables** (correlations, business rules).
    6.  **Characterise the cleaned data** (final profiling).

2.  **Comprehensive Profiling Tasks:** Organised into 62 specific tasks framed as questions, covering:
    *   **Data Characterisation:** Understanding what the data *is* (e.g., structure, distributions, patterns).
    *   **Data Quality Investigation:** Assessing how *fit* the data is for purpose, focused on:
        *   **Completeness** (missing data, coverage).
        *   **Accuracy** (extreme, incorrect, or implausible values).
        *   **Consistency** (format, units, semantics across sources).

3.  **Practical Implementation:** The methods are supported by a **Python package (`vizdataquality`)** for use in Jupyter Notebooks, which automates parts of the workflow and helps generate reports.

### **Key Insight:**
The guide is built on research showing most data scientists' profiling is superficial. It offers a **structured checklist** to ensure thoroughness, while allowing practitioners to adapt it to their specific needs, avoiding a rigid "one size fits all" approach.