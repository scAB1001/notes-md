9. Principal Component Analysis (PCA)
### Principal Components
Principal Component
When performing SVD, we’ve left the columns in the “data” matrix UΣ unnamed.
● Their common name: “Principal Components”\*.
● Example: The second column of UΣ is the “2nd principal component” of our
original matrix.
![[svd-applied.png]]
*: There’s an important technical detail we’ll need to fix first*
#### Interpreting Principal Components
Let’s look at a geometric interpretation of:
● principal components.
● The rank-k approximation as a whole.
This is easiest if we work with a simple dataset with 2 attributes
We’ll use the dataset below showing
the age 5 mortality rates and fertility rates
for most of the countries around the world.
![[mortality-fertility.png]]
Data, Rank 2, and Rank 1 Approximation
Below, we see the original data, along with the rank 2 and rank 1
approximations.
● As before, we see that approximation at rank 1 is still fairly decent.
● The degree to which it is decent depends on how strong the relationship is
between mortality and fertility.
● We can get a lot more insight by looking at things visually.
![[mortality-fertility-vis.png]]
The Rank 1 Approximation Visually
We see that the rank 1 approximation projects the data on to a 1D subspace.
![[mortality-fertility-vis-rank-1.png]]
There’s a significant issue with our rank 1 approximation. What is it?
Our approximation goes through the origin, but original data has non-zero y-
intercept.
For other datasets, the impact of this y-intercept mismatch can be severe.
Typically, to deal with this issue we recenter the data by subtracting the mean
of each column for all values in that column. Resulting projection is much
better
Data Uncentering
After performing the approximation, you can add back the means to get back to
the original x and y scale

Intuitive Picture of Low Rank Approximation
Consider the maternal fertility vs. child
mortality data we discussed earlier.
Technically, every country has its own
separate fertility and mortality.
![[mortality-fertility-vis-pc.png]]
● Looking at the plot, we see that we can
approximate by saying that each
country lies on a continuum between
“bottom left” and “top right”.
In essence, the 1st principal component
gives each country a single score along this
intuitive axis!
Given U, Σ, and VT below, write an expression to
compute PC1 for Afghanistan.
● Hint the first column of UΣ is the 1st PC (for all
countries).
Since the principal components are just the columns of
UΣ, we just compute the top left entry of UΣ:
● -0.0954 * 43.48 + 0.023 * 0 = -4.15
![[mortality-fertility-vis-pc-table.png]]
Naturally, we can plot countries vs. PC1.
● Only one dimension so we end up with
a rug plot.
PC1 Back to 2D
Our rank 1 approximation is essentially just
going from PC1 back into 2 dimensions.
Give expressions that compute the rank 1
approximation for the fertility and mortality
rates for Afghanistan
To get our rank 1 estimates for Afghanistan:
● Recall our original data X = UΣVT.
● So just multiply the first value of the
column (principal component) of UΣ by
the first row of VT.
● Mortality: -4.14 \* -0.929 = 3.86
● Fertility: -4.14 \*-0.368 = 1.53

The first principal component captures
much of the variance, but not all.
● Differently worded: Countries live near
the line, but not exactly on it.
By using 2nd principal component, we can
get back original data exactly.

PC2
Low PC2 countries
High PC2 countries
PC2 is harder to interpret.
● It tells you how much you lie “above”
or “below” the PC1 line.
The gray area shows countries with similar
values of PC1.
● Countries at the top left of the oval
have “large” PC2.
● Countries at the bottom left have
“small” PC2

### Low Rank Approximation vs. Regression
To get a deeper understanding of what’s going on, let’s see a comparison to
linear regression which you saw in Data 8.
Let’s start with a very quick review of linear regression.
● This will be purely conceptual glance.
Regression: The Big Idea
Suppose we know the child mortality rate
of a given country.
● Linear regression tries to predict the
fertility rate from the mortality rate.
● For example, if the mortality is 6, we
might guess the fertility is near
The regression line tells us the “best”
prediction of fertility given all possible
mortality values
● Minimizes the root mean squared
error [see vertical red lines, only some
shown].

We can also perform a regression in the
reverse direction.
That is, given the fertility, we try to predict
the mortality.
In this case, we get a different regression
line which minimizes the root mean
squared length of the horizontal lines.

Rank 1 Approximation of Fertility / Mortality Data
For our mortility data and the rank 1 approximation.
If we make a line plot instead, this starts to look a lot like a regression line.
● Note: The approximation is just the data projected onto this line.
The rank 1 approximation is not the same as the mortality regression line.
● They’re vaguely close, but not the same.
The fertility regression line is pretty close, but is also different.
● The closeness is just a coincidence.
#### SVD: Minimizing the Perpendicular Error
Instead of minimizing horizontal or
vertical error, our rank 1
approximation minimizes the error
perpendicular to the subspace
onto which we’re projecting.
That is, SVD finds the line such
that if we project our data onto
that line, the error between the
projection and our original data is
minimized.
![[mortality-fertility-vis-svd.png]]
Looking at adiposity and bicep size
from our body measurements
dataset, we see the 1D subspace
onto which we are projecting is
between the two regression lines.
Beyond 1D/2D
In higher dimensions the idea behind principal components is just the same!
Example: Suppose we have 30 dimensional data and decide to use the first 5
principal components.
● Our procedure minimizes the error between:
○ The original 30 dimensional data.
○ The projection of that 30 dimensional data on to the “best” 5
dimensional subspace. 
### Principal Components and Variance
Back to our original rectangle data.
Recall: Before we can identify the principal components of this data, we must
first center the data by subtracting the mean of each column from that column
Our centered data is easy to interpret:
● The first observation had a width that was 2.97 greater than the mean,
length 1.35 greater than the mean, etc.
● Each entry is given in its original pre-centering units (e.g. meters, square
meters, etc.).
#### Singular Value Interpretation (Informally)
SVD decomposes our data into U, Σ and VT.
● Let’s talk more about the singular values Σ.
Informally, the ith singular value tells us how valuable the ith principal
component will be in reconstructing our original data.
● First principal component does most of the work.
● Next two principal components contribute about equally.
● Fourth principal component does nothing (4th singular value is zero).

Suppose we do SVD of a 6 dimensional dataset and get back the singular values
shown, what might be the most appropriate rank to use for our approximation?

2? We don’t get nearly as much additional accuracy by also using the next
4 principal components.
#### Singular Value Interpretation (More Formally)
Formally, the ith singular value tells us how much of the variance is captured by
the ith principal component.
To understand this, first let’s define the total variance of your data as the sum
of the individual variances of the attributes.
Width variance: 7.7
Height variance: 5.3
Area variance: 338.7
Perimeter: 50.8
Total variance: 402.56

Formally, the ith singular value tells us how much of the variance is captured by
the ith principal component.
● The amount of variance captured by the ith principal component is equal
to (ith singular value)2 / N.
width length area perimeter
2.97 1.35 24.78 8.64
-3.03 -0.65 -15.22 -7.36
-4.03 -1.65 -20.22 -11.36
...
-3.03 1.35 -11.22 -3.36
Total variance: 402.56
Variance captured by 1st PC: 197.392/100 = 389.63
Variance captured by 2nd PC: 27.432/100 = 7.52
Variance captured by 3rd PC: 23.262 / 100 = 5.41
#### Variance Fraction Arrays and Scree Plots
To get a sense of the relative importance of each principal component, we can
compute the fraction of the variance captured in each coordinate:
Or we can plot them on an axis in what is known as a “scree plot"

Principal Component Analysis (PCA) is the name for what we’ve done today.
● PCA is the process of (linearly) transforming data into a new coordinate
system such that the greatest variance occurs along the first dimension,
the second most along the second dimension, and so on.
### Principal Component Analysis in Practice
#### Introducing PCA: Principal Component Analysis
“The purpose of computing is insight, not numbers.” - Richard Hamming
So far we’ve done a lot of numbers. Why is this useful?
Goal: Plot high dimensional data as a 2 dimensional approximation that results
from a linear combinations of attributes.
Related Goal: Determine whether this two-dimensional plot is really showing
the variability in the data. (If not, be wary of conclusions drawn using PCA.)
#### PCA is appropriate for EDA when:
● Visually identifying clusters of similar observations in high dimensions.
● You are still exploring the data.
(If you already know what to predict, you probably don’t need PCA.)
● You have reason to believe that the data are inherently low rank: there are
many attributes, but only a few (perhaps unobserved) attributes mostly
determine the rest through a linear association.
Singular value decomposition (SVD) describes a matrix decomposition:
● X = UΣVT (or XV = UΣ) where U and V are orthonormal and Σ is diagonal.
● If X has rank r, then there will be r non-zero values on the diagonal of Σ.
● The values in Σ, called singular values, are ordered from greatest to least.
● The columns of U are the left singular vectors.
● The columns of V are the right singular vectors.
#### Principal component analysis (PCA) is a specific application of SVD:
● X is a data matrix centered at the mean of each column.
● The largest n singular values are kept, for some choice of n;
All other singular values are set to zero: the dimensionality reduction.
● The first n rows of VT are directions for the n principal components.
● The first n columns of UΣ (or XV) contain the n principal components of X.
● Primarily utilizes UΣ (by plotting first two columns and scree plot of Σ).
#### SVD for PCA
Inspecting Principal Component Directions (Axes)
Instead of looking at UΣ, we can instead look at VT.
A principal component direction is a linear combination of attributes.
● Given as rows of VT.
Plotting the values of the first principal component direction can provide insight
into how attributes are being combined.
● High variance attributes are typically included (but not always).
● Many attributes are often included, even if only a few are really important.
Interpreting other principal components is challenging; the constraint that they
are orthogonal to prior components strongly influences their directions.
### Extra Slides
Why are those two goals
equivalent?
Maximizing variance =
spreading out red dots
Minimizing error = making
red lines short
Imagine that the black line
is a stick, and the red lines
are springs attached to the
stick from the points.
The first PC is where the
stick comes to rest.
SVD finds this for us.
### FAQ
● PCA is a technique to summarize data
○ PCA has one goal stated two different ways:
■ Find directions that minimize projection error
■ Find directions that maximize captured variance
○ To conduct PCA, we use SVD (alternatives possible too)
● If PCA is successful, the 2D plot will still preserve structure of
original data
● Scree plots tell us how much information lost in PCA