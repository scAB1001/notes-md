Data Science Roadmap
1. Frame problem
2. Understand data
3. Extract Features
4. Model and analyse
5. Present results, Deploy code, Frame problem.
Exploratory Data Analysis
(EDA) Guiding Principles
EDA is unboxing for
data!
“Exploratory data analysis is an
attitude, a state of flexibility, a
willingness to look for those things
that we believe are not there, as well
as those that we believe to be there.”
– John Tukey

Example
Objective: Predict the survival of the
passengers aboard RMS Titanic.
Dataset: Titanic DataSet
• provides the data on all the passengers
who were aboard the RMS Titanic when it
sank on 15 April 1912 after colliding with
an iceberg in the North Atlantic ocean.
○ a combination of variables based on personal
characteristics such as age, class of ticket and
gender, and tests one’s classification skills.
○ 891 rows and 12 columns

Visualizations and Simple Metrics
Typically, a good analytics project starts (after cleaning and understanding the
data) with exploratory visualizations that help you develop hypotheses and get
a feel for the data.
• Then you carefully manicure figures that make the final results visually
obvious.
Today:
• We will cover several of the most important visualization techniques.
• We will also cover some exploratory metrics (such as correlations), which
capture some of the patterns that are clear from a good visual
• Our eyes can trick us, so it’s important to have a cold hard statistic too.
○ We will cover some standard statistical metrics

matplotlib
The main visualization tool for Python is a
library called matplotlib.
• the most standard tool
• sufficient for most data science
• integrates well with the other libraries
• powerful, flexible
vs.
• Syntax is not intuitive
• Interactivity
pandas
Pandas is a library written for the Python programming language for
data manipulation and analysis.
• Offers data structures and operations for manipulating numerical
tables and time series.
• Visualizations
○ wrapper‐around matplotlib
Typically, you make an image by calling the plot() method on a Pandas
object, and Pandas does all the image formatting under the hood.
Then you use matplotlib’s pyplot module for things such as setting the
title and the final act of either displaying the image or saving it to a file.
Numpy
Numpy is a library for the Python programming language,
adding support for large, multi-dimensional arrays and
matrices, along with a large collection of high-level
mathematical functions to operate on these arrays.
○ Highly optimized library use for numerical
computation
Scikit-learn
Scikit-learn is a free software machine learning library for
the Python programming language
• features various classification, regression and clustering
algorithms including support vector machines, random
forests, gradient boosting, k-means and DBSCAN,
• designed to interoperate with the Python numerical and
scientific libraries NumPy and SciPy.
Iris Dataset
An example
Import the relevant libraries and create a DataFrame containing the sample
dataset (which comes built‐in to scikit‐learn)
![[iris-dataset.png]]
But first, some data exploration
Let’s break it down: Load Iris dataset
from sklearn.datasets import load_iris
iris= load_iris()

Now we have loaded iris dataset let’s print its features/input and
target/output
```python 
# Store features matrix in X
X= iris.data
#Store target (labels) vector in
y= iris.target
# Names of features/columns in iris dataset
print(iris.feature_names)

# Output : ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)’]
# we have 4 features in the iris dataset named Sepal, Petal, length, and width

# Names of target/output in iris dataset
print(iris.target_names)
Output : ['setosa' 'versicolor' 'virginica']

# size of feature matrix
print(iris.data.shape)
Output: (150, 4)
#we have total of 150 rows/samples/observations/records/instances and 4 features/columns

# size of target vector
print(iris.target.shape)
# Output : (150,)
# Iris target stores 150 samples of iris species

#how is it stored?
# Feature Data
#let's print features
print(iris.data)
Output
[[5.1 3.5 1.4 0.2]
[4.9 3. 1.4 0.2]
[4.7 3.2 1.3 0.2]
[4.6 3.1 1.5 0.2]
[5. 3.6 1.4 0.2] ...}
#5.1 is sepal length (cm), 3.5 is sepal width (cm), … etc

# Target Data
# print target vector iris species: 0 = setosa, 1 = versicolor, 2 = virginica
# print(iris.target)
#Output :
# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1
# 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2
# 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]

#This built-in iris dataset is stored as a NumPy array so first let’s convert it into a data frame using pandas
#Converting into Dataframe
import pandas as pd
import numpy as np
df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
columns= iris['feature_names'] + ['Species'])

#Get Distribution of Iris species in the target variable
df['Species'].value_counts()
Output :
2.0 50
1.0 50
0.0 50
Name: Species, dtype: int64
# Here we can see the distribution of each Iris species is equal (50 samples for each class
# where the total of 3 classes is 150 samples

#Describe function
# Using the describe function give us statistical information about our dataset
df.describe()
```
![[describe-dataset.png]]
Central tendency tells you about the centers of the data. Useful measures include the mean
(sum of all values divided by the number of values), median (middle value when all numbers
are arranged in order), and mode (most frequent value).
Variability tells you about the spread of the data. Useful measures include variance (how far each value in
the dataset is from the mean, on average) and standard deviation (the square root of the variance).
Standard Deviation of the Values
Pie Charts
One of the clearest ways to present data:
• making sense of the numbers requires cognitive effort and attention.
• on the other hand, we make immediate sense of pie charts with less mental
effort.
• conveys information in a way that the human brain will understand and care
about.
• Where to use?
• doing exploratory analysis of a dataset (How many of our customers are senior
citizens? How many of the page views came from Leeds?)
• communicating the results of a classifier.

#### Pie chart
```python
#Generating a basic pie chart :
#calling the plot() method on a Pandas Series object whose index gives us the flower species
sums_by_species = df.groupby('species').sum()
var = 'sepal width (cm) '
sums_by_species [var]. plot (kind= 'pie', fontsize=20) plt.ylabel(var, horizontal alignment='left') plt.title('Breakdown for ' + var, fontsize=25) plt.savefig('iris_pie_for_one_variable.jpg')
plt.close()

#calling plot() on a DataFrame with multiple columns, we can generate a different chart for each column all in the same figure
sums_by_species = df.groupby('species').sum() sums_by_species.plot(kind='pie', subplots=True layout= (2, 2),legend=False)
plt.title('Total Measurements, by Species') plt.savefig('iris_pie_for_each_variable.jpg')
plt.close()
```
When to use:
What data types is it for:
Example:
#### Bar chart
```python
#Generating a basic bar chart:
#relative sizes of the different flowers
sums_by_species = df.groupby('species').sum() 
var = 'sepal width (cm)'
sums_by_species [var].plot (kind='bar', fontsize=15, rot=30)

plt.title('Breakdown for' + var , fontsize=20) plt.savefig('iris_bar_for_one_variable.jpg')
plt.close()

sums_by_species = df.groupby('species').sum() 
sums_by_species.plot(kind='bar', subplots=True, fontsize=12)

plt.suptitle('Total Measurements, by Species')
plt.savefig('iris_bar_for_each_variable.jpg')
plt.close()
```
When to use:
What data types is it for:
Example:
#### Historgram
a sense of whether there are a few distinct outliers
• how much variation is in the population etc.
```python
#Generating histograms for all the columns and put them together in one figure :
df.plot(kind='hist', subplots=True, layout = (2,2) plt.suptitle('Iris Histograms', fontsize=20)
plt.show()

#plotting each species separately, but on the same axes and in different colors:
for spec in df ['species'].unique(): 
	forspec = df [df ['species'] == spec] 
	forspec ['petal length (cm) '].plot(kind='hist', alpha=0.4, label=spec) 

plt.legend (loc='upper right') 
plt.suptitle('Petal Length by Species')
plt.savefig('iris_hist_by_spec.jpg')
```
When to use:
What data types is it for:
Example:
### Means, Standard Deviations, Medians, and Quantiles
 summarize a distribution down to just a few meaningful numbers
• based on the assumption that your data’s distribution is bell‐shaped, and your goal is to give some idea of where
the peak of the bell is and how widely it spreads.
1) Give the mean and standard deviation. These are the more historically popular metrics, and they are much easier to
compute.
2) Give the median, 25th percentile, and 75th percentile. These metrics are more robust to pathologies in the data, but
they are computationally more expensive.
```python
col = df ['petal length (cm) ']
Average = col.mean()
Std col.std()
Median = col.quantile (0.5) 
Percentile25 = col.quantile (0.25)
Percentile75 = col.quantile (0.75)
```
The handful of very large outliers have pulled the mean well to
the right of the actual hump in the distribution.
For a real‐world distribution, there always is a mean.
But when there are massive outliers, just a single data point is
likely to dominate the value of the mean and standard
deviation.

A common solution is to, before calculating the mean, throw out all data points that are deemed to
be outliers: a common criterion is anything below the 25th percentile or above the 75th percentile.
```python
Clean_Avg = col[(col>Percentile25)&(col<Percentile75)].mean()
```
• For some cases though, the outliers are extremely important:
the amount of money in a transaction -> for fraud detection
#### Box plots
Boxplots are a convenient way to summarize a dataset by showing the median, quantiles, and
min/max values for each of the variables.
![[iqr-ex.png]]
```python
#boxplot of the sepal length for each of the species in the iris dataset:

col = 'sepal length (cm)'
df['ind'] = pd.Series(df. index).apply(lambda i: i%50) 
df.pivot('ind', 'species') [col].plot (kind='box') 
plt.show()
```
An advantage of boxplots is that major
outliers are very visually obvious
The distance from the 25th to
the 75th percentiles is known
as the "interquartile range"
and abbreviated as IQR
The boxplot makes it obvious that the three species
are different from one another
When to use:
What data types is it for:
Example:
### Scatter Plots
Tips for better visualization:
• Color coding. Often data points that fall into different categories are given different colors.
• Size. Changing the size of data points communicates another dimension of information, similarly
to color coding.
• Opacity. In scatterplots and other visualizations, it is often useful to make things partially
transparent in case they overlap with other parts of the visualization.
By passing optional arguments into the scatter() function:
c: A string indicating the color to make the dots. You can also pass in a sequence of such strings if
the dots are to be of different colors.
s: The size that each point should be, in pixels. Alternatively, you can pass in a sequence of sizes.
marker: A string indicating what marker should be used in the plot.
alpha: The transparency
![[scatter-plot-linear-axis.png]]
```python
df.plot(kind="scatter", x="sepal length (cm) ", y="sepal width (cm) ") plt.title("Length vs Width")
plt.show()
plt.close()

colors = ["r", "g", "b"]
markers= [".", "*", "^"]
fig, ax = plt.subplots(1, 1)
for i, spec in enumerate (df ['species'].unique() ) :
	ddf = df [df ['species'] == spec]
	ddf.plot (kind="scatter", x="sepal width (cm) ", y="sepal length (cm)", alpha=0.5, s=10* (i+1), ax=ax, color=colors [i], marker-markers [i], label-spec)

plt.legend()
plt.show()
```
Iris setosa flowers stand apart as having
sepals that are markedly longer and
narrower
When to use:
What data types is it for:
Example:
#### Scatterplots with Logarithmic Axes
A key variation on scatterplots is using logarithmic axes. In many applications, the numbers being plotted
are all positive (or at least nonnegative), but they can vary by orders of magnitude.
In a scatterplot of data such as this, all but the largest data points will be squashed to one side, making the
plot essentially unreadable.
A scatterplot of the crime rate in a neighborhood versus the median home value

Note how almost all of the data points are squashed to the left,
making the graph hard to read for all but the most high‐crime
neighborhoods.
Instead, we could have made the x‐axis logarithmic:

| Normal                            | Log                              |
| --------------------------------- | -------------------------------- |
| ![[scatter-plot-normal-axis.png]] | ![[scatter-plot-log-axis 1.png]] |
We make a logarithmic plot by taking the log of the raw data
and using that to tell where to place the points.
Logarithmic plots only work when all values are greater than 0.
Note how almost all of the data points are squashed to the left, making the
graph hard to read for all but the most high‐crime neighborhoods.
With this rescaling, we can see that there is a clear inverse relationship between
crime rate and median home value that exists across all levels of crime.
Taking the log of the raw data ensures that spacing between points corresponds
correctly to orders of magnitude.

When to use:
What data types is it for:
Example:
### Scatter Matrices
scatterplot comparing every pair of features, arranging them in what’s sometimes called a “scatter matrix”
![[scatter-matrix.png]]
Note that along the diagonal, we have a histogram of
each feature, rather than a scatterplot of the feature
versus itself (which would just be a straight line).

When to use:
What data types is it for:
Example:
### HeatMaps
scatterplots is that they can become visually cluttered
if you have a lot of data points.
• You may reduce the size of the data points
• alpha parameter to adjust the transparency of the
points
Or: we don’t actually care about the actual points
themselves. We care about the density of points in the
different regions. -> we can visualize this with a
heatmap which color‐codes different regions of the
plane by their relative density of points.

When to use:
What data types is it for:
Example:
### Time Ser
### Papers
## A Practical Guide to Characterising Data and Investigating Data Quality
### **Summary**

This guide is a **practical, structured handbook for data profiling**, which is the critical step of understanding and assessing data before analysis. Based on research with practitioners, it addresses the common problem of ad-hoc, superficial profiling by providing a formalised yet flexible system.

### **Core Framework:**

1.  **Six-Step Workflow:** A logical sequence to efficiently find and fix issues early.
    1.  Initial visual inspection.
    2.  Identify special values (e.g., missing codes).
    3.  Check for missing data and duplicates.
    4.  Inspect individual variables (type, distribution, plausibility).
    5.  Check relationships between variables (correlations, business rules).
    6.  Final characterisation of the cleaned dataset.

2.  **Comprehensive Task Lists:** 62 specific tasks, framed as questions, divided into:
    *   **Data Characterisation** (What *is* the data?): Covers structure, variable properties, distributions, and inter-variable relationships (e.g., clusters, trends).
    *   **Data Quality Investigation** (How *good* is the data?): Focuses on **Completeness** (missingness, coverage), **Accuracy** (extreme, invalid, implausible values), and **Consistency** (formats, units, semantics).

3.  **Practical Implementation:** Supported by a **Python package (`vizdataquality`)** that implements the workflow in Jupyter Notebooks, integrates visual analytics, and helps generate profiling reports.

### **Key Purpose:**
To transform data profiling from an informal "art" into a **rigorous, reproducible, and thorough engineering practice**, while avoiding overly rigid prescriptions. It serves as a checklist to ensure no critical profiling step is overlooked.
## Tasks and Visualizations Used for Data Profiling: A Survey and Interview Study
### **Summary**

This study investigated how data analysts perform **data profiling** (characterising data and assessing its quality) through a survey of 53 professionals and in-depth interviews with 24 of them.

### **Key Findings:**

1.  **Profiling is Often Ad-Hoc and Superficial:** Most analysts lack a rigorous, reproducible method. The number of profiling tasks performed varied widely (5 to 38 out of 44 identified). A core set of basic tasks (e.g., checking distinct values, missing data) was common, but many more advanced tasks were used only by a comprehensive minority.

2.  **Comprehensive Task List:** The study identified **44 specific profiling tasks**, more comprehensive than previous literature. These fall into:
    *   **Data Characterisation** (cardinalities, distributions, patterns).
    *   **Data Quality Investigation** (completeness, correctness).

3.  **Visualization is Underused for Quality Checks:** Visualization is employed more for characterisation than for investigating data quality. While common charts (scatter plots, bar charts, histograms) are workhorses, many other techniques are used only by a minority. Practitioners expressed a need for better guidance on applying visualization effectively.

4.  **Desire for Formalisation:** Analysts expressed a strong wish to move profiling from an "art" to a more **rigorous science**. They want **formalised processes, checklists, and rulebooks** to ensure consistency, avoid missed steps, and capture team knowledge. However, they cautioned against overly rigid "one-size-fits-all" solutions that might stifle creativity.

5.  **Exemplars of Practice:** The interviews revealed insightful and sometimes unusual uses of visualisations (e.g., treemaps to understand script dependencies, funnel plots for hospital mortality alerts, geographical maps for context and outlier checks).

### **Recommendations:**
The paper concludes that "good profiling" involves:
1.  Performing a **more comprehensive set** of profiling tasks.
2.  Making **greater and more varied use of visualization**, especially for quality assessment.
3.  **Formalising the process** through automation, checklists, and rulebooks that provide exemplars and guidance while preserving flexibility.