8. Dimensionality Reduction
High Dimensional Data
Dimensionality
Suppose we have a dataset of N observations
with d attributes.
● Can think of data as N points or vectors in
a d dimensional space
Can also say the dimensionality of dataset is d
Example table of 1 dimensional:

| Weight (lbs) | Weight (kg) |
| ------------ | ----------- |
| 113.0        | 51.3        |
| 136.5        | 61.9        |
| 153.0        | 69.4        |
3 dimension:

| Height (in) | Weight (lbs) | Weight (kg) | Age (yr) |
| ----------- | ------------ | ----------- | -------- |
| 65.8        | 113.0        | 51.3        | 17       |
| 71.5        | 136.5        | 61.9        | 21       |
| 69.4        | 153.0        | 69.4        | 18       |
3 because 2 weight
columns are
redundant.
Dimensionality
3 dimensions
In linear algebra terms, we’d observe that this matrix has rank 3.
More generally: Can think of a dataset’s dimensionality as the rank of the
matrix representing the data.

#### Visualizing High-Dimensional Data
we’ve used visualizations as a tool for
Exploratory Data Analysis.
● Works very well for 2 dimensional data.
● Gets harder as dimensionality goes above 2.
○ Can use hue, size, time, etc. to show more dimensions, but only
poorly.
To explore associations between two quantitative variables, can do scatterplots
of only those two variables at a time.

To explore clusters of similar observations: try reducing to two dimensions.
● If the rank of the data matrix is larger than 2, you will lose information!
● One idea: Pick two attributes, namely those with highest variance.
○ Intuition: More likely to differentiate observations.
● Another idea: Principal Component Analysis (in our next lecture).

You measure the width, length, area,
and perimeter of 100 rectangular
backyards.
What do you expect to be the rank of
the observed data matrix?
If less than 4, what smaller matrices
could you multiply together to recover
the data matrix?
The rank is 3. While area is redundant, it cannot be computed using linear
operations.
There are many possible matrices we could multiply to get back full results
### Singular Value Decomposition Experiment
Singular value decomposition is an important concept in linear algebra.
● We assume you have taken (or are taking) a linear algebra course.
● Will not explain in its entirety.
Instead, I’m going to present things backwards from how you’d usually learn
SVD.
● Will give a better context for what SVD means in the context of data
science.
● Not recommended as a first exposure to SVD.
#### Redundancy and Decomposition
![[redundancy-decomp-matrix.png]]
Earlier, we saw that our rectangle data could be decomposed into two matrices:
● The data without the perimeter.
● A matrix that transforms the data from 3D into 4D, computing the perimeter
in the process
Total amount of information
stored is less!
Required manual work on our
part to identify the magical
transformation matrix.
The “Singular Value Decomposition” technique will automatically do a similar
transformation for us.

What is different about this decomposition from the one we did manually?
● The 4th dimension is all zeroes.
● The other 3 dimensions of our “data” are NOT the width, length, and area.
● The SVD results are bigger (100 x 4 instead of 100 x 3, 4 x4 instead of 3 x 4).
● Transformation matrix has smaller, strange values.
● Transformation matrix is 4x4.
● Data being transformed still has 4 columns. Also no longer seems to have
width, height, and area.