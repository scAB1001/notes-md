8. Dimensionality Reduction
High Dimensional Data
Dimensionality
Suppose we have a dataset of N observations
with d attributes.
● Can think of data as N points or vectors in
a d dimensional space
Can also say the dimensionality of dataset is d
Example table of 1 dimensional:

| Weight (lbs) | Weight (kg) |
| ------------ | ----------- |
| 113.0        | 51.3        |
| 136.5        | 61.9        |
| 153.0        | 69.4        |
3 dimension:

| Height (in) | Weight (lbs) | Weight (kg) | Age (yr) |
| ----------- | ------------ | ----------- | -------- |
| 65.8        | 113.0        | 51.3        | 17       |
| 71.5        | 136.5        | 61.9        | 21       |
| 69.4        | 153.0        | 69.4        | 18       |
3 because 2 weight
columns are
redundant.
Dimensionality
3 dimensions
In linear algebra terms, we’d observe that this matrix has rank 3.
More generally: Can think of a dataset’s dimensionality as the rank of the
matrix representing the data.

#### Visualizing High-Dimensional Data
we’ve used visualizations as a tool for
Exploratory Data Analysis.
● Works very well for 2 dimensional data.
● Gets harder as dimensionality goes above 2.
○ Can use hue, size, time, etc. to show more dimensions, but only
poorly.
To explore associations between two quantitative variables, can do scatterplots
of only those two variables at a time.

To explore clusters of similar observations: try reducing to two dimensions.
● If the rank of the data matrix is larger than 2, you will lose information!
● One idea: Pick two attributes, namely those with highest variance.
○ Intuition: More likely to differentiate observations.
● Another idea: Principal Component Analysis (in our next lecture).

You measure the width, length, area,
and perimeter of 100 rectangular
backyards.
What do you expect to be the rank of
the observed data matrix?
If less than 4, what smaller matrices
could you multiply together to recover
the data matrix?
The rank is 3. While area is redundant, it cannot be computed using linear
operations.
There are many possible matrices we could multiply to get back full results
### Singular Value Decomposition Experiment
Singular value decomposition is an important concept in linear algebra.
● We assume you have taken (or are taking) a linear algebra course.
● Will not explain in its entirety.
Instead, I’m going to present things backwards from how you’d usually learn
SVD.
● Will give a better context for what SVD means in the context of data
science.
● Not recommended as a first exposure to SVD.
#### Redundancy and Decomposition
![[redundancy-decomp-matrix.png]]
Earlier, we saw that our rectangle data could be decomposed into two matrices:
● The data without the perimeter.
● A matrix that transforms the data from 3D into 4D, computing the perimeter
in the process
Total amount of information
stored is less!
Required manual work on our
part to identify the magical
transformation matrix.
The “Singular Value Decomposition” technique will automatically do a similar
transformation for us.

What is different about this decomposition from the one we did manually?
![[svd.png]]
● The 4th dimension is all zeroes.
● The other 3 dimensions of our “data” are NOT the width, length, and area.
● The SVD results are bigger (100 x 4 instead of 100 x 3, 4 x4 instead of 3 x 4).
● Transformation matrix has smaller, strange values.
● Transformation matrix is 4x4.
● Data being transformed still has 4 columns. Also no longer seems to have
width, height, and area.
#### SVD Question
For this data, it is guaranteed that the last column of UΣ is all zeros. Suppose we
simply delete it from the matrix. What else can we throw away?
A. First row of VT. **C. Last row of V^T**.
B. First column of VT. D. Last column of VT.
![[svd-no-v-t.png]]
### Singular Value Decomposition Theory
Manual Decomposition
Before, we took our data and manually created a 3D to 4D transformation matrix.
Decomposed data into:
● Truncated data.
● Transformation matrix.
![[trunc-data-and-matrix.png]]
Then we used SVD which automatically created a 3D to 4D transformation matrix.
Decomposed data into:
● Mysteriously rescaled data UΣ.
● Different transformation matrix VT.
![[svd-applied.png]]
The truth about UΣ and VT:
● Σ is a diagonal matrix. Contains the so-called “singular values” of X.
● The columns of U and V are each an orthonormal set

Diagonal Matrices and Σ
A diagonal matrix is a matrix with zeros everywhere except possibly the
diagonal.
● Multiplying by a diagonal matrix is equivalent to scalin the columns.
$$
\begin{bmatrix}  
| & | & | \\
{\rightarrow \atop c_1} & {\rightarrow \atop c_2} & {\rightarrow \atop c_3} \\  
| & | & |   
\end{bmatrix}
\begin{bmatrix}  
a_1 & 0 & 0 \\  
0 & a_2 & 0 \\
0 & 0 & a_3 
\end{bmatrix} = 
\begin{bmatrix}  
| & | & | \\
a_1{\rightarrow \atop c_1} & a_2{\rightarrow \atop c_2} & a_3{\rightarrow \atop c_3} \\  
| & | & |   
\end{bmatrix}
$$
In Σ, the singular values appear in decreasing order.
● Singular values are always non-negative.
● Singular values beyond rank r will be zero.
● Example of singular values for our rectangle data:
○ Note the 4th singular value is 0.
$$ \Sigma = 
\begin{bmatrix}  
363 & 0 & 0 & 0\\  
0 & 63 & 0 & 0\\  
0 & 0 & 25 & 0\\  
0 & 0 & 0 & 0\\  
\end{bmatrix}
$$
The truth about UΣ and VT:
● Σ is a diagonal matrix. Contains the so-called “singular values” of X.
● The columns of U and V are each an orthonormal set.
#### Orthogonality, Dot Products, Vector Length
Two orthogonal vectors:
● Meet at a right angle.
● Have a dot-product of zero.
A unit vector has length 1.
Side fact: The length of a vector v is the square
root of v • v.
$V_1 = [-1.5,3] V_2 = [2,1]$
$\text{dot}(V_1, V_2) = (-1.5)\cdot2+3\cdot1=0$
A set of vectors is said to be an orthonormal set if:
● All of the vectors are unit vectors, i.e. have length 1.
● All of the vectors are orthogonal
Challenge: Given VT, propose a procedure to verify that the
columns of V are an orthonormal set.
Give your answer in terms of dot product operations.
● Check dot products of rows with each other.
○ If we dot product a row with itself, what
should we get? 1
○ If we dot product a row with another, what
should we get? 0
Verify that:
● Dot product of any row of VT with itself is 1.
● Dot product of any row of VT with any other row is 0.
Side fact: If the matrix is square, then the transpose of such
a matrix is also its inverse.
### Low Rank Approximation Experiment
Given high dimensional data that lives on a lower dimensional subspace, we
can manually decompose by inspection into the product of:
● A lower dimensional dataset.
● A transformation matrix.
Singular Value Decomposition
Singular value decomposition automates this process, generating U, Σ and VT.
● The columns of U and V form an orthonormal set, and Σ is a diagonal
matrix containing the singular values of the matrix.
● For the purposes of comparison with our manual approach, we showed the
decomposition as UΣ and VT.
![[svd-applied.png]]
Come up with a 1 dimensional data matrix and transformation that restores the
original data.
Why isn’t this possible?
● Width and length are independent from each other
Noisy Rectangle Data
made up of width, length, area and perimiter
Suppose our rectangle area and perimeter measurements have some noise.
What is the rank of this matrix? 4. Even the small amount of noise means that perimeter is not 2w + 2l.
SVD Output on Noisy Rectangle Data
Singular values for the noisy data are:
This results in a small (but nonzero) 4th column in UΣ.
Removing the last column of UΣ and last row of VT, we get a good approximation!
Note: If you try to carry out this matrix
multiplication by hand, you’ll get different
results because I had to round off the
entries in UΣ and VT so they’d fit on the
slide
What we just did is sometimes called the “rank 3 approximation” of our data.
● We used 3 dimensions of UΣ.
Can think of our original data as informally “just barely more than rank 3”.
● Thus, our rank 3 approximation will be very good
Below we see our original data as well as the rank 3 approximation.
![[rank3-data.png]]
We can also perform a rank 2 approximation by using only the first 2 columns of
UΣ and first two rows of VT.
We can also perform a rank 1 approximation by using only the first column of UΣ
and first row of VT.
● Intriguingly, it’s not completely terrible!
Original Data and Rank 1 Approximation (Your Answers)
Do you notice any interesting patterns in the rank 1 data?
● The rows are multiples of each other -- the columns are multiples of each
other.
● Area looks like roughly 3 * (width + height).
The rank 1 approximation is just the product of a 100 x 1 “data” matrix and a
1 x 4 transformation matrix that lifts the data from 1D back up to 4D.

Impossible Challenge Revisited
Another perspective: SVD can “solve” our impossible challenge from before.
● Note: Result is not actually a solution, it’s just an approximation!
