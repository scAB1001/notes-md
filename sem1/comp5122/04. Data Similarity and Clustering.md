What will you learn?
• Calculating Data Similarity
• Data Distance Metrics
• k-Means Clustering
• Agglomerative Clustering
• Hierarchical clustering and dendrograms

Data Similarity
• Once an object can be represented as data, we can measure the similarity between
objects, or alternatively the distance between objects.
• Let’s consider we represent each object as a feature vector. Then, the closer two
objects are in the space defined by the features, the more similar they are.
• • Amazon, Netflix, Spotify use similarity to provide recommendations
“People who like X also like Y”

Data Similarity: Euclidian Distance
![[euclidian-dist.png]]
### k-Means Clustering
• k-means clustering is one of the simplest and most commonly used clustering
algorithms.
• It tries to find cluster centers that are representative of certain regions of the data.
• The algorithm alternates between two steps: assigning each data point to the closest
cluster center, and then setting each cluster center as the mean of the data points that
are assigned to it.
• The algorithm is finished when the assignment of instances to clusters no longer
changes.

![[k-means-cluster-ex.png]]
1. Cluster centers are shown as triangles, while data points are shown as circles. Colors indicate cluster membership.
2. We are looking for three clusters, so the algorithm was initialized by declaring three data points randomly as cluster centers (“Initialization”).
3. Then the iterative algorithm starts. First, each data point is assigned to the cluster center it is closest to (“Assign Points”).
4. Next, the cluster centers are updated to be the mean of the assigned points (“Recompute Centers”).
5. Then the process is repeated two more times. After the third iteration, the assignment of points to cluster centers remained unchanged, so the algorithm stops. Given new data points, k- means will assign each to the closest cluster center.
6. Cluster centers and cluster boundaries found by the k- means algorithm.

#### In another example
![[k-means-cluster-ex2.png]]
Above, we see the results after iteration 4 and 5:
● Centers moved slightly between iteration 4 and 5.
● But no points changed color.
● Are we done?
○ Yes! If we tried iteration 6, we’d see that
centers don’t move at all.

### k-Means Clustering (Python Example)
We instantiate the k-Means class, and set the number of clusters we are looking for. Then we call the fit
method with the data:
```python
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
# generate synthetic two-dimensional data
X, y = make_blobs(random_state=1)
# build the clustering model
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
```
During the algorithm, each training data point in X is assigned a cluster label. You can find these labels
in the `kmeans.labels_` attribute:
`print("Cluster memberships:\n{}".format(kmeans.labels_))`
Cluster memberships:
[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2
2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1
1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]
As we asked for three clusters, the clusters are numbered 0 to 2.

You can also assign cluster labels to new points, using the predict method. Each new point is assigned
to the closest cluster center when predicting, but the existing model is not changed.
Running predict on the training set returns the same result as labels_:
`print(kmeans.predict(X))`
[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2
2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1
1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]
You can see that clustering is somewhat similar to classification, in that each item gets a label.
However, there is no ground truth, and consequently the labels themselves have no a priori meaning.
No significance to the fact that one group was labeled 0 and another one was labeled 1. Running the
algorithm again might result in a different numbering of clusters because of the random nature of the
initialization.
Cluster assignments and cluster
centers found by k-means with
three clusters

Each time you run K-Means, you get a different output, depending on where
centers started
Which is best?
● One approach: Define some sort of loss function.
### Loss Functions: Inertia and Distortion
To evaluate different clustering results, we need a loss function.
Two common loss functions:
● Inertia: Sum of squared distances from each data point
to its center.
● Distortion: Weighted sum of squared distances from
each data point to its center.
![[loss-funcs.png]]
Example:
● Inertia: $0.47^2+0.19^2+0.34^2+0.25^2+0.58^2+0.36^2+0.44^2$
● Distortion: $(0.47^2+0.19^2+0.34^2)/3 + (0.25^2+0.58^2+0.36^2+0.44^2)/4$
![[inertia-vs-distortion.png]]
Among these three choices, our inertia loss function says that the leftmost clustering is best (inertia: 44.96)
and rightmost clustering (inertia: 54.35) is worst.
#### Failure cases of k-means
Even if you know the “right” number of clusters for a given dataset, k-means might not always be able to recover them. Each cluster is defined solely by its center, which means that each cluster is a convex shape. As a result of this, k-means can only capture relatively simple shapes. 

k-means also assumes that all clusters have the same “diameter” in some sense; it always draws the boundary between clusters to be exactly in the middle between the cluster centers. -> does not always work

1. Both cluster 0 and cluster 1 have some points that are far away from all the other points in these clusters that “reach” toward the center.
![[k-means-fail-1.png]]
*Cluster assignments found by k-means when clusters have different densities*

2. k-means also assumes that all directions are equally important for each cluster.
The groups shown are stretched toward the diagonal. As k-means only considers the distance to the nearest cluster center, it can’t handle this kind of data
![[k-means-fail-2.png]]
*k-means fails to identify nonspherical clusters*

3. k-means for complex shapes
![[k-means-fail-3.png]]
*k-means also performs poorly if the clusters have more complex shapes*
#### Vector quantization, or seeing k-means as decomposition
k-means tries to represent each data point using a cluster center.
• You can think of that as each point being represented using only a single component, which is given by the cluster center. This view of k-means as a decomposition method, where each point is represented using a single component, is called vector quantization.
• An interesting aspect of vector quantization using k-means is that we can use many more
clusters than input dimensions to encode our data.

• An interesting aspect of vector quantization using k-means is that we can use many more clusters than input dimensions to encode our data
#### K-Means advantages and disadvantages 
k-means is a very popular algorithm for clustering, not only because it is relatively easy to understand and implement, but also because it runs relatively quickly. K-means scales easily to large datasets. 

One of the drawbacks of k-means is that it relies on a random initialization, which means the outcome of the algorithm depends on a random seed. 

Further downsides of k-means are the relatively restrictive assumptions made on the shape of clusters, and the requirement to specify the number of clusters you are looking for (which might not be known in a real-world application).
#### Agglomerative Clustering
Agglomerative clustering refers to a collection of clustering algorithms that all build upon the
same principles: the algorithm starts by declaring each point its own cluster, and then merges the
two most similar clusters until some stopping criterion is satisfied.
• e.g. the number of clusters: similar clusters are merged until only the specified number of
clusters are left.
There are several linkage criteria that specify how exactly the “most similar cluster” is measured.
This measure is always defined between two existing clusters.
Agglomerative clustering iteratively joins the two closest clusters

*Initialisation to step 4*
![[agglomerative-1.png]]
Initially, each point is its own cluster. Then, in each step, the two clusters that are closest are merged. In the first four steps, two single-point clusters are picked and these are joined into two-point clusters

*Step 5 to 9*
![[agglomerative-2.png]]
In step 5, one of the two-point clusters is extended to a third point, and so on. In step 9, there are only three clusters remaining.
As we specified that we are looking for three clusters, the algorithm then stops.
• Because of the way the algorithm works, agglomerative clustering cannot make predictions for new data points.

*Cluster assignment using agglomerative clustering with three clusters*
![[agglomerative-3.png]]
#### Hierarchical clustering and dendrograms
Agglomerative clustering produces what is known as a hierarchical clustering.
The clustering proceeds iteratively, and every point makes a journey from being a single point cluster to
belonging to some final cluster.
Each intermediate step provides a clustering of the data (with a different number of clusters).
• It is sometimes helpful to look at all possible clusterings jointly.

![[hierarchical-clustering.png]]
An overlay of all the possible clusterings providing some insight into how each cluster breaks up into smaller clusters. Hierarchical cluster assignment (shown as lines) generated with agglomerative clustering, with numbered data points

While this visualization provides a very detailed view of the hierarchical clustering, it relies on the two-
dimensional nature of the data and therefore cannot be used on datasets that have more than two
features.
There is, however, another tool to visualize hierarchical clustering, called a dendrogram, that can handle
multidimensional datasets.
SciPy provides a function that takes a data array X and computes a linkage array, which encodes
hierarchical cluster similarities. We can then feed this linkage array into the scipy dendrogram function
to plot the dendrogram.

```python
# Import the dendrogram function and the ward clustering function from SciPy
from scipy.cluster.hierarchy import dendrogram, ward
X, y = make_blobs(random_state=0, n_samples=12)

# Apply the ward clustering to the data array X
# Ward() returns an [] of distances bridged after agglomerative clustering
linkage_array = ward(X)

# Plot the dendrogram for linkage_array with the distances between clusters
dendrogram(linkage_array)

# Mark the cuts in the tree that signify two or three clusters
ax = plt.gca()
bounds = ax.get_xbound()
ax.plot(bounds, [7.25, 7.25], '--', c='k')
ax.plot(bounds, [4, 4], '--', c='k')
ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})
ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})
plt.xlabel("Sample index")
plt.ylabel("Cluster distance")
```

![[dendogram.png]]
1. The dendrogram shows data points as points on the bottom (numbered from 0 to 11). Then, a tree is plotted with these points (representing single- point clusters) as the leaves, and a new node parent is added for each two clusters that are joined.
2. Reading from bottom to top, the data points 1 and 4 are joined first. Next, points 6 and 9 are joined into a cluster, and so on. At the top level, there are two branches, $L:\{R:\{11, 0, 5, 10, 7, 6, 9\}$. These correspond to the two largest clusters in the lefthand side of the plot.
3. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algorithm two clusters get merged. The length of each branch also shows how far apart the merged clusters are. The longest branches in this dendrogram are the three lines that are marked by the dashed line labeled “three clusters.”
4. That these are the longest branches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the two remaining clusters into a single cluster again bridges a relatively large distance. Unfortunately, agglomerative clustering still fails at separating complex shapes.