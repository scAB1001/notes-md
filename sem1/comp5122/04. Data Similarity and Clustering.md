What will you learn?
• Calculating Data Similarity
• Data Distance Metrics
• k-Means Clustering
• Agglomerative Clustering
• Hierarchical clustering and dendrograms

Data Similarity
• Once an object can be represented as data, we can measure the similarity between
objects, or alternatively the distance between objects.
• Let’s consider we represent each object as a feature vector. Then, the closer two
objects are in the space defined by the features, the more similar they are.
• • Amazon, Netflix, Spotify use similarity to provide recommendations
“People who like X also like Y”

Data Similarity: Euclidian Distance
![[euclidian-dist.png]]
### k-Means Clustering
• k-means clustering is one of the simplest and most commonly used clustering
algorithms.
• It tries to find cluster centers that are representative of certain regions of the data.
• The algorithm alternates between two steps: assigning each data point to the closest
cluster center, and then setting each cluster center as the mean of the data points that
are assigned to it.
• The algorithm is finished when the assignment of instances to clusters no longer
changes.

![[k-means-cluster-ex.png]]
1. Cluster centers are shown as triangles, while data points are shown as circles. Colors indicate cluster membership.
2. We are looking for three clusters, so the algorithm was initialized by declaring three data points randomly as cluster centers (“Initialization”).
3. Then the iterative algorithm starts. First, each data point is assigned to the cluster center it is closest to (“Assign Points”).
4. Next, the cluster centers are updated to be the mean of the assigned points (“Recompute Centers”).
5. Then the process is repeated two more times. After the third iteration, the assignment of points to cluster centers remained unchanged, so the algorithm stops. Given new data points, k- means will assign each to the closest cluster center.
6. Cluster centers and cluster boundaries found by the k- means algorithm.

#### In another example
![[k-means-cluster-ex2.png]]
Above, we see the results after iteration 4 and 5:
● Centers moved slightly between iteration 4 and 5.
● But no points changed color.
● Are we done?
○ Yes! If we tried iteration 6, we’d see that
centers don’t move at all.

### k-Means Clustering (Python Example)
We instantiate the k-Means class, and set the number of clusters we are looking for. Then we call the fit
method with the data:
```python
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
# generate synthetic two-dimensional data
X, y = make_blobs(random_state=1)
# build the clustering model
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
```
During the algorithm, each training data point in X is assigned a cluster label. You can find these labels
in the `kmeans.labels_` attribute:
`print("Cluster memberships:\n{}".format(kmeans.labels_))`
Cluster memberships:
[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2
2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1
1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]
As we asked for three clusters, the clusters are numbered 0 to 2.

You can also assign cluster labels to new points, using the predict method. Each new point is assigned
to the closest cluster center when predicting, but the existing model is not changed.
Running predict on the training set returns the same result as labels_:
`print(kmeans.predict(X))`
[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2
2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1
1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]
You can see that clustering is somewhat similar to classification, in that each item gets a label.
However, there is no ground truth, and consequently the labels themselves have no a priori meaning.
No significance to the fact that one group was labeled 0 and another one was labeled 1. Running the
algorithm again might result in a different numbering of clusters because of the random nature of the
initialization.
Cluster assignments and cluster
centers found by k-means with
three clusters

Each time you run K-Means, you get a different output, depending on where
centers started
Which is best?
● One approach: Define some sort of loss function.
### Loss Functions: Inertia and Distortion
To evaluate different clustering results, we need a loss function.
Two common loss functions:
● Inertia: Sum of squared distances from each data point
to its center.
● Distortion: Weighted sum of squared distances from
each data point to its center.
![[loss-funcs.png]]
Example:
● Inertia: $0.47^2+0.19^2+0.34^2+0.25^2+0.58^2+0.36^2+0.44^2$
● Distortion: $(0.47^2+0.19^2+0.34^2)/3 + (0.25^2+0.58^2+0.36^2+0.44^2)/4$
![[inertia-vs-distortion.png]]
Among these three choices, our inertia loss function says that the leftmost clustering is best (inertia: 44.96)
and rightmost clustering (inertia: 54.35) is worst.
#### Failure cases of k-means
Even if you know the “right” number of clusters for a given dataset, k-means might not always be
able to recover them.
Each cluster is defined solely by its center, which means that each cluster is a convex shape. As a
result of this, k-means can only capture relatively simple shapes.
k-means also assumes that all clusters have the same “diameter” in some sense; it always draws
the boundary between clusters to be exactly in the middle between the cluster centers.
-> does not always work

Both cluster 0 and
cluster 1 have some
points that are far
away from all the
other points in these
clusters that “reach”
toward the center.
![[k-means-fail-1.png]]
*Cluster assignments found by k-means when clusters have different densities*

k-means also assumes that all directions are equally important for each cluster.
The groups shown are stretched toward the diagonal. As k-means only considers the distance to the nearest cluster center, it can’t handle this kind of data

*k-means fails to identify nonspherical clusters*

k-means also assumes
that all directions are
equally important for
each cluster.
The groups shown are
stretched toward the
diagonal. As k-means
only considers the
distance to the nearest
cluster center, it can’t
handle this kind of
data

#### Vector quantization, or seeing k-means as decomposition
k-means tries to represent each data point using a cluster center.
• You can think of that as each point being represented using only a single component, which is
given by the cluster center. This view of k-means as a decomposition method, where each
point is represented using a single component, is called vector quantization.
• An interesting aspect of vector quantization using k-means is that we can use many more
clusters than input dimensions to encode our data.

• An interesting aspect
of vector quantization
using k-means is that
we can use many
more clusters than
input dimensions to
encode our data