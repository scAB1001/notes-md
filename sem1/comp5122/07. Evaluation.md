7. Evaluation
Going back to the Iris dataset:
Making Predictions:
After we train our classification model, we can now make predictions using this model on new data for
which we might not know the correct labels.
Imagine we found an iris in the wild with a sepal length of 5 cm, a sepal width of 2.9 cm, a petal length
of 1 cm, and a petal width of 0.2 cm.
What species of iris would this be?
We have one sample / flower with 4 features / attributes

Making Predictions:
To make a prediction, we call the predict method of the knn object:
```python
prediction = knn.predict(X_new)
print("Prediction: {}".format(prediction))
print("Predicted target name: {}".format(
iris_dataset['target_names'][prediction]))
```
Prediction: [0]
Predicted target name: ['setosa’]
Our model predicts that this new iris belongs to the class 0, meaning its species is Setosa.
how do we know whether we can trust our model?

### Evaluating the Model:
This is where the test set that we created earlier comes in. This data was not used to build the model, but we do know what the correct species is for each iris in the test set. 
-> we can make a prediction for each iris in the test data and compare it against its label (the known species).
We can measure how well the model works by computing the accuracy, which is the fraction of flowers for which the right species was predicted:
```python
y_pred = knn.predict(X_test)
print("Test set predictions:\n {}".format(y_pred))

#Test set predictions:
[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]

print("Test set score: {:.2f}".format(np.mean(y_pred == y_test)))
#Test set score: 0.97
#Also use the score method of the knn obj, to compute the test set accuracy:
print("Test set score: {:.2f}".format(knn.score(X_test, y_test)))
# Test set score: 0.97
```
For this model, the test set accuracy is about 0.97, which means we made the right prediction for 97%
of the irises in the test set.
-> we can expect our model to be correct 97% of the time for new irises.
-> this high level of accuracy means that our model may be trustworthy enough to use.

We **evaluated** the model using the score method, which computes **the accuracy** of the model. We applied the score method to the test set data and the test set labels and found that our model is about **97% accurate, meaning it is correct 97% of the time on the test set**.
This gave us the **confidence** to apply the model to new data (in our example, new flower
measurements) and **trust that the model will be correct about 97% of the time**.

-> The accuracy metric computes how many times a model made a correct prediction across the entire
dataset. This can be a reliable metric only if the dataset is class-balanced; that is, each class of the
dataset has the same number of samples.
Nevertheless, real-world datasets are heavily class-imbalanced, often making this metric unviable.
### Confusion Matrices
Confusion matrices are often better at evaluating classifiers than accuracy metrics are, especially when dealing with **unbalanced** datasets.
Consider building a classifier that aims to predict the prevalence of a rare disease. If only 5% of the people have the disease, a classifier that always predicts healthy individuals will achieve an accuracy score of 95%.
However, the classifier is relatively useless since it never predicts any instances of the disease.
![[confusison-matrix.png]]
The specificity is also known as the true
negative rate (TNR). The specificity
evaluates how many cases we correctly
identified as false out of all the real
negative cases.
Another way to think about the specificity
is whether or not the classifier raises a flag
in the specific cases we want it to. The
specificity is intuitively the ability of the
classifier to find all the negative samples.
The specificity aims to minimize false
positives.
As long as the classifier identifies all
negative cases, the specificity will be 1.0,
even if false negatives exist.

The precision answers the question, “What
is the value of a hit?”
The precision is appropriate when we want
to minimize false positives and is a metric
that quantifies the number of correct
positive predictions made. The precision is
intuitively the ability of the classifier not to
label as positive a sample that is negative

The recall is also known as the sensitivity or
true positive rate (TPR) and is appropriate
when focusing on minimizing false
negatives.
Imagine a classifier that predicts whether a
web page results from a search engine
request. The recall answers the question,
“Of the valuable web page results, how
many did the classifier identify or recall
correctly?”
Intuitively, the recall is the classifier’s
ability to find all the positive samples.

Precision and recall offer a trade-off, i.e.,
one metric comes at the cost of another.
The F1 score combines precision and recall
using their harmonic mean, and
maximizing the F1 score implies
simultaneously maximizing both precision
and recall.
$$F1_{score}={2 \over \text{Precision}^{-1}+\text{Recall}^{-1}} = {2\cdot\text{Precision}\cdot\text{Recall} \over \text{Precision}+\text{Recall}}$$
#### Measuring Accuracy Using Cross Validation
More strategies to overcome overfitting:
One solution is to divide the data into another hold-out set, called the “validation set,” which provides evaluation after the
model completes training.
A disadvantage of this approach is that it reduces the number of samples available to train the model since it partitions the
data into three sets: training, validation, and test sets.
➔ cross-validation (k-fold)
![[cross-validation.png]]
Split the training data into k smaller groups.
Train a model using k- 1 of the folds as training data
Use the remaining part of the data to validate the model
Compute a performance measure such as accuracy
The performance measure reported by k-fold CV is the average of the values computed in the loop