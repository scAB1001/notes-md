6. Regression and Model Fitting
What will you learn?
• Define what regression is
• Fit a model
• Fit a Polynomial Curve
• Define what overfitting is
• Prevent overfitting
• Learn about Decision Trees and overfitting
• Restrict Decision Tree Complexity
• Define what Random Forest is
• Use Decision Trees for Regression
Regression
• The goal of regression is to predict the value of one or more continuous target
variables t given the value of a D-dimensional vector x of input variables.
• Regression is similar to classification: you have a number of input features, and you
want to predict an output feature. In classification, this output feature is either
binary or categorical. With regression, it is a real‐valued number.
Typically, regression algorithms model the output as a linear combination of the
inputs.

• Given the following values of X and Y, what is the value of Y when X = 5.(1,1), (2,2),
(4,4), (100,100), (20, 20)
• The answer is : 5. Not very difficult, right?
• Now, let’s take a look at different example. Say you have the following pairs of X and
Y. Can you calculate the value of Y, when X = 5?
• (1,1), (2,4), (4,16), (100,10000), (20, 400)
• The answer is : 25. Was it difficult?

• Let’s understand a bit as to what happened in the above examples. When we look
at the first example, after looking at the given pairs, one can establish that the
relationship between X and Y is Y = X. Similarly, in the second example, the
relationship is Y = X\*X.
• In these two examples, we can determine the relationship between two given
variables (X and Y) because we can easily identify the relationship between them.
• Overall, machine learning works in the same way.
• Your computer looks at some examples and then tries to identify “the most suitable” relationship
between the sets X and Y. Using this identified relationship, it will try to predict (or more) for new
examples for which you don’t know Y.
• Detecting whether a region of an image is a face or not -> Classification
• Predicting the coordinates of the bounding box around the face -> Regression

The standard way to fit a line is called least squares. In Python, it can be fit
using the Linear Regression class in the example scripts, and the fit coefficients
can be found in the following way:
Least squares works by picking the values of m and b that minimize the “penalty
function,” which adds up an error term across all of the points:
$$ L=\sum_{\substack{i}}(y_i-(mx_i+b))^2 $$
The simplest example of regression is one that you probably saw in high school:
fitting a line to data. You have a collection of x/y pairs, and you try to fit a line
to them of the form: $y = mx + b$
Plot the points out, fit a line to them by eye, trace the line with a ruler, and use that to
pull out $m$ and $b$.
Each of the four datasets has the same line of best fit and the same quality of fit.

The key thing to understand here is that this penalty function makes least squares
regression extremely sensitive to outliers in the data: three deviations of size 5 will give
a penalty of 75, but just a single larger deviation of size 10 will give the larger penalty of
size 100. Linear regression will bend the parameters so as to avoid large deviations of
even a single point, which makes it unsuitable in situations where a handful of large
deviations are to be expected.
An alternative approach that is more suitable to data with outliers is to use the penalty
function: $$ L=\sum_{\substack{i}} \lvert y_i-(mx_i+b)\rvert$$
where we just take the absolute values of the different error terms and add
them. This is called “L1 regression,” among other names. Outliers will still have
an impact, but it is not as egregious as with least squares. On the other hand,
L1 regression penalizes small deviations from expectation more harshly compared
to least squares, and it is significantly more complicated to implement
computationally

Fitting Nonlinear Curves
Fitting a curve to data is a ubiquitous problem not just in data science but in
engineering and the sciences in general. Often, there are good a priori reasons
that we expect a certain functional form, and extracting the best‐fit parameters
will tell us something very meaningful about the system we are studying. A few
Examples:
Exponential decay to some baseline. This is useful for modeling many processes
where a system starts in some kind of agitated state and decays to a
baseline: $y=ae^{bx}$

Logistic growth, which is useful in biology for modeling the population density
of organisms growing in a constrained environment that can only support
so many individuals: $y=a{e^{bx}\over c+e^{bx}}$

• Example: Polynomial Curve Fitting
• Suppose we observe a real-valued input variable x and we wish to use this
observation to predict the value of a real-valued target variable t.
• -> example using synthetically generated data
• generated from the function sin(2πx) with random noise included in the target
values

Regression
• suppose that we are given a training set comprising $N$ observations of $x$, written 
$x ≡ (x_1, . . . , x_N)^T$, together with corresponding observations of the values of t, denoted 
$t≡ (t_1, . . . , t_N)^T$.
• Note: $≡$ identical to
• a training set comprising $N = 10$ data points
• xn, for n = 1, . . . , N, spaced uniformly in range [0, 1]
• t was obtained by first computing the corresponding
values of the function sin(2πx) and then adding a small
level of random noise having a Gaussian distribution

• Our goal is to exploit this training set in order to make predictions of the value t̂
• of the target variable for some new value ̂ x of the input variable
• → discover the underlying function sin(2πx)
• → not easy task: we have to generalize from a finite
dataset

$y(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M= \sum_{\substack{j=0}}w_jx^j$
• curve fitting
• fit the data using a polynomial function of the form where M is the order of the
polynomial, and xj denotes x raised to the power of j.
• The polynomial coefficients w0, . . . , wM are collectively denoted by the vector w.
• Note that, although the polynomial function y(x,w) is a nonlinear function of x, it is
a linear function of the coefficients w
→ Functions, such as the polynomial, which are linear in the unknown parameters
have important properties and are called linear models
• The values of the coefficients will be determined by fitting the polynomial to the
training data. This can be done by minimizing an error function that measures the
misfit between the function y(x,w), for any given value of w, and the training set
data points
![[regression-interp.png]]
*The geometrical interpretation of the sum- of-squares error function*
• One simple choice of error function, which is widely used, is given by the sum of the
squares of the errors between the predictions y(xn,w) for each data point xn and
the corresponding target values tn, so that we minimize
$$E(w)=1/2 \displaystyle\sum_{i=1}^N{\{y(x_n,w)-t_n\}^2}$$→ We can solve the curve fitting problem by choosing the value of w for which E(w) is
as small as possible.
→ There remains the problem of choosing the order M of the polynomial:
model comparison or model selection

As it progresses:
![[regression-interp-orders-of-m.png]]
We notice that the constant (M = 0) and first order (M = 1) polynomials give rather
poor fits to the data and consequently rather poor representations of the function
sin(2πx). The third order (M = 3) polynomial seems to give the best fit to the
function sin(2πx)
When we go to a much higher order polynomial (M = 9), we obtain an excellent fit
to the training data. In fact, the polynomial passes exactly through each data point
and E(w\*) = 0. However, the fitted curve oscillates wildly and gives a very poor
representation of the function sin(2πx). This latter behaviour is known as over-
fitting.
• the goal is to achieve good generalization by making accurate predictions for new
data.
 We can obtain some quantitative insight into the dependence of the generalization
performance on M by considering a separate test set comprising 100 data points
generated using exactly the same procedure used to generate the training set
points but with new choices for the random noise values included in the target
values. For each choice of M, we can then evaluate the residual value of E(w*) given
by for the training data, and we can also evaluate E(w*) for the test data set.
• we can then evaluate the residual value of E(w*) given by for the training data, and
we can also evaluate E(w*) for the test data set.
• the root-mean-square (RMS) error: $$E_{RMS}=\sqrt{wE(w^{*})/N}$$![[regression-sum-squares-reduces-over-fitting.png]]
Plots of the solutions obtained by minimizing the sum-of-squares error function using the $M=9$ polynomial for $N=15$ data points (left plot) and $N=100$ data points (right plot). We see that increasing the size of the data set reduces the over-fitting problem.
### Visualizing Our New Model
Comparing our model trained on 150 vs. 110 data points, we see slight
differences in the generated model.
● Naturally, both models get very high accuracy on the data that they use for
training (100% for non-overlapping points).
When we look at the performance of Model 2D-110 on the test set, we see that we
make some errors that aren’t just from overlapping data.
● 99% accuracy on training set (2 overlapping points). 95% accuracy on test set.
#### Multidimensional Decision Trees
Naturally, we can include even more features. For example, if we want to use
the petal AND sepal measurements, we simply train the decision tree on all four
columns of the data.
The resulting model Model 4D-110 gets:
● 100% accuracy on the training set (no overlapping data points).
● 95% accuracy on the test set.
Cannot easily visualize Model 4D-110’s decision boundaries because the
prediction space is 4 dimensional.
#### Overfitting and Sepal Data
If we use the sepal data only (no petal data), we will run into serious overfitting
issues.
Below, we see our three flowers plotted in the sepal space.
#### Sepal Decision Boundaries
After training on 110/150 points, we get the decision boundaries below.
● Decision boundaries are erratic!
● Many overlapping points leads to only 93% accuracy on training set.
Performance on test set is quite poor.
● Only 70% accuracy: 28/40 predictions are correct.
The decision boundaries for our sepal model were quite complex.
● Or drawn out as a tree, we also see a highly complex structure.
### Restricting Decision Tree Complexity
Overfitting and Our Algorithm
A “fully grown” decision tree built with our algorithm runs the risk of overfitting.
One idea to avoid overfitting: Don’t allow fully grown trees.
Regularization Terms and Decision Trees?
We can’t just use the regularization term idea from linear models.
● There is no global function being minimized.
● Instead, the decision tree is built up node by node in a “greedy” fashion.
#### Approach 1: Preventing Growth
Approach 1: Set one or more special rules to prevent growth.
Examples:
● Don’t split nodes with < 1% of the samples.
● Don’t allow nodes to be more than 7 levels deep in the tree.
Slide Ref: Principles and Techniques of Data Science, UC Berkeley
#### Approach 2: Pruning
Approach 2: Let tree fully grow, then cut off less useful branches of the tree. For
example, consider the highlighted branch below:
● Many rules that affect few points.
● How can we avoid this?
Slide Ref: Principles and Techniques of Data Science, UC Berkeley

Specific Pruning Example
One way to prune:
● Before creating the tree, set aside a
validation set.
● If replacing a node by its most common
prediction has no impact on the
validation error, then don’t split that
node.
Slide Ref: Principles and Techniques of Data Science, UC Berkeley

Overfitting and Our Algorithm
A “fully grown” decision tree built with our algorithm runs the risk of overfitting.
One idea to avoid overfitting: Don’t allow fully grown trees.
● Approach 1: Set rules to prevent full growth.
● Approach 2: Allow full growth then prune branches afterwards.
Won’t discuss these in any great detail.
● There’s a completely different idea called a “random forest” that is more
popular and IMO more beautiful.
### Random Forests
Random Forests: Harnessing Variance
As we’ve seen, fully‐grown decision trees will
almost always overfit data.
● Low model bias, high model variance.
● In other words, small changes in dataset
will result in very different decision tree.
● Example: Two models on the right trained
on different subsets of the same data.
Random Forest Idea: Build many decision
trees and have them vote.

What do we mean by vote?
![[random-forest-9-models.png]]
● For a given x/y, use
whichever prediction is
most popular.
Consider example with
9 models.
● 6 votes orange, 3 votes
green.
● Random forest prediction is
orange.

Building Many Trees
Big fundamental problem: We only have one training set.
How can we build many trees using one training set?
#### Bagging
Bagging: Short for Bootstrap AGGregatING.
● Generate bootstrap resamples of training data.
● Fit one model for each resample.
● Final model = average predictions of each small model.
● Invented by Leo Breiman in 1994 (Berkeley Statistics!)
● Bagging often isn’t enough to reduce model variance!
○ Decision trees often look very similar to each other.
○ E.g. one strong feature always used for first split.
● Idea: Only use a sample of m features at each split.
○ Usually m = sqrt(p) for decision trees used for classification.
○ Here p is the number of features.
● Algorithm creates individual trees, each overfit in a different way.
○ The hope is that the the overall forest has low variance
#### Random Forests Algorithm
● Bootstrap training data T times. For each resample, fit a decision tree by
doing the following:
○ Start with data in one node. Until all nodes pure:
○ Pick an impure node.
○ Pick a random subset of m features. Pick the best feature x and split
value β such that the loss of the resulting split is minimized, e.g. x =
petal_width, β = 0.8 has loss 0.66.
○ Split data into two nodes, one where x < β, and one where x ≥ β.
● To predict, ask the T decision trees for their predictions and take majority
vote.
This approach has two hyperparameters T and m.

Avoiding Overfitting with Heuristics
We’ve seen many approaches to avoid overfitting decision trees.
● Preventing growth.
● Pruning.
● Random forests.
These ideas are generally “heuristic”.
● Not provably best or mathematically optimal.
● Instead, they are just ideas that somebody thought sounded good,
implemented, then found to work in practice acceptably well.

Why Random Forests?
● Versatile: does both regression and classification.
● Invariant to feature scaling and translation.
● Automatic feature selection.
● Nonlinear decision boundaries without complicated feature engineering.
● Doesn’t overfit as often as other nonlinear models (e.g. polynomial
features).
● Example of ensemble method: combine the knowledge of many simple
models to make a sophisticated model.
● Example of using bootstrap to reduce model variance.
### Decision Trees for Regression
logistic regression model
can be usedfor classification.
Decision trees can be an alternative technique for classification.
We could do the same exercise for regression.
● Rather than using a linear model, we could build a regression tree.

Summary
Decision trees provide an alternate non‐linear framework for classification and
regression.
● The underlying principle is fundamentally different.
● Decision boundaries can be more complex.
● Danger of overfitting is high.
Keeping complexity under control is not nearly as mathematically elegant and
relies on heuristic rules.
● Hard constraints.
● Pruning rules.
● Random forests.
○ Very interesting application of bootstrapping.
Define what regression is
● Fit a model
● Fit a Polynomial Curve
● Define what overfitting is
● Prevent overfitting
● Learn about Decision Trees and overfitting
● Restrict Decision Tree Complexity
● Define what Random Forest is
● Use Decision Trees for Regression