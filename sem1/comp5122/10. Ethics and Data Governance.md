# 10. ETHICS AND DATA GOVERNANCE 
LEARNING OBJECTIVES AND
To develop your understanding of:
•Why data analytics raises ethical issues (lecture 1)
•Data governance and the legal situation (lecture 2)
•Regulations and data, and the reasons for these
•The relationship between ethical issues and the laws
•Key ethical issues or risks in data science, processing, analysis
•Issues of data quality, bias, privacy, anonymisation
•Ethical issues in the use of data analysis
•Whether responsibility can go ‘backwards’
•“I just generate the knowledge, what others do with it us up to them”
•Depending on time: Q&A
Point of the lectures:
•To help with assessment (clear an immediate obstacle).
• Providing you with guidance to help complete the assessment
•To help your employability (clear future obstacles).
• Reflecting on your role in society as a data scientist; how you ‘add value’ with specialist
skills
• How these determine your wider responsibilities to employers/customers/society
• Employers will like this
•To hopefully offer interesting ideas for reflecting on your studies/practice
## 1. What is Ethics
Learning objectives:
•To gain a general understanding of the purpose of ethical thinking
•Begin to identify why data analytics is a practice that raises ethical
issues

WHAT IS ETHICS?
•As a starting point: ethics is a way of deciding what we should (and should not) do
•Evaluating our actions and choices
•Assessing ethical judgements
•Interrogating the reasoning behind those judgements
•Two goals:
1. Figure out what the right thing to do is (or, what is ok/permissible to do)
2. Figure out why it is right (or ok/permissible)
Hopefully… leading to us act accordingly (following through on that)

Trolley problem example
Seems like we have the following intuition:
-It’s better to pull the lever
Reasoning helps clarify why that seems the best
action:
-It’s morally better to save more lives than fewer
This seems like a good reason (hard to dispute)
When our intuitions and reasons are ‘in sync’:
no problem – what we sense is right also seems
most reasonable / rational to do
So might as well do it

HOW TO CAST DOUBT
Intuition:
-It’s better to pull the lever
Reasoning:
-It’s morally better to save more lives than fewer
Doubt method 1: Critique
the action. Highlight
reasons against
Doubt method 2: Critique the reason. Highlight the problem implications

What, Why
Q: Are these reasons enough to sway our intuitions? Pull the leverDon’t pull the lever Causing the death of someone, killing them Not up to us to intervene More lives saved = morally better Different reasoning justifies different courses of action
Doubt method 1: what about the reasons against?

EXAMPLE 2: FAT MAN BRIDGE What should you do
A: Push the fat man (kill 1)
B: Don’t push the fat man (allow 5 to die)
C: Not sure (also allow 5 to die)
Push the fat man: More lives saved = morally better Don’t push the fat man: We would be causing the death of someone, killing them Not up to us to intervene
From a 2006 BBC Survey: Push fat man: 26.88% Don’t: 73.12% Cross-culturally consistent
### INTUITIONS AND REASONS 
Most people have the following intuition: -It’s better to not push the fat man Justified by: -It’s not our place to intervene / cause his death But in the ‘split tracks’ case we thought that: -It’s morally better to save more lives than fewer Was the reason we should intervene (/ kill 1 to save 5) What’s going on? Are (most of us) inconsistent, and therefore irrational?
20
Q: Should we act against our intuitions in the ‘split track’ case? Or in the ‘fat man bridge’ case? What Why Pull the lever More lives saved = morally better Push the fat man Puzzle: conflict in intuitions and reasons
Context A: Split tracks 
Context B: Fat man over bridge Puzzle: the same reason justifies opposed actions, depending on contexts So, why not push the fat man? If saving lives is not most important…then why pull the lever? If you are really acting on the basis of this reason when pulling the lever:
To be rational, you either:
1. Should push the fat man
2. Interrogate your reasoning further (this is not enough to justify pulling it)

Solution 1: Act against
intuitions
Context A Context B
Benefit: Consistency. You’re acting for the reason you judge
most justified, so are in one sense acting rationally
Problem 1: You’re doing things against your moral conscience
Problem 2: Debatable you’re really acting rationally, if you see
that your reasons also justify actions that are obviously wrong

Solution 2: Interrogate our
reasons
More lives saved =
morally better
Benefits of this approach:
- Do not act against our intuitions
(/conscience)
- We learn more about our ethical
processing
- Refine our ‘system’ of ethical
principles
Consider:
What does this capture that seemed reasonable?
What does it need qualifying with?
Does it need adding to? Do not expose others to harm\* UNLESS \*the man on the track is already in some way in harm’s way, whereas the fat man is a bystander, out of harms way
This approach is essentially what doing Ethics is 
End goal: arrive at a reliable set of principles, to guide us where when intuitions are murky Similar to refining a decision-making algorithm

This approach is essentially what doing Ethics is
### NAVIGATING MURKY ETHICAL ISSUES
Eventually: can reach
judgements on issues
where our intuitions are
murky

More lives saved = morally better What does this capture that seemed reasonable? What does it need qualifying with? Does it need adding to? Do not expose others to harm/ violate their rights UNLESS UNLESS Do not prolong unnecessary suffering Eventually: can reach judgements on issues where our intuitions are murky AND/OR Respect others’ autonomous decisions
### OVERVIEW OF ETHICS
•Basically: ethics is a way of looking at what we should and should not do
•Evaluating and assessing our actions, choices, for how they impact others
•Trying to reach consistency, sound principles for action
•Considers how to uphold people’s moral rights (distinct from legal)
•And their wellbeing (welfare, happiness, quality of life)
•Making sure we have good reasons to justify actions, where either of these are at issue
### ETHICAL CONCEPTS
Thinking through the ethical reasons behind our actions gives rise to
ethical concepts
•fairness (not taking advantage of/hoarding resources for yourself over
others)
•responsibility (living up to an entrusted role, recognising the impacts
your actions can have)
•duty (owing an action to others, based on contract/promise)
•harm / welfare (e.g. whether causing suffering to others)
•Rights (can restrict / require actions from others)
### TECHNOLOGY AND ETHICS
•Warren and Brandeis (1890) – Harvard law professors made the case for new ‘right to be let alone’, because of advances in the printing press (sharing of information), and photography
•At the time, photographs were slow – they required someone to sit stock still in very specific conditions, facing the photographer (the camera being difficult to set up)

•Previously: unless you agree / are on board with the photo being taken, it’s very
easy to prevent it happening – you just move away
•‘Hit and run’ photographs now possible - taken without your knowledge or
consent, put to any use the photographer sees fit
•Depending on where they found you, could be embarrassing, even dangerous,
for you
Key Point:
Technological progress can impact people’s rights and wellbeing
So technology becomes an ethical issue: need ethical reasoning to navigate
### DATA AND ETHICS
•Being a data scientist, you are in the role of someone who can ‘extract
useful knowledge from data’ (Provost and Fawcett, p.2)
•Exhuming buried/hidden information, patterns, relationships between
variables in data sets
•These might be about particular people (indirectly), categories of people
•So you’re dealing with the knowledge of others
• Key point: with knowledge about others comes the power to affect
them
•New information be used to trace back to those individual people
•An early example of harm resulting from primitive use of datasets:
•Actress Rebecca Schaeffer, murdered by a fan stalker in 1989, Los
Angeles
•Murderer found her home address through motor vehicle licence
records
•Led to a change in the law
•Key point: unpredictable what others can do on the basis of information,
what they can cross reference it with
•Hindsight is a great teacher HTTPS://EN.WIKIPEDIA.ORG/WIKI/REBECCA_SCHAEFFER

•Sets of information about individual
people
•Could be used to trace back to those
individual people
•Could fall into the hands of certain
groups who want to oppress people /
categories of people
HTTPS://PRIVACYINAFRICA.COM/2017/09/06/AFRICAN-GOVERNMENTS-ARE-REQUESTING-MORE-
USER-DATA-FROM-FACEBOOK-GOOGLE-AND-TWITTER-THAN-EVER-BEFORE/
•So much information about all of us now generated. Stored on servers, in
the cloud
•Increasing exponentially; ‘internet of things’
•Is useful: better goods and services, we can generate new knowledge about
people’s behaviour
•Gives data scientists more raw material to analyse, increasing our
knowledge, increasing effectiveness of e.g. business, govt, accuracy of
research
•Also exponentially increasing the ethical risks, unpredictability of potential
harm
What are some of the ethical issues raised by data?
### ASSORTED ISSUES
Usage of data science issues (responsible usage)
Bias baked into toolsBias in interpretations Black box/opacity Control
“Harm to subjects” issues (’caught in the dredging net’)
Privacy Consent Breaches / lax handling
Externalities (what might be on the horizon)
Shaping preferences Sustainability Interpersonal relations Automation and work
## 2. DATA ETHICS AND THE LAW (/REGULATION)
Learning objectives:
•To understand…
•The relationship between ethics and the law (/regulations)
•Key roles in data governance (Data Processor, Controller, Subject)
•Principles of UK-GDPR (2020), DPA (2018)
•Definitions of Personal Data, Special Personal Data

Ethics and law ‘come apart’. Not the same thing
• Some legal obligations are not ethical obligations, eg. exact tax
regime, side of the road to drive on.
• The law is incomplete: not everything unethical should be illegal
(e.g. lying to a friend)
•It is difficult to ‘keep up’ with rapidly advancing technology. Laws
are updated through us realizing different things are right/wrong.
Can’t just look to the law to see what you should do:
•Laws contain ethical concepts: need interpreting via ethical
reasoning

UK-GDPR
Principles:
1. Personal data (PD) must be
processed fairly, lawfully and
transparently
2. Only collected/processed for explicit and legitimate purpose(s) Fairly and transparently Legitimate purposes “conduct that falls short of the standards expected of a person where a duty of care is owed and which causes foreseeable damage to another person.” Peter De Cruz, Nutshells Medical Law. Negligence = Falls short of standards expected Duty of care

‘Just following the law’ is
not a sensible policy:
- Some legal acts are still
unethical
- Sometimes laws are
unethical/incomplete/ne
ed updating
- To be able to follow laws
requires you think about
the ethics of situations
### ETHICS AND LAW
The function of ethics:
Determine what we should do, what moral (not legal) rights we have Based on how our actions impact others’ moral rights and wellbeing The function of law (ideally): Provide guide rails to restrict behavior that could lead to harm Clarify consequences of that behavior / contain the ‘fallout’ from it (no vengeance spirals) Protect rights of individuals from powerful others (/the state) Reassure citizens about participating in a society, accessing goods/services Implement society’s ethical viewpoint, updated based on results of ethical reasoning
### DATA PROTECTION AND THE LAW
To track you/access your identity, what sources of data could someone
look at? •Search histories
•Automatic Number Plate Recognition
•CCTV
•Medical records
•RFID – in shops / cashpoints etc
•WiFi / Bluetooth / mobile towers
•Google and Apple location histori
With everyone leaving an extensive digital ‘exhaust trail’, this leaves
people open to harm and exploitation
•Data science: ‘extract[ing] useful knowledge from data’ (Provost and
Fawcett, p.2)
•Maybe about / or relating to people
Potentially harmful consequences for individuals and groups
#### A HISTORICAL EXAMPLE
•From primitive datasets:
•Actress Rebecca Schaeffer, murdered by a fan stalker in 1989, Los
Angeles
•Murderer hired PI who found her home address through vehicle licence
database
•Led to a change in US law (Driver’s Privacy Protection Act), preventing
such releases
•Key point: unpredictable what others will want to do with data
•and how they will ‘link’ information known with new information
#### CONTEMPORARY EXAMPLE
•Sets of information about individual
people
•Could be used to trace back to those
individual people
•Could fall into the hands of certain
groups who want to oppress people /
categories of people
#### MODERN data
Modern times, we have exponentially increased: Ease of collection / generation (many more devices spilling out data) Ease of storage (greater hard drive space, cloud storage) Sophistication of processing (faster processors, ability to find out new information from datasets) Leading to… Unforeseen uses (as we can learn more from data, the more we can use it for, for good or bad) The law’s functions (see earlier slide) seem to be needed
### Now, in the UK
UK DPA (2018) UK-GDPR (2020) is based on the EU GDPR (2018) Can’t give a thorough breakdown of it, but there is an emphasis on: Data controllers and data processors need to respect privacy and seek consent from data subjects …not assume consent. Data subjects have more power: e.g. the right to be forgotten
### SOME KEY DEFINITIONS
- Data Controller: determines the purposes and means of processing personal data 
- Data Processor: responsible for processing personal data on behalf of a controller 
- Data Subject: an individual who is the subject of personal data Information Commissioner: The person or group who is responsible for enforcing the GDPR/DPA. In the UK that is ICO (Information Commissioner's Office) These two definitions are distinguished so that the right moral and legal responsibilities can be allocated. However, they are not mutually exclusive. That is, it is possible to be both.
#### Examples
Q: IN THESE EXAMPLES, WHO IS THE CONTROLLER AND WHO THE PROCESSOR?
Definitions
- Data Controller: A person who determines the purposes for which and the manner in which any personal data are, or are to be, processed.
- Data Processor: Any person (other than an employee of the data controller) who processes the data on behalf of the data controller.

Example 1: A government department sets up a database of information about every child in the country. It does this in partnership with local councils. Each council provides personal data about children in its area, and is responsible for the accuracy of that data. Each council may access the data of the other councils (and must abide by data protection principles).
Answer:
E1: Government department and the councils are data controllers, in relation to the personal data on the database.

Example 2: Organisation O outsources its employee payroll to company A. O also engages a marketing company B to do a satisfaction survey of its customers. Company A will need information about O’s employees. Company B will need information about O’s customers. Company A and B have their own employees
Answer:
E2: Both A & B will be processing information on behalf of O, so they are both data processors, while O is the data controller. A & B also process personal data about their own employees and will also be the data controllers for that.
### DATA GOVERNANCE
Data governance concerns to the collection, availability, usability, integrity and security of how data is managed in a given enterprise. What is data governance? The control of data collection, processing and use How does it work? Through sets of processes that ensure certain standards are maintained What are the standards? To uphold security and integrity Why bother? Protects both the data, and the well-being of the people the data relates to Data governance ties together all other aspects of data management – it is the core component
### TYPES OF DATA ON GDPR UK-GDPR:
“Personal data”: relates to a person
“Special category data”: ‘sensitive’ data relating
to a person
Processing special category data requires you
offer extra legal justification / reason for doing
so
#### PERSONAL DATA
Definitions:
Personal data is data which relates to a living individual who can be identified: a) From that data or b) From that data and other information which is in the possession of, or is likely to come into the possession of, the data controller.

Q: LOOK AT THESE EXAMPLES. ARE THEY INSTANCES OF “PERSONAL DATA”?
Example 3: An organisation holds data on microfiche. The microfiche records do not identify individuals by name, but bear unique reference numbers which can be matched to a card index system to identify the individual concerned. Does this count as personal data?
Answer: E3: Yes – it can be linked

Example 4: A dataset involves the exact addresses and the prices of every house that was sold in Leeds in the year 2005. Does this count as personal data?
Answer: E4: It could be (see the notes below)
### SPECIAL CATEGORY DATA
The UK GDPR defines special category data as:
•personal data revealing racial or ethnic origin;
•personal data revealing political opinions;
•personal data revealing religious or philosophical beliefs;
•personal data revealing trade union membership;
•genetic data;
•biometric data (where used for identification purposes);
•data concerning health;
•data concerning a person’s sex life; and
•data concerning a person’s sexual orientation.
This does not include personal data about criminal allegations, proceedings or convictions, as separate
rules apply. For further information, please see our separate guidance on criminal offence data.
Q: WHY ARE THESE TYPES
OF PERSONAL DATA
TREATED MORE
SERIOUSLY BY THE LAW?
### GDPR: SEVEN PRINCIPLES
All data controllers/processors must follow the seven
principles of the GDPR, concerning how to collect,
process and store personal data:
1. Must be done lawfully, fairly and transparently
2. For explicit and legitimate purpose(s)
3. Limited to the purpose(s) (data minimization)
4. Accurate and up to date
5. Storage limitation (not stored for longer than
need for purpose(s))
6. Integrity and confidential (security)
7. Controller shows accountability
Research Exemption:
In certain cases, when it will not cause harm to subjects, data may be kept indefinitely for research purposes. The research purposes cannot go against the stated purpose for the collection of the data, and any publication of the research must be sure to adequately anonymise the data.
### GDPR: DATA SUBJECT RIGHTS
Subjects have the right to:
8. Be informed: how their data is used; how long stored; who shared with
9. Access: to see the data (make a ‘Subject Access Request’)
10. Rectify/correct anything wrong/incomplete
11. Right to erasure (to be forgotten: the right to have their data erased) (not absolute)
12. Restrict processing (not absolute)
13. Data portability: can take it with them
14. Object to processing of their data in certain circs. (they can stop direct marketing)
15. Re: any automated decisions; be informed (know); request human intervention; to
challenge
(https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/individual-rights/)
## 3. 
•Explore key ethical issues in the processing of data
• Privacy
• Consent
• Anonymisation
THE “LIFE CYCLE” OF
DATA
5
1. Collection
 Issues of consent
2. Processing
 Issues of privacy
3. Storage
 Issues of security
4. Use
 Issues of objectivity and responsibility
We can think of the
“lifecycle” of data as having
four main parts
Each part has its own
accompanying ethical
issues
### PRIVACY
Q1: How would you define something being ‘private’?
Private to you Known / visible to Examples of things that should be…
Q2: How should we decide what needs to be private?
Q3: Why is data privacy an issue?
•It seems like data privacy is an issue
because of risk of harm to data subjects
when data is accessible
•What if we know the data is ‘harmless’ – i.e.
can’t be used against someone?
•Does privacy still matter? Or irrelevant
without risk?
•Suppose everyone in this lecture was – unknown
to you – given secret access to a stream of
video/audio from your laptop camera and
microphone, and a feed of your desktop, for the
entire last week.
•Let’s suppose that nothing we saw could be used
to hurt you materially, financially, physically,
and that there wasn’t even anything potentially
embarrassing to you
•In that case, would you say after the fact:
A. I suffered no harm/wrong
B. I suffered some harm/wrong
•Point of the example: privacy matters not just because of risk to a subject’s
wellbeing
•Can matter because of a rights violation
•The right to ownership and control over facts about you
•In my example: I took control away from you when setting up the laptop
stream
•Why would this be wrong?
### CONSENT
Consent: the power to determine who can access, use,
affect something we have a right of control over
If others violate that consent, they can be blamed
Governing when someone can touch you, use your
belongings, and so on. Generally: access things you
may care about
We can shape the boundaries of our right to privacy by
consent: granting permission to others
#### VALID CONSENT: 3 CONDITIONS
1. INFORMED
subjects have to be able to understand what their data will be
used for
2. AUTONOMOUS
subjects have to freely decide (not forced, coerced) to give
data
3. ONGOING
subjects should be able to withdraw / reclaim their data at any
time

Q1: Is ‘implied’ consent ever
legitimate?
Q2: Are ‘terms and conditions’ a
good method of gaining
informed consent?
Q3: Do you do something wrong
when you use datasets without
checking consent was given by
the subjects?
### PRIVACY, CONSENT, AUTONOMY
•Privacy, consent and autonomy: important throughout
our lives
•Children exert autonomy by exerting privacy
•Older people lose this ability and require help
•Acting with autonomy is fundamental to wellbeing
•Collection, trade and processing of personal data can
undermine autonomy, because it takes some control
away
•Privacy recognized as a human right by the Universal
Declaration of Human Rights (1948) and the European
Convention on Human Rights (1950)
These connected with:
• Individuality: capacity to think/speak/act as we like
• Experimentation with new ideas or actions, without
fear of causing offence
• Innovation and progress: ideas to disrupt, change
• Reflection and independent thinking
• Intimacy: if everyone knows everything about
everyone, who counts as a close friend?
### THE VALUES PROTECTED BY PRIVACY
“The creation, collection, and storage of personal data carries with it a moral responsibility and a duty of care to protect that data and minimize risk. If my analysis about privacy duties is right, it is a violation of people’s right to privacy to collect personal data about them without their meaningful consent or when there are no outweighing considerations (2024: 184)” The values that are protected by privacy: control over self-presentation, reputation, autonomy, creativity, security, freedom, equality and fairness, well-being, democracy
### PRIVATE VS. USEFUL
Just as Henrietta Lack’s cells have been
incredibly useful and valuable, so is data
•Accept that consent is a necessary
requirement
•A problem remains: no data, once given, is
completely safe from privacy being
undermined
The problem we saw with the definition of
personal data was this:
any data set could in theory become
personal data, depending on what other data
it could be linked to.
This makes the following question a difficult one:
•Can we balance the need for upholding privacy with
the need to do useful stuff with data?
•Key: ensure subject’s consent for how data is used
•The privacy trade-off can be agreed as justified
### ANONYMISATION / LINKAGE
•Steps should be taken to ensure that
data used is anonymized
•Personal data, and special category data:
•Can be embarrassing
•Can put people at risk of harm
•And: ethically important to respect
people’s autonomy / choice / consent
•(Also: even if they’ve provided
information willingly, you may be more
informed, able to understand the
riskiness of that information)
•Also to limit potential for data linkage
“Data linkage”: when two (or more) pieces of information from different sources are brought together, to reveal information that is not available in one dataset.
This means that even if data is anonymized within the context of its own data set, linkage may de- anonymize the data. Important to remove these
### DIRECT INDICATORS
Can be used to directly identify the data
subject. Includes:
•Name/initials
•Address, including full or partial post code
•Telephone numbers or email addresses
•Unique identifying numbers (NI number, NHS
number)
•Vehicle identifiers
•Medical device identifiers
•IP addresses
•Facial photographs
•Names of relatives
•Individual dates
### INDIRECT INDICATORS
Can be used in conjunction with other
information (direct or indirect) to identify the
data subject. Includes:
•Place of treatment or GP name
•Sex
•Place of birth
•Socio-economic data (income, etc.)
•Ethnicity
•Age
•Household or family constitution
•Illicit drug use, or other substance use
•Rare diseases and treatments
•Transcripts or verbatim responses
### QUESTION  
Definitions: 
Personal data is information that relates to an identified or identifiable individual: a) From that data or b) From that data in combination with other information Sensitive personal data involves matters of ethnicity, sexuality, and religion (among other things).  
Example 3: An organisation   holds data on microfiche. The   microfiche records do not   identify individuals by name,   but bear unique reference   numbers which can be matched   to a card index system to   identify the individual   concerned. Does this count as   personal data?Example 4: A dataset involves   the exact addresses and the   prices of every house that was   sold in Leeds in the year 2005.   Does this count as personal   data?  
Examples from ICO Guide to DPA:  
https://ico.org.uk/for-organisations/guide-to-  
data-protection/key-definitions/  
Q: LOOK AT THESE EXAMPLES.  
ARE THEY INSTANCES OF  
“PERSONAL DATA”?  
Answers:  
E3: The information held on the  
microfiche records is personal data, as  
the data subjects can be identified by  
them.  
E4: The data relates to houses not to  
individuals. However, we might question  
whether this data could be related to  
specific individuals through linkage with  
other publically available information.  
See notes.
## 4. 
RECAP AND STARTING POINT
OF TODAY
So this question (our overall concern in these lectures):
Is what I’m doing with data (processing it, storing it) ethically ok?
Answer depends on:
1. Does the subject know (has consented to) how their data is being
used?
2. Are you able to ensure that you can responsibly achieve that aim?
If you’re unsure on either (1) or (2), then you can not safely answer ‘yes’
Re 2:
Can fail to meet this if
you: a. expose subjects to harm / treat them unfairly / breach their rights b. undermine (1), by ‘drifting’ away from the original stated usage they consented to
3. Does the subject know (has consented to) how their
data is being used?
4. Are you able to ensure that you can responsibly
achieve that aim?
### PRIVACY, CONSENT, USE 
#### Target Example
In 2012 Target employed a data scientist called Andrew Pole to “figure out if a customer is pregnant, even if she didn’t want us to know” To better market pregnancy products Pole started by looking at the dataset of customers who told Target they were pregnant. Analyzed shopping habits, to discover patterns. Developed a “pregnancy prediction” score. He could estimate due dates, and send coupons timed for specific times in the pregnancy.
Source:
http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html

A year later an angry customer demanded: “Are you encouraging my teenage daughter to get pregnant?” Target had been sending her coupons. The manager apologized profusely. A week later the manager contacted the man. This time it was the man’s turn to apologize. Daughter was actually pregnant, just hadn’t told him until this event had prompted it. Target knew before the father.

Which of these conditions does this case pose a problem for…
1. Does the subject know (has consented to) how their data is being used?
2. Are you able to ensure that you can responsibly (w/out causing further harm or undermining 1) achieve that aim?
#### Washington Post example
Gillian Brocknell in Washington Post: Her baby died, but she received ads for new baby products, causing ongoing grief.
“Please, Tech Companies, I implore you: If your algorithms are smart enough to realize that I was pregnant, or that I’ve given birth, then surely they can be smart enough to realize that my baby died, and advertise to me accordingly — or maybe, just maybe, not at all.”
Which of these…
1. Does the subject know (has consented to) how their data is being used?
2. Are you able to ensure that you can responsibly (w/out causing further harm or undermining 1) achieve that aim?
#### FACEBOOK!
2014: was revealed that data scientists at Facebook had, in conjunction with Cornell and the University of California, experimented on 689,000 of its users. During 2012, user’s news feeds were unknowingly altered, so that only positive or only negative stories were shown. The experiment aimed to see how “emotional contagion” worked online. They found: by shaping people’s news feeds you could effectively shape their moods. People only shown positive stories were more likely to post positively, and vice versa.

Facebook argued that altering th1e news feed was "consistent with Facebook1's data use policy, to which all user1s agree prior to creating an account on Facebook, constituting informed consent for this research".
Which of these…
1. Does the subject know (has consented to) how their data is being used?
2. Are you able to ensure that you can responsibly (w/out causing further harm or undermining 1) achieve that aim?
#### CASE STUDY: FACEBOOK
Few people read the terms and conditions, especially when they are so
long. If we cannot expect people to read them, how can they form the
basis of informed consent?
Even if they did, it is unlikely that when participants signed up 10 years ago, that they expected their data to be used in the way that it now is. The data is created in highly context-sensitive spaces, and it is very likely that users would not give permission for their data to be used elsewhere, or in the ways it is currently being used.

People speculated (at that time) about the possible future ramifications. If you can effect people’s mood and opinions, could this be used to swing elections or cause civil unrest?
Then 2016: ‘Cambridge Analytica’ purported to have swayed elections results of Brexit Referendum, and the US Presidential election that year, with targeted advertising. The ability to target people’s political and sexual preferences, shopping habits, and feed them further content… Enables control / manipulation of their further actions

From Kirby and Kirton (2019) [Facebook] seeks to categorise and bracket users into marketable demographics, based on the data it gathers from the user. May include political views. Troubling content e.g. fabricated stories about particular ethnic groups is attention grabbing Users bracketed into groups, which can be more easily targeted with attention grabbing content. Users encouraged to connect with like-minded others, to share the same content. Result: congealing of users in discussion groups around extreme, socially divisive viewpoints.

How should we understand the wrong of what Facebook’s algorithm (and similar ‘social control/manipulation’ algorithms) do with data?
### AUTONOMY
Eric Schmidt, Google’s CEO from 2001 to 2017 said “I actually think most people don’t want
Google to answer their questions. They want Google to tell them what they should be
doing next” (Veliz, 2024: 187-8)  potential to manipulate behaviour.
Veliz: “to allow a company like Google to tell us what to do before we even ask would
amount to surrendering our autonomy. Companies might want to argue that they are
enhancing our autonomy by collecting our personal data, inferring our values, and making
sure that we act in accordance with them. But companies have their own interests, which
they put before ours (e.g., to sell us products, even when we’d be better off without them)”
(p. 188).
Problem: threat to individual autonomy (if data is used to manipulate
Are there some things that consent cannot make ok?
Question: if we all did consent to the kind of manipulation and influence that Facebook-style algorithm/data processing entails, Would it therefore be ok? Are some uses of data just ‘not good’, regardless of whether individuals consent to them?
### LIMITATIONS OF CONSENT, BAD USES OF DATA
There are some kinds of data/algorithmic processing uses that seem just dubious in their nature. We ask ‘why would anyone consent to that’? (e.g. Facebook manipulation) There are also uses where our ability to consent to them is undermined
E.g. by corporations (e.g. assessing whether we can qualify for a loan) that govern our access to things we need (recall: coercion)

But other cases where we have no power to consent: Use of predictive policing / judgements by the state You cannot choose whether your data processed by a predictive policing algorithm (predicting likelihood of you committing a crime) or giving a sentence
-The state assumes ‘tacit’ consent (i.e. as with paying taxes – you reap benefits of state protection, so must pay)
-Does evaluating the ethics here no longer depend on (1)?
But other cases where we have no power to consent: Use of predictive policing / judgements by the state You cannot choose whether your data processed by a predictive policing algorithm (predicting likelihood of you committing a crime) or giving a sentence
-The state assumes ‘tacit’ consent (i.e. as with paying taxes – you reap benefits of state protection, so must pay)
-Does evaluating the ethics here no longer depend on(1)?
### BIAS
Bias can mean your data processing treats people unfairly
#### CASE STUDY: COMPAS
E.g. COMPAS – an AI for sentencing offenders in America. Judged man on left as 3/10 risk of reoffending. Man on right as a 10/10, for similar crimes.
All the AI did here was replicate the unfair/unjust decisions that had been made against African Americans in the past (historical bias). They were not likely to reoffend, but the white justice system viewed them this way By training this algorithm on data that ‘contained’ unjust decisions… the algorithm was then making decisions that were like those unjust decisions. Because the decisions impacted people’s lives (so their wellbeing and rights), this made the algorithm an unethical tool

Finding a strong correlation between A and B, by analysis, tells us nothing about why A and B are correlated
E.g. suppose People of type F did commit more crime
according to the figures...
Maybe they do not have the opportunities that others do, so are forced into crime? Maybe they are harassed? Maybe the people recording the data follow type F people more closely/pay more attention to them? Finding a strong correlation between A and B, could be the result of our actions leading them to being correlated 

This is an example of an AI program seeming to display biased behavior, and it was not picked up for nearly 20 years. Even if race is not a feature of the dataset, the system can include and analyze other data which is correlated to race to produce a biased result. Because the program is left to develop its own way of analyzing the data and deciding the risk, the decision making process is not transparent (even to programmers) Without transparency, ethical thought and professional oversight, these biases can be harder to diagnose and address than human biases, and it is hard to assign blame or accountability for an unjust decision, when it is made by an automated decision-making tool. Also concerns about whether predicting reoffending is a good idea (!)
#### DIFFERENT TYPES OF BIAS
List them and why theyre bad data to use
#### USES OF PREDICTIVE ANALYSIS
‘Biometric mirror’ technology exists to predict…
Your attractiveness
Your emotional stability
Your degree of responsibility
Your sociability
Your ‘weirdness’
Q: Can we think of legitimate uses for such tech? Ones that would satisfy…
1. Does the subject know (has consented to) how their data is being used?
Or if being used by a state:
2. Are you able to ensure that you can responsibly (w/out causing further harm or undermining 1) achieve that aim?
### MORAL RESPONSIBILITY
•But as an analyst you may wonder whether you should be held responsible
for…
1. How the knowledge you get from analyzing sets comes to affect people
2. How that knowledge can be used by others
A. I just identify patterns & relationships, what use that information is put
to afterwards is not my responsibility
B. I am at least partially responsible for the outcomes and effects on
society that the data analyses I undertake/algorithms I design go on to
have