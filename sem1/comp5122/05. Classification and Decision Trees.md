What will you learn?
‚Ä¢ Define what classification is
‚Ä¢ Classify Iris Species using k-nearest neighbors
‚Ä¢ Classify Iris Species using Decision Trees
‚Ä¢ Building Decision Trees
‚Ä¢ Entropy and Defining a Best Feature
### A First Application: Classifying Iris Species

| General             | 3 Types             |
| ------------------- | ------------------- |
| ![[iris-petal.png]] | ![[iris-types.png]] |
‚Ä¢ A botanist is interested in distinguishing the species of some iris flowers that she
has found. She has collected some measurements associated with each iris: the
length and width of the petals and the length and width of the sepals, all measured
in centimeters
‚Ä¢ Our goal is to build a machine learning model that can learn from the
measurements of these irises whose species is known, so that we can predict the
species for a new iris.
 Because we have measurements for which we know the correct species of iris, this
is a supervised learning problem.
‚Ä¢ We want to predict one of several options (the species of iris). This is an example of
a classification problem.
‚Ä¢ The possible outputs (different species of irises) are called classes.
‚Ä¢ Every iris in the dataset belongs to one of three classes, so this problem is a three-
class classification problem.
‚Ä¢ The desired output for a single data point (an iris) is the species of this flower. For a
‚Ä¢ particular data point, the species it belongs to is called its label.

We want to build a machine learning model from this data that can predict the species
of iris for a new set of measurements.
How to measure success? Training and Testing Data
To assess the model‚Äôs performance, we show it new data (data that it hasn‚Äôt seen before)
for which we have labels.
This is usually done by splitting the labeled data we have collected (here, our 150 flower
measurements) into two parts.
One part of the data is used to build our machine learning model, and is called the training
data or training set.
The rest of the data will be used to assess how well the model works; this is called the test
data, test set, or hold-out set.

Shuffling the dataset:
Using a test set containing only one of the three classes would not tell us much about how well our model generalizes, so we shuffle our data to make sure the test data contains data from all classes. Splitting the dataset We can use 75% of the rows in the data as the training set, together with the corresponding labels for this data. The remaining 25% of the data, together with the remaining labels, is declared as the test set. %75, %25 %80, %20 %70, %30

The output of the train_test_split function (scikit-learn) is X_train, X_test, y_train, and y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset, and X_test contains the remaining 25%:
```python
print("X_train shape: {}".format(X_train.shape))
print("y_train shape: {}".format(y_train.shape))
X_train shape: (112, 4)
y_train shape: (112,)

print("X_test shape: {}".format(X_test.shape))
print("y_test shape: {}".format(y_test.shape))
X_test shape: (38, 4)
y_test shape: (38,)
```

*Output:Three classes seem to be relatively well separated using the sepal and petal measurements*
![[iris-clusters.png]]
Building Your First Model: k-Nearest Neighbors:
There are many classification algorithms in scikit-learn that we could use.
Here we will use a k-nearest neighbors classifier.
‚Ä¢ Stores the training set.
‚Ä¢ To make a prediction for a new data point, the algorithm finds the point in the training set that is closest to the new point.
‚Ä¢ Then it assigns the label of this training point to the new data point.
The k in k-nearest neighbors signifies that instead of using only the closest neighbor to the new data point, we can consider any fixed number k of neighbors in the training (for example, the closest three or five neighbors). Then, we can make a prediction using the majority class among these neighbors.

1. In K-NN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case.
![[iris-knn-1.png]]
In the figure above, suppose yellow colored ‚Äú?‚Äù let's say P is the point, for which label needs
to predict. First, you find the one closest point to P and then the label of the nearest point
assigned to P.
2. Second, you find the k closest point to P and then classify points by majority vote of its K neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, we find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance, and Minkowski distance. 
![[iris-knn-2.png]]
The algorithm has the following basic steps: 
1.Calculate distance 2.Find closest neighbors 3.Vote for labels
3. We formulated the task of predicting which species of iris a particular flower belongs to by using physical measurements of the flower.
We used a dataset of measurements that was annotated by an expert with the correct species to
build our model, making this a supervised learning task.
There were three possible species, setosa, versicolor, or virginica, which made the task a three-class
classification problem.
The possible species are called classes in the classification problem, and the species of a single iris is
called its label.
We split our dataset into a training set, to build our model, and a test set, to evaluate how well our
model will generalize to new, previously unseen data.
### Decision Trees
A Decision Tree is a very simple way to classify data. It is simply a tree of
questions that must be answered in sequence to yield a predicted classification.
#### Classifying Iris Species : Using Petal Data Only
The plot below shows the width and length of the petals of each flower, with
the species annotated in the form of color.
We can build a decision tree manually just by looking at this picture.

| Decision Tree                    | Scatter Plot                   |
| -------------------------------- | ------------------------------ |
| ![[iris-decision-tree.png\|300]] | ![[iris-decision-scatter.png]] |
How accurate is our decision tree model on the training data?
It seems like it gets every point correct.
Is this good or bad?
I‚Äôd argue bad.
Seems likely to result in overfitting!
Will discuss overfitting more later.
First, let‚Äôs see how we can build decision trees for classification using scikit-learn.
#### Visualizing Decision Tree Models
![[decision-tree-vis.png]]
In each box, we see:
‚Ä¢ The rule.
‚Ä¢ The gini impurity (chance a sample would be misclassified if randomly assigned at this point).
‚Ä¢ The number of samples still unclassified.
‚Ä¢ The number of samples in each class still unclassified.
‚Ä¢ The most likely class

There is one terminal decision point where there is more than one possible right answer. The model was unable to come up with a decision rule to resolve these last 3 samples. Let‚Äôs see why using the query method of the dataframe class. There is one terminal decision point where there is more than one possible right answer.
‚Ä¢ In the original data set, there was a versicolor iris with the same petal measurements as two virginicas.

Plotting the decision boundaries for our model:
Decision tree has nonlinear boundary
![[iris-decision-scatter-scikit.png]]Running the code below, we see that we only get 99.3% accuracy.
```python
from sklearn.metrics import accuracy_score 
predictions = decision_tree_model.predict(iris_data[["petal_length", "petal_width"]]) 
accuracy_score(predictions, iris_data["species"])
```
To understand why, let‚Äôs look back at our decision tree model.
#### Overfitting and Decision Trees
scikit-learn makes it easy to generate decision trees.
Perfect accuracy on the training data, EXCEPT when there are samples from
different categories with the exact same features.
That is, if the versicolor above had a petal_length of 4.800001, we‚Äôd have
100% training accuracy.
-> This tendency for perfect accuracy should give us concern about overfitting.
#### Multidimensional Decision Trees
Naturally, we can include even more features. For example, if we want to use
the petal AND sepal measurements, we simply train the decision tree on all four
columns of the data.
```python
decision_tree_model_4d = tree.DecisionTreeClassifier() 
decision_tree_model_4d = 
	decision_tree_model_4d.fit(train_iris_data[["petal_length", "petal_width", 
		]], train_iris_data["species"])
```
The resulting model gets:
100% accuracy on the training set (no overlapping data points).
95% accuracy on the test set
#### Model 2D-150 vs. Model 4D-110 Tree Visualization
![[tree-models-vis.png]]
Models are quite similar:
Model 4D-110 uses only 110 samples.
The decision rules are nearly identical,
except that Model4D-110 uses the
sepal_width exactly once to resolve
the case that we couldn‚Äôt resolve with
petal_length and petal_width.
Model 4D-110 seems marginally better, but
both got only 95% accuracy on test set.
Need more data to know for sure.
### Decision Tree Generation
Let‚Äôs discuss how decision trees are created from data.
Traditional decision tree generation algorithm:
All of the data starts in the root node.
Repeat until every node is either pure or unsplittable:
‚óã Pick the best feature x and best split value Œ≤, e.g. x = petal_length, Œ≤ = 2.
‚óã Split data into two nodes, one where x < Œ≤, and one where x ‚â• Œ≤.
Notes: A node that has only one type is called a ‚Äúpure‚Äù node. A node that has
duplicate data that cannot be split is called ‚Äúunsplittable‚Äù.
#### Defining a Best Feature
Question: Which feature and split value is best?
Equivalently: Which horizontal or vertical line do we want to draw?
We need some sort of rigorous definition for a good split.
#### Node Entropy
Let ùëùùê∂ be the proportion of data points in a node with label C.
For example, for the node at the top of the
decision tree, $ùëù_0 = 34/110 = 0.31, ùëù_1 = 36/110 = 0.33, ùëù_2 = 40/110 = 0.36$.
Define the entropy S of a node as: $$
S = -\sum_{\substack{c}} {p_c} \log_2{p_c}$$
For example, S for the top node is:
$‚àí0.31 log_20.31 ‚àí 0.33 log_20.33 ‚àí 0.36 log_20.36 = 0.52 + 0.53 + 0.53 = 1.58$
Can think of entropy as how unpredictable a node is. Low entropy means
more predictable. High entropy means more unpredictable.
![[entropy-ex.png]]
Observations about entropy:
A node where all data are part of the
same class has zero entropy.
$‚àí1 log_2 1 = 0$
A node where data are evenly split
between two classes has entropy 1.
$‚àí0.5 log_2 0.5 ‚àí 0.5 log_2 0.5= 1$
A node where data are evenly split
between 3 classes has entropy $1.58$.
$3 √ó (‚àí0.33 log_2 0.33) = 1.58$
A node where data are evenly split into
$C$ classes has entropy $log2C$.
$C √ó (‚àí1/C log_2 1/C) = ‚àílog_2 1/C = log_2 C$
#### Weighted Entropy as a Loss Function
We can use Weighted Entropy as a loss function in helping us decide which split
to take.
Suppose a given split results in two nodes X and Y with N1 and N2 total samples
each. The loss of that split is given by: $$L = {N_1S(X) + N_2S(Y) \over N_1+N_2}$$
#### Defining a Best Feature
Split choice #1: width > 1.5. Compute entropy of child nodes:
`entropy([50, 46, 3]) = 1.16`
`entropy([4, 47]) = 0.4`
Weighted average: $99/150 √ó 1.16 + 51/150 √ó 0.4 = 0.9$

Split choice #2: length > 4. Compute entropy of child nodes:
`entropy([50, 9]) = 0.62`
`entropy([41, 50]) = 0.99`
Weighted Average: $0.84$: Better than split choice #1!

Split choice #3: width > 0.5. Compute entropy of child nodes:
`entropy([2, 50, 50]) = 1.12`
`entropy([48]) = 0`
Weighted average: $0.76$: Lower than split choice #2!

Split choice #4: width > 0.9. Compute entropy of child nodes:
`entropy([50, 50]) = 1`
`entropy([50]) = 0`
Weighted average: $0.66$: Lower than split choice #3!
#### Decision Tree Generation
Traditional decision tree generation algorithm:
‚óè All of the data starts in the root node.
‚óè Repeat until every node is either pure or unsplittable:
‚óã Pick the best feature x and split value Œ≤ such that the loss of the resulting
split is minimized, e.g. x = petal_width, Œ≤ = 0.8 has loss 0.66.
‚óã Split data into two nodes, one where x < Œ≤, and one where x ‚â• Œ≤.
Notes: A node that has only one type is called a ‚Äúpure‚Äù node. A node that has
duplicate data that cannot be split is called ‚Äúunsplittable‚Äù.
Let‚Äôs now turn our attention to avoiding overfitting.