# Test
## Outline
- Lecture 1: Introduction to Data Science
- Lecture 2: Data Understanding & Profiling
- Lecture 3: Data Science Roadmap & Exploratory Data Analysis (EDA)
- Lecture 4: Machine Learning Overview
- Lecture 5: Clustering & Similarity
- Lecture 6: Classification, k-NN, & Decision Trees
- Lecture 7: Regression, Overfitting, and Random Forests
- Lecture 8: Model Evaluation
- Lecture 9: Dimensionality Reduction
- Lecture 10: Principal Component Analysis (PCA)
- Lecture 11: Ethics and Data Governance
## MCQ
Question 1
What is the purpose of a confusion matrix in classification problems?
a) To determine the importance of each feature in the model
b) To summarise regression model performance
c) To calculate the overall accuracy of the model
d) To visualise the performance of a classification model by showing true vs predicted classifications

Question 2
What is the main advantage of using ensemble methods in machine learning?
a) They are simpler to interpret
b) They require less data
c) They improve predictive performance by combining multiple models
d) They speed up the training process

Question 3
What is the principle benefit of using the k-fold cross-validation technique in model evaluation?
a) It guarantees that the model will have no bias
b) It helps to mitigate overfitting by providing a better assessment of model performance
c) It allows for the use of more complex models without any data
d) It reduces the size of the training dataset

Question 4
Which type of learning uses labelled datasets to make predictions?
a) Reinforcement learning
b) Semi-supervised learning
c) Unsupervised learning
d) Supervised learning

Question 5
What is the main purpose of performing feature scaling in data preprocessing?
a) To reduce the number of features
b) To ensure that all features contribute equally to the distance calculations
c) To increase the size of the dataset
d) To eliminate missing values

Question 6
Which evaluation metric is best suited for imbalanced classification problems?
a) Accuracy
b) Mean Squared Error
c) R-squared
d) F1 Score

Question 7
What does the term 'overfitting' refer to in machine learning?
a) When data is lost during the training process
b) When a model is too simple to capture the underlying patterns
c) When a model performs equally well on both training and test data
d) When a model learns the training data too well, including its noise and outliers

Question 8
What is the significance of outlier detection in data preprocessing?
a) To ensure all data points are equally represented
b) To enhance data visualisation capabilities
c) To reduce the dimensionality of the dataset
d) To identify and handle anomalies that can skew model performance

Question 9
What is the primary purpose of the 'train-test split' in model evaluation?
a) To optimise hyperparameters
b) To assess the performance of a model on unseen data
c) To reduce dimensionality
d) To improve training speed

Question 10
What is an important advantage of using random forests in machine learning?
a) They require fewer training samples to achieve high accuracy
b) They can only be used for classification tasks
c) They provide linear relationships between features and outcomes
d) They reduce overfitting compared to a single decision tree

Question 11
What is the role of feature engineering in machine learning?
a) To create new input features from existing data to improve model performance
b) To handle missing values in the dataset
c) To select the best machine learning algorithm
d) To optimise hyperparameters

Question 12
Which of the following statements about clustering algorithms is true?
a) They are used exclusively for time series analysis
b) They predict outcomes based on labelled data
c) They group similar data points together based on features
d) They require a predefined target variable

Question 13
Which of the following is a key characteristic of time series data?
a) It consists of random data points that are independent of each other
b) It can only be analysed using classification techniques
c) It is sequential and often dependent on time-related factors
d) It does not exhibit any trends or seasonality

Question 14
Which of the following Python libraries is primarily used for numerical calculations and data manipulation?
a) Beautiful Soup
b) NumPy
c) TensorFlow
d) Flask

Question 15
What does 'hyperparameter tuning' refer to in machine learning?
a) Adjusting the number of features
b) Selecting the training algorithm
c) Scaling the input data
d) The process of optimising the settings of a machine learning model

Question 16
Which of the following metrics is used to evaluate the performance of a regression model?
a) Mean Absolute Error (MAE)
b) F1 Score
c) Accuracy
d) Precision

Question 17
What does PCA stand for, and what is its primary purpose?
a) Probabilistic Classification Algorithm; classification
b) Partial Correlation Analysis; feature selection
c) Predictive Component Assessment; model evaluation
d) Principal Component Analysis; dimensionality reduction

Question 18
In machine learning, which of the following methods is typically used for classification tasks?
a) K-means Clustering
b) Linear Regression
c) Principal Component Analysis
d) Support Vector Machines

Question 19
Which algorithm is commonly used for unsupervised learning tasks?
a) k-Means clustering
b) Logistic Regression
c) Random Forest
d) Naive Bayes

Question 20
What does the term 'ensemble learning' refer to in machine learning?
a) Enhancing a single model's performance through hyperparameter tuning
b) Implementing unsupervised techniques to classify data
c) Combining multiple models to produce improved predictions
d) Using a hierarchical structure to manage data processing

Question 21
Which of the following is a common data visualisation tool used in Data Science?
a) Microsoft Word
b) Tableau
c) Notepad
d) Photoshop

Question 22
What is cross-validation primarily used for in building machine learning models?
a) To replace the need for a validation set
b) To increase the number of training examples
c) To assess how well the model generalises to an independent dataset
d) To refine the selection of hyperparameters only

Question 23
Which of the following techniques is used to improve the performance of a machine learning model by reducing complexity?
a) Feature scaling
b) Regularisation
c) Cross-validation
d) Hyperparameter tuning

Question 24
In the context of machine learning, what is 'bagging'?
a) A technique for performance improvement in unsupervised learning
b) A technique to reduce variance by training multiple models on random subsets
c) A method to increase bias by consolidating model predictions
d) An approach to feature selection

Question 25
In which situation would you use a decision tree algorithm?
a) For text analysis only
b) For classification or regression tasks where interpretability is important
c) For time-series forecasting
d) For images only

Question 26
In the context of clustering algorithms, what does the term 'centroid' refer to?
a) A random point selected from the dataset
b) The farthest point in the cluster
c) The central point of a cluster, calculated as the mean of all points in the cluster
d) The first point added to the cluster

Question 26
In the context of clustering algorithms, what does the term 'centroid' refer to?
a) A random point selected from the dataset
b) The farthest point in the cluster
c) The central point of a cluster, calculated as the mean of all points in the cluster
d) The first point added to the cluster

Question 27
In the context of data science, what is 'feature selection'?
a) The method of combining multiple features into one
b) The process of scaling features to the same range
c) The technique of generating new features from existing ones
d) The process of selecting a subset of relevant features for model training

Question 28
What is the main function of a loss function in machine learning?
a) To quantify how well the model's predictions match the actual outcomes
b) To track the number of model iterations
c) To optimise data preprocessing
d) To visualize model performance

Question 29
What is the primary goal of data normalisation in machine learning?
a) To eliminate missing values
b) To reduce the size of the dataset
c) To enhance the complexity of the features
d) To scale features to a similar range for effective training

Question 30
What is the purpose of data preprocessing in machine learning?
a) To clean and prepare data for analysis and model training
b) To select the best machine learning algorithm
c) To fine-tune hyperparameters
d) To analyse model performance

Question 31
Which measure is commonly used to assess the accuracy of a classification model?
a) Mean Squared Error
b) R-squared
c) Silhouette Score
d) F1 Score

Question 32
Which type of data visualisation is best for showing the distribution of a numerical variable?
a) Histogram
b) Bar chart
c) Scatter plot
d) Line chart

Question 33
What is the purpose of using a test set in machine learning?
a) To evaluate the model's performance on unseen data
b) To perform feature selection
c) To optimise hyperparameters
d) To train the model on the entire dataset

Question 34
What is the main purpose of using a validation set in machine learning?
a) To create a holdout set for future predictions
b) To train the model on all available data
c) To tune hyperparameters and avoid overfitting
d) To assess the model's performance after training

Question 35
In which scenario would you most likely use a recommender system?
a) To suggest products to users based on their preferences
b) To detect anomalies in transaction data
c) To predict future stock prices
d) To classify images into categories

Question 36
What is the purpose of applying one-hot encoding in data preprocessing?
a) To normalise numerical values
b) To reduce the dimensionality of data
c) To convert categorical variables into a numerical format
d) To remove duplicates in data

Question 37
In ensemble learning, what is the key idea behind 'boosting'?
a) To sequentially combine weak learners to create a strong learner
b) To combine models in parallel to reduce variance
c) To enhance models by adding noise to the training data
d) To train a single model with different initial conditions

Question 38
What is the purpose of regularisation in machine learning?
a) To improve the computation speed of the algorithm
b) To enhance the complexity of the model
c) To increase the number of features used
d) To prevent overfitting by adding a penalty for larger weights

Question 39
Which of the following is a common assumption made by linear regression techniques?
a) The data must be normally distributed
b) The input variables are categorical
c) The relationship between the input variables and the target is linear
d) The target variable is continuous only

Question 40
Which of the following scenarios would be best suited for unsupervised learning?
a) Predicting disease outcomes based on patient history
b) Grouping customers based on purchasing behaviour
c) Identifying spam emails
d) Forecasting stock prices

Question 41
Which Python library is widely used for data manipulation and analysis, offering data structures like DataFrame?
a) Pandas
b) Matplotlib
c) Scikit-learn
d) Seaborn

Question 42
What does 'dimensionality reduction' aim to accomplish in data analysis?
a) To reduce the number of features while preserving important information
b) To increase the number of features in the dataset
c) To eliminate all outliers completely
d) To change the data distribution

Question 43
What does 'recall' measure in the context of classification?
a) The ability of a model to find all relevant instances in a dataset
b) The capacity of the model to produce true negatives
c) The trade-off between precision and false positives
d) The overall accuracy of the model

Question 44
Which of the following scenarios is most suitable for a supervised learning approach?
a) Finding patterns in customer purchasing behaviour
b) Dimensionality reduction of high-dimensional data
c) Clustering similar documents together
d) Predicting house prices based on historical data

Question 45
Which technique can be used to evaluate the effectiveness of a machine learning model while preventing overfitting?
a) Cross-validation
b) Random forest
c) Grid search
d) Support vector machine

Question 46
Suppose you are developing a music streaming platform where artists can upload their music and listeners can stream the music collection uploaded on the platform. You can assume that a sound analysis algorithm extracts different features from each uploaded song such as tempo, sound level, pitch, and instrument, and there is a list of possible genres defined which are currently entered manually by the artists for each song. The system you will develop should be able to automatically predict the genre of a song when an artist uploads their music. What is the task described as predicting the genre of a song an example of? Select one correct answer.
a) Dimensionality Reduction
b) Regression
c) Classification
d) Clustering
e) Reinforcement Learning

Question 47
Which of the following statements are correct regarding the task described in Q1? Select two correct answers.
It is an Unsupervised Learning task.
PCA can be used directly to predict the genre of a song.
It is a Supervised Learning task.
The task requires labeled training data.
K-Means Clustering can be used for this task.
Select up to 5 options

Question 48
Working for the same platform described in Q1, you are also asked to develop a recommendation system
which recommends to a user the playlists created by another user who listens to the most similar music to the
user. Which specific machine learning algorithm is appropriate for this, and how can we use this algorithm
for this task? Explain. (Note that deep learning algorithms are out of the scope of this course)

Question 49
Suppose you are developing a face recognition system for a company with 40 employees. You collect 10
different images of each person and for each image, you have information about who is in the photo. You
can assume that a computer vision algorithm extracts five different numeric features from each of these
images. This system will identify the employee by taking a photo of their face when they enter the building.
Which machine learning algorithm that we have learned about in class is not appropriate for the task
described?
a) Decision Tree
b) Naive Bayes
c) K-Nearest Neighbours
d) Random Forests
e) K-Means Clustering

Question 50
Which of the following statements are correct regarding the task described in Q4? Select three correct answers. Select up to 5 options
a) It is a Supervised Learning task.
b) It is an Unsupervised Learning task.
c) The task requires labeled training data.
d) The visual features extracted from the image are the labels for each image.
e) The employee identity is the label for each image.

Question 51
Suppose you are developing a self-driving autonomous vehicle system. You collect video data recordings
that show the road ahead and the kinematic data of the angle of the wheel. This system will learn to
automatically adjust the angle of the wheel according to the visual features extracted from the video data
for each time frame.  What is the task described as predicting the degree of the wheel task to adjust it an example of? Select one correct answer.
a) Clustering
b) Reinforcement Learning
c) Regression
d) Classification
e) Dimensionality Reduction

Question 52
Which of the following can be said for the initial step of the agglomerative clustering algorithm? Select one correct answer.
a) Each data point is determined as a cluster center.
b) The median value of the data points is calculated and a cluster center with these values is assigned.
c) Randomly determined values are assigned as cluster centers.
d) A randomly determined number of K cluster centers are assigned at the beginning.
e) The data point closest to the middle value of the data points is selected and determined as the cluster center.

Question 53
Given the training dataset below and a test instance with attributes (M, 0.80), which of the following
ship types would a 1-Nearest Neighbour classifier predict, using Euclidean distance? Select one correct answer.
Training dataset:
Instance 1: (L, 1.00) - Cargo Ship
Instance 2: (L, 0.10) - Cargo Ship
Instance 3: (M, 0.15) - Cruise Ship
Instance 4: (M, 0.45) - Tanker Ship
Instance 5: (H, 0.30) - Tanker Ship
Instance 6: (H, 0.45) - Container Ship
Instance 7: (M, 0.11) - Cruise Ship
a) Cruise Ship
b) No ship type can be predicted
c) Container Ship
d) Tanker Ship
e) Cargo ship

Question 54
Imagine you are given a high dimensional dataset with thousands of features but limited training examples. What challenges might this pose for building a machine learning model, and what strategies could you apply to address them? Name two strategies and explain how they would address the challenges.

Question 55
Age: [22.0, 38.0, 26.0, 35.0, 35.0]
Survived: [0, 1, 1, 1, 0]
Female: [0, 1, 1, 1, 0]
Table: The first 5 rows of the Titanic dataset
Suppose we want to build a classifier to predict whether a person survived the sinking of the Titanic. The
first 5 rows of our dataset are given above in Table.
For a given classifier, suppose the first 10 predictions of our classifier and 10 true observations (ground
truth) are as follows:
Prediction: [1,1,1,1,1,0,1,1,1,1]
Label: [0,1,1,1,0,0,0,1,1,1]
What is the accuracy of our classifier on these 10 predictions? Select one correct answer.
a) %60
b) %80
c) %50
d) %90
e) %70
## Section Specific
### Section 1: Introduction & Data Understanding (Lectures 1-2)
#### Question 1.1 (1 mark)
Which of the following best defines **datafication**?
a) The process of cleaning and structuring raw data.
b) The practice of using algorithms to predict future trends.
c) The process of taking aspects of life and turning them into quantifiable data.
d) The statistical analysis of large datasets to find correlations.
#### Model Solution for 1.1:
**c) The process of taking aspects of life and turning them into quantifiable data.**
*   **Explanation**: This is the core definition from Lecture 1. **Datafication** is the transformation of qualitative, real-world phenomena (e.g., sleep, social interactions) into **data points** that can be tracked and analyzed, creating new forms of value and **actionable insights**.
#### Question 1.2 (2 marks)
According to Donoho's "50 Years of Data Science" paper, what is a key distinction between **Lesser Data Science (LDS)** and **Greater Data Science (GDS)**? Select **two** correct answers.
a) LDS is focused on commercial big-data tools, while GDS proposes a more rigorous, academic framework.
b) LDS includes the study of data ethics, while GDS does not.
c) GDS incorporates the **Common Task Framework (CTF)** as a driver of progress, while LDS ignores it.
d) GDS is solely concerned with predictive modelling, while LDS covers the full data lifecycle.
e) A core division of GDS is **"Science about Data Science"**, the empirical study of data analysis methods themselves.
#### Model Solution for 1.2:
**a) LDS is focused on commercial big-data tools, while GDS proposes a more rigorous, academic framework.** and **e) A core division of GDS is **"Science about Data Science"**, the empirical study of data analysis methods themselves.**
*   **Explanation**: Donoho critiques the narrow, tool-focused **"Lesser Data Science"** driven by industry. His **"Greater Data Science"** vision is broader and includes six divisions, the last and most forward-looking being the empirical, evidence-based study of data science methodology itself. The CTF is highlighted as important, but not as the defining difference.
#### Question 1.3 (1 mark)
A dataset containing customer records has a column for 'Postcode' and a separate column for 'City'. During profiling, you find that for some records, the postcode does not match the city (e.g., a Leeds postcode with 'Manchester' as the city). This is primarily an issue of data:
a) Granularity
b) Scope
c) Temporality
d) Faithfulness
#### Model Solution for 1.3:
**d) Faithfulness**
*   **Explanation**: **Faithfulness** refers to how well data captures reality, including checking for internal **consistency** and violations of obvious dependencies. A mismatched postcode and city is an example of such an inconsistency, indicating a potential data entry error that reduces the data's faithfulness.
#### Question 1.4 (3 marks)
You are exploring a dataset of patient blood test results. The 'Cholesterol' column has 5% missing values. Describe **three** different strategies for handling these missing values, and for each, state one potential **advantage** or **disadvantage**.
#### Model Solution for 1.4:
**(1) Deletion:** Remove all rows with a missing cholesterol value.
*   *Advantage:* Simple and fast to implement.
*   *Disadvantage:* Can introduce **bias** if the missingness is not random (e.g., if sicker patients are less likely to have results recorded), and reduces dataset size.

**(2) Mean/Median Imputation:** Replace missing values with the **mean** or **median** cholesterol level of all patients.
*   *Advantage:* Preserves the dataset size and is simple.
*   *Disadvantage:* Reduces variance and can distort relationships, especially if the missing values have a different underlying distribution.

**(3) Regression Imputation:** Build a model (e.g., linear regression) using other patient features (age, weight) to **predict** the missing cholesterol values.
*   *Advantage:* Can be more accurate than simple imputation if other features are predictive.
*   *Disadvantage:* More complex, risks **overfitting**, and assumes the relationship used for prediction is correct. It also underestimates uncertainty as imputed values are treated as certain.
#### Question 1.5 (2 marks)
For each of the following variables, state whether it is best classified as **Continuous, Discrete, Ordinal, or Nominal**.
i) Shirt size (S, M, L, XL)
ii) Number of products purchased in a transaction
iii) Temperature in degrees Celsius
iv) Political party affiliation (Conservative, Labour, Liberal Democrat)
#### Model Solution for 1.5:
i) **Ordinal**. The categories have a clear order (S < M < L < XL), but the intervals between sizes are not necessarily consistent or meaningful.
ii) **Discrete**. The count of products is a finite, countable number (0, 1, 2, ...).
iii) **Continuous**. Temperature can be measured to an arbitrary level of precision (e.g., 20.0, 20.01, 20.015°C).
iv) **Nominal**. Party names are categories with no inherent order or ranking.
#### Question 1.6 (1 mark - Select Two)
Which of the following are key focuses during the **data profiling** stage of a project, as outlined in the "Practical Guide to Characterising Data"?
a) Training the final machine learning model.
b) Investigating **data quality** (completeness, accuracy, consistency).
c) Characterising the data (understanding its structure and distributions).
d) Deploying the model to a production API.
e) Writing the final project report.
#### Model Solution for 1.6:
**b) Investigating **data quality** (completeness, accuracy, consistency).** and **c) Characterising the data (understanding its structure and distributions).**
*   **Explanation**: The guide's core framework separates **data characterisation** (what the data *is*) from **data quality investigation** (how *fit* it is for purpose). Profiling is the investigative phase before modelling begins.
#### Question 1.7 (5 marks - Scenario Based)
*Scenario:* You are a data scientist at a retail bank. The fraud team wants to build a system to identify potentially fraudulent credit card transactions in **real-time**. They provide you with a historical dataset containing millions of transactions, each with features like `transaction_amount`, `merchant_category`, `time_of_day`, `customer_age`, and a binary label (`is_fraud`: 0 or 1).

a)  What type of machine learning task is this? (1 mark)
b)  The dataset is heavily **imbalanced** (less than 0.1% of transactions are fraud). Why would **accuracy** be a misleading metric to evaluate a model for this task? (2 marks)
c)  Suggest a more appropriate **evaluation metric** and briefly justify your choice. (2 marks)
#### Model Solution for 1.7:
a)  This is a **binary classification** task. The goal is to predict a categorical label (`fraud` or `not fraud`) for each transaction.

b)  **Accuracy** measures the proportion of correct predictions overall. A naïve model that simply predicts "not fraud" for every transaction would be correct over 99.9% of the time, yielding a very high accuracy score. However, this model is useless as it fails to identify **any** of the fraudulent cases (the **positive class** we care about). Therefore, accuracy hides the model's failure on the critical **minority class**.

c)  The **F1 Score** would be a more appropriate metric.
    *   **Justification**: The F1 Score is the **harmonic mean of Precision and Recall**.
    *   **Precision** is important because we want to minimize **false positives** (legitimate transactions flagged as fraud, which annoys customers).
    *   **Recall** is critically important because we want to catch as many **true positives** (actual fraud) as possible.
    The F1 score balances these two competing concerns, providing a single measure that is robust to **class imbalance**. Alternatively, the **Area Under the Precision-Recall Curve (PR-AUC)** is specifically designed for imbalanced classification.

### Section 2: EDA, Visualization & k-NN (Lectures 3 & 6)
#### Question 2.1 (2 marks)
Which of the following statements about **Exploratory Data Analysis (EDA)** are correct? Select **two**.
a) Its primary goal is to train and finalize the predictive model.
b) It is an attitude of flexibility aimed at discovering both expected and unexpected patterns.
c) It typically occurs after model deployment to explain predictions.
d) It makes use of **summary statistics** and **visualizations** to understand data structure and inform modelling.
e) It requires that all data be perfectly cleaned before any plots are made.
#### Model Solution for 2.1:
**b) It is an attitude of flexibility aimed at discovering both expected and unexpected patterns.** and **d) It makes use of **summary statistics** and **visualizations** to understand data structure and inform modelling.**
*   **Explanation**: EDA, as defined by Tukey, is an investigative, **hypothesis-generating** process that uses simple tools (stats, plots) to understand data. It is a precursor to modelling, not the final step, and often involves visualizing data during the cleaning process.
#### Question 2.2 (1 mark)
You have a single **continuous** variable representing the delivery time (in minutes) for 1000 online orders. You want to visualize its distribution to see the central tendency, spread, and potential **outliers**. Which visualization is **most** appropriate?
a) Pie Chart
b) Scatter Plot
c) Bar Chart
d) Histogram
e) Heatmap
#### Model Solution for 2.2:
**d) Histogram**
*   **Explanation**: A **histogram** is specifically designed to show the **frequency distribution** of a single **continuous** or discrete variable. It bins the data and shows counts, allowing you to see the shape (normal, skewed), central location, spread, and presence of outliers. A **box plot** would also be a suitable alternative.
#### Question 2.3 (3 marks)
For the following scenarios, select the **most suitable** type of plot from the list. Each option can be used once, more than once, or not at all.
*Plots:* Scatter Plot, Box Plot, Time Series Plot, Heatmap, Bar Chart.

i) Comparing the **median** income and the **interquartile range (IQR)** across five different professions.
ii) Investigating the relationship between advertising spend (x-axis) and sales revenue (y-axis) for 50 marketing campaigns.
iii) Showing the **density** of data points in a 2D space where a simple scatter plot appears too cluttered with 100,000 points.
#### Model Solution for 2.3:
i) **Box Plot**. A **box plot** visualizes the median (line inside the box), the IQR (the box itself), and the range/outliers (whiskers and points), making it ideal for comparing distributions across categories.
ii) **Scatter Plot**. This is the standard plot for visualizing the relationship (correlation, trend) between two **continuous** variables.
iii) **Heatmap**. A **heatmap** can color-code a 2D grid based on the density of points in that region, solving the **overplotting** problem in a dense scatter plot.
#### Question 2.4 (2 marks)
When using the **k-Nearest Neighbors (k-NN)** algorithm for classification, what is the effect of increasing the value of `k` (the number of neighbors)?
a) The decision boundary becomes **more complex** and the model is **more likely to overfit**.
b) The decision boundary becomes **smoother** and the model is **more likely to underfit**.
c) The computational cost of prediction decreases.
d) The model becomes more sensitive to noise and outliers in the training data.
#### Model Solution for 2.4:
**b) The decision boundary becomes **smoother** and the model is **more likely to underfit**.**
*   **Explanation**: In **k-NN**, a small `k` (like 1) means predictions are based on the single closest point, leading to a very complex, jagged boundary that is highly sensitive to noise (**high variance**). A large `k` averages over many neighbors, smoothing the boundary and reducing variance, but can oversimplify and miss important patterns (**high bias** or **underfitting**).
#### Question 2.5 (5 marks - Open Ended)
You are asked to build a prototype system to classify types of iris flowers (Setosa, Versicolor, Virginica) based on sepal and petal measurements using **k-NN**.

a)  What is a critical **data preprocessing step** you must perform before training the k-NN model, and why is it necessary? (2 marks)
b)  You train the model but find its performance on a test set is poor. You suspect the issue is the choice of `k`. Describe how you could systematically use **cross-validation** to choose a good value for `k`. (3 marks)
#### Model Solution for 2.5:
a)  **Feature Scaling** (e.g., **Standardization** or **Normalization**) is critical.
    *   **Why**: k-NN uses distance metrics (like **Euclidean distance**). If features are on different scales (e.g., sepal length in cm, petal width in mm), the feature with the larger scale will dominate the distance calculation. Scaling ensures all features contribute equally to the similarity measure.

b)  To choose `k` using **cross-validation**:
    1.  Define a range of potential `k` values to test (e.g., k = 1, 3, 5, 7, 9, 11).
    2.  Apply **k-fold cross-validation** (e.g., 5-fold) on the *training set*. For each candidate `k` value:
        *   Shuffle and split the training data into `k` folds.
        *   For each fold, treat it as a validation set and train the k-NN model with the candidate `k` on the remaining folds.
        *   Calculate the chosen performance **metric** (e.g., **accuracy** or **F1 score**) on the held-out fold.
        *   Average the performance metric across all `k` folds. This is the **cross-validation score** for that candidate `k`.
    3.  Select the `k` value that yields the **highest average cross-validation score**. This method provides a robust estimate of how each `k` will generalize, helping to avoid choosing a `k` that overfits the specific training-test split.
#### Question 2.6 (1 mark - Select Three)
What are the main **disadvantages** of the k-Nearest Neighbors algorithm? Select **three**.
a) It is very **interpretable**.
b) It has **high computational cost at prediction time**.
c) It performs well with a very large number of features (**high dimensionality**).
d) It is **sensitive to the scale of the features**.
e) Its performance degrades with **irrelevant features**.
#### Model Solution for 2.6:
**b) It has **high computational cost at prediction time**.**, **d) It is **sensitive to the scale of the features**.**, and **e) Its performance degrades with **irrelevant features**.**
*   **Explanation**: k-NN must compute the distance from a new point to *every* point in the training set to make a prediction, which is computationally expensive for large datasets (**b**). As a distance-based algorithm, it requires features to be on comparable scales (**d**) and suffers from the **curse of dimensionality**, where performance drops as the number of irrelevant or noisy features increases (**e**). Interpretability (**a**) is often considered an *advantage* for small k.
### Section 3: Decision Trees & Random Forests (Lectures 6 & 7)
#### Question 3.1 (2 marks)
In the context of building a **Decision Tree** for classification, which of the following statements about **Entropy** are correct? Select **two**.
a) A node where all data points belong to the same class has an entropy of 1.
b) **Entropy** is a measure of **impurity** or disorder in a node.
c) The algorithm selects the split that results in the **largest increase** in the weighted average entropy of the child nodes.
d) A node with data evenly split between two classes has a higher entropy than a node where 90% of data is in one class.
e) The formula for entropy is $$S = -\sum_{c} p_c \log_2 p_c$$, where $p_c$ is the proportion of class $c$.
#### Model Solution for 3.1:
**b) **Entropy** is a measure of **impurity** or disorder in a node.** and **e) The formula for entropy is $$S = -\sum_{c} p_c \log_2 p_c$$, where $p_c$ is the proportion of class $c$.**
*   **Explanation**: **Entropy** quantifies impurity; a pure node has entropy 0 (not 1). The algorithm aims to *reduce* impurity, so it selects the split that gives the **largest *decrease*** in weighted entropy, also known as **information gain**. A 50/50 split represents maximum impurity/entropy for two classes.
#### Question 3.2 (1 mark)
A **fully grown** decision tree (with no constraints) is trained on a dataset and achieves 100% accuracy on the training set. What is the most likely assessment of this model's performance on unseen data?
a) It will also achieve 100% accuracy due to its perfect fit.
b) Its performance will be poor because it has **underfitted** the training data.
c) Its performance will be poor because it has **overfitted** the training data.
d) Its performance cannot be predicted.
#### Model Solution for 3.2:
**c) Its performance will be poor because it has **overfitted** the training data.**
*   **Explanation**: A decision tree that grows until all leaves are pure has **high variance**. It has essentially memorized the training data, including its **noise** and outliers. This complex model is unlikely to **generalize** well to new data, a classic sign of **overfitting**.
#### Question 3.3 (3 marks)
Match the following **regularization techniques** for controlling decision tree complexity with their correct descriptions.
1. **Pruning**
2. **Setting `max_depth`**
3. **Setting `min_samples_leaf`**

a) A **post-hoc** method where the fully grown tree is simplified by removing branches that provide little predictive power.
b) Prevents a node from being split if the number of samples in any resulting child node would fall below a threshold.
c) Directly limits how many times the data can be split, preventing the tree from becoming too deep and complex.
#### Model Solution for 3.3:
1. **Pruning** -> **a) A **post-hoc** method where the fully grown tree is simplified by removing branches that provide little predictive power.**
2. **Setting `max_depth`** -> **c) Directly limits how many times the data can be split, preventing the tree from becoming too deep and complex.**
3. **Setting `min_samples_leaf`** -> **b) Prevents a node from being split if the number of samples in any resulting child node would fall below a threshold.**
*   **Explanation**: These are key methods to reduce **overfitting**. `max_depth` and `min_samples_leaf` are **pre-pruning** constraints applied during tree building. **Pruning** is a **post-pruning** technique applied after the tree is fully grown.
#### Question 3.4 (2 marks - Select Two)
Which of the following are core ideas behind the **Random Forest** algorithm?
a) It builds a single, very deep decision tree.
b) It uses **bagging (Bootstrap Aggregating)** to create diversity among trees.
c) It uses **boosting** to sequentially correct the errors of previous trees.
d) When splitting a node, it considers only a **random subset of features**.
e) The final prediction is made by averaging the predictions of all trees.
#### Model Solution for 3.4:
**b) It uses **bagging (Bootstrap Aggregating)** to create diversity among trees.** and **d) When splitting a node, it considers only a **random subset of features**.**
*   **Explanation**: **Random Forest** is an **ensemble** of decision trees. **Bagging** (training on bootstrap samples) reduces variance. The **random feature subset** at each split decorrelates the trees further, making the ensemble more robust. The final prediction for classification is by **majority vote**, not averaging (which is for regression).
#### Question 3.5 (5 marks - Scenario Based)
*Scenario:* A telecom company wants to predict customer churn (whether a customer will leave within the next month). They have data on call duration, contract type, customer service calls, and tenure. The team builds both a **single Decision Tree** and a **Random Forest** model.

a)  The single Decision Tree is very easy for the business team to interpret but is suspected of **overfitting**. Name **two** hyperparameters you could adjust to reduce its complexity and combat overfitting. (2 marks)
b)  The Random Forest model performs significantly better on the validation set. Explain **why** Random Forests generally have better **generalization** performance than a single Decision Tree. (2 marks)
c)  What is a **key disadvantage** of the Random Forest model compared to the single Decision Tree in this business context? (1 mark)
#### Model Solution for 3.5:
a)  Two hyperparameters to reduce overfitting in a Decision Tree:
    1.  `max_depth`: Limits the maximum depth of the tree, preventing it from learning overly specific rules.
    2.  `min_samples_split`: Sets the minimum number of samples required to split an internal node, preventing splits on very small groups that likely represent noise.

b)  Random Forests improve **generalization** through **ensemble learning**:
    *   **Variance Reduction via Bagging**: By training many trees on different **bootstrap samples** of the data and averaging (voting), the **high variance** of individual trees is reduced. Errors from one tree are often canceled out by others.
    *   **Decorrelation via Random Features**: Using a random subset of features at each split ensures trees are diverse and not all making the same error based on the strongest feature. This further reduces the ensemble's overall variance, leading to better performance on unseen data.

c)  **Loss of Interpretability**. While a single Decision Tree provides clear **if-then** rules that are easy to explain (a **white box** model), a Random Forest's prediction comes from the consensus of hundreds of trees, making it much harder to understand *why* a specific prediction was made (a **black box** model). This can be a problem when business decisions require justification.
#### Question 3.6 (1 mark)
When using a Decision Tree for a **regression** task (predicting a continuous value), what does the algorithm typically predict for a data point that falls into a particular leaf node?
a) The most frequent class label in that node.
b) The **mean** (or median) of the target values of the training samples in that node.
c) A linear combination of the input features.
d) A random value from the distribution in that node.
#### Model Solution for 3.6:
**b) The **mean** (or median) of the target values of the training samples in that node.**
*   **Explanation**: In **regression trees**, the splitting criterion minimizes the **variance** (or MSE) within child nodes. The prediction for a new point is simply the average target value of all training points that reside in the same leaf node, resulting in a **piecewise constant** prediction function.

### Section 4: Clustering & Similarity (Lecture 5)
#### Question 4.1 (1 mark)
The **k-Means** clustering algorithm is most appropriate for which of the following scenarios?
a) Grouping news articles into an unknown number of topics based on text similarity.
b) Finding arbitrarily shaped clusters in spatial data with noise.
c) Partitioning customers into a pre-defined number of segments based on purchasing behavior, expecting spherical clusters.
d) Reducing the dimensionality of a dataset for visualization.
#### Model Solution for 4.1:
**c) Partitioning customers into a pre-defined number of segments based on purchasing behavior, expecting spherical clusters.**
*   **Explanation**: **k-Means** requires the number of clusters `k` to be specified in advance and assumes clusters are **spherical** (defined by distance to a **centroid**) and of similar size/density. It is not suitable for arbitrary shapes (use **DBSCAN**) or unknown `k`, and is not a dimensionality reduction technique (use **PCA**).
#### Question 4.2 (2 marks - Select Two)
Which of the following are **failure cases** or limitations of the standard **k-Means** clustering algorithm?
a) It can effectively identify clusters with complex, non-convex shapes.
b) It is sensitive to the initial random placement of **centroids**.
c) It requires the data to be **scaled** for features on different units.
d) It automatically determines the optimal number of clusters `k`.
e) It performs well when clusters have very different densities.
#### Model Solution for 4.2:
**b) It is sensitive to the initial random placement of **centroids**.** and **c) It requires the data to be **scaled** for features on different units**.**
*   **Explanation**: k-Means uses **distance metrics**, so different initializations can lead to different local optima, and features on different scales will distort distances. It **cannot** handle non-convex shapes (**a** is false), does **not** determine `k` automatically (**d** is false), and struggles with differing densities (**e** is false).
#### Question 4.3 (3 marks)
The **Agglomerative Hierarchical Clustering** algorithm starts with each data point as its own cluster. Which of the following **linkage criteria** is described in each statement?

*Linkage Options:* Single, Complete, Average, Ward.

i) Merges clusters based on the **smallest maximum distance** between any point in one cluster and any point in the other. Tends to create compact clusters.
ii) Merges clusters based on the **average distance** between all pairs of points in the two clusters. A compromise between single and complete.
iii) Merges clusters to minimize the increase in total within-cluster **variance**. Tends to create clusters of roughly equal size.
#### Model Solution for 4.3:
i) **Complete**. This uses the furthest neighbor distance, making it sensitive to outliers but encouraging compact clusters.
ii) **Average**. This is the standard compromise, less sensitive to outliers than single or complete.
iii) **Ward**. This is a variance-minimizing approach and is the default in many libraries like `scikit-learn`.
#### Question 4.4 (1 mark)
In the output of a **dendrogram** from hierarchical clustering, what does the **height** of a branch (the y-axis value) represent?
a) The number of data points in the merged cluster.
b) The **distance** (dissimilarity) at which the two clusters were merged.
c) The cluster's assigned label.
d) The purity of the cluster.
#### Model Solution for 4.4:
**b) The **distance** (dissimilarity) at which the two clusters were merged.**
*   **Explanation**: The **dendrogram** visualizes the sequence of merges. The height indicates the linkage distance at which two clusters combine. A large jump in height suggests merging two very dissimilar clusters, which can help in deciding where to "cut" the tree to obtain a flat clustering.
#### Question 4.5 (5 marks - Open Ended)
You are given a dataset of sensor readings from a network of industrial machines. The goal is to identify groups of machines with similar operational patterns (**unsupervised learning**). You apply **k-Means**.

a)  How could you use the **inertia** (within-cluster sum of squares) metric to help choose a value for `k`? Describe the common heuristic. (2 marks)
b)  You plot the clusters and discover they are long, elongated ellipses, not spheres. Why is k-Means poorly suited for this data shape, and name **one** alternative clustering algorithm that would be more appropriate. (2 marks)
c)  Your colleague suggests using **Euclidean distance**. The features include `temperature (°C)`, `pressure (kPa)`, and `vibration (mm/s)`. What **preprocessing step** is absolutely essential before applying k-Means, and why? (1 mark)
#### Model Solution for 4.5:
a)  To choose `k` using **inertia**, you plot inertia against a range of `k` values (the "elbow method"). **Inertia** decreases as `k` increases. The heuristic is to look for an **"elbow point"** where the rate of decrease sharply slows. This point suggests that increasing `k` beyond that value yields diminishing returns, providing a good trade-off between model complexity and explained variance.
b)  **k-Means assumes clusters are spherical** because it assigns points to the nearest **centroid** based on Euclidean distance. This creates **Voronoi cell** boundaries that are linear. For elongated or manifold-shaped clusters, this model is incorrect.
    *   **Alternative Algorithm**: **DBSCAN** (Density-Based Spatial Clustering) is more appropriate as it can find clusters of arbitrary shape based on regions of high density and is robust to outliers.
c)  **Feature Scaling** (e.g., **Standardization**) is essential. Because k-Means uses **Euclidean distance**, features with larger magnitudes and units (like `pressure` in kPa, which could be in the thousands) would dominate the distance calculation compared to `temperature` (tens of degrees) or `vibration`. Scaling ensures each feature contributes equally to the similarity measure.
#### Question 4.6 (2 marks)
Consider the following distance matrix between four points (A, B, C, D). Using **Single Linkage** hierarchical clustering, which two clusters would be merged in the **first step**?
```
   A    B    C    D
A  0    2    6    10
B  2    0    5    9
C  6    5    0    4
D  10   9    4    0
```
a) A and B
b) B and C
c) C and D
d) A and D
#### Model Solution for 4.6:
**a) A and B**
*   **Explanation**: **Single Linkage** uses the *minimum* distance between points in two clusters. Initially, each point is its own cluster. The smallest distance in the entire matrix is 2 (between A and B). Therefore, clusters containing A and B are merged first.

### Section 5: Regression, Overfitting & Model Evaluation (Lectures 7 & 8)
#### Question 5.1 (1 mark)
You are fitting a **polynomial regression** model of the form $y = w_0 + w_1x + w_2x^2 + ... + w_Mx^M$. As you increase the polynomial degree $M$ from 1 to 9, you observe the training error drops to nearly zero, but the test error increases sharply. This is a classic sign of:
a) **Underfitting**
b) **High bias**
c) **Overfitting**
d) Perfect generalization
#### Model Solution for 5.1:
**c) **Overfitting****
*   **Explanation**: A model that performs almost perfectly on training data but poorly on unseen test data has **low bias** but **high variance**. It has learned the training data's noise and specifics too well, failing to **generalize**. This is the definition of **overfitting**, which often occurs with overly complex models like high-degree polynomials.
#### Question 5.2 (2 marks)
Which of the following strategies are effective in **preventing overfitting** in a machine learning workflow? Select **two**.
a) Using a more complex model (e.g., higher degree polynomial).
b) Applying **regularization** (e.g., Lasso or Ridge regression).
c) Getting more high-quality training data.
d) Reducing the size of the test set.
e) Evaluating the model only on the training set.
#### Model Solution for 5.2:
**b) Applying **regularization** (e.g., Lasso or Ridge regression).** and **c) Getting more high-quality training data.**
*   **Explanation**: **Regularization** adds a penalty for model complexity, discouraging overfitting. **More data** helps the model learn the true underlying pattern rather than noise. Using a more complex model (**a**) typically increases overfitting risk. Reducing the test set (**d**) or not using one (**e**) prevents proper detection of overfitting.
#### Question 5.3 (3 marks)
For each of the following evaluation contexts, select the **most appropriate primary metric**.
*Metrics:* R-squared ($R^2$), Mean Absolute Error (MAE), Accuracy, F1 Score, Silhouette Score.

i) Evaluating a **linear regression** model predicting house prices.
ii) Evaluating a **binary classifier** for detecting a rare disease (1% prevalence).
iii) Evaluating the quality of clusters found by **k-means** (no ground truth labels).
#### Model Solution for 5.3:
i) **Mean Absolute Error (MAE)** or **R-squared**. MAE is easily interpretable (average error in price units). $R^2$ explains the proportion of variance captured.
ii) **F1 Score**. For **imbalanced classification**, accuracy is misleading. **F1 Score** balances **precision** (minimizing false alarms) and **recall** (catching all true cases), which are both critical in medical diagnosis.
iii) **Silhouette Score**. This is an **internal clustering validation** metric that measures how similar an object is to its own cluster compared to other clusters, without requiring true labels.
#### Question 5.4 (1 mark - Select Two)
Which of the following are **advantages** of using **k-fold cross-validation** compared to a single train/test split?
a) It reduces the total computational cost of training models.
b) It provides a more **robust** estimate of model performance by using multiple data splits.
c) It makes more efficient use of the available data for both training and validation.
d) It guarantees the selected model will not overfit.
e) It eliminates the need for a final, held-out test set.
#### Model Solution for 5.4:
**b) It provides a more **robust** estimate of model performance by using multiple data splits.** and **c) It makes more efficient use of the available data for both training and validation.**
*   **Explanation**: By averaging performance over `k` different splits, CV reduces the variance of the performance estimate. Every data point is used for both training (in k-1 folds) and validation (in 1 fold), maximizing data usage. It is **more** computationally expensive (**a** false), does not guarantee no overfit (**d** false), and a final test set is still needed for an unbiased evaluation of the model chosen after CV (**e** false).
#### Question 5.5 (5 marks - Scenario Based)
*Scenario:* You are building a model to predict student final exam scores (0-100) based on hours studied, attendance, and previous grades.

a)  What type of machine learning task is this? (1 mark)
b)  You fit a **linear regression** model. The assumptions of linear regression include that the relationship between features and target is linear, and errors are normally distributed. What **plot** would you create to check for **non-linearity**? (1 mark)
c)  The model's performance on a test set is measured as: **MAE = 8.5**, **$R^2$ = 0.62**. Interpret these results. What does an $R^2$ of 0.62 mean? (2 marks)
d)  To improve performance, you add polynomial features (e.g., `(hours_studied)^2`). The training $R^2$ increases to 0.95, but the test $R^2$ drops to 0.55. What is happening, and what technique could you use to address this? (1 mark)
#### Model Solution for 5.5:
a)  This is a **regression** task (predicting a continuous numerical score).

b)  A **residual plot** (plotting residuals $y - \hat{y}$ against predicted values $\hat{y}$). If the relationship is linear, residuals should be randomly scattered around zero. A clear pattern (e.g., U-shape) suggests **non-linearity**.

c)  **Interpretation**:
    *   **MAE = 8.5**: On average, the model's predictions are off by 8.5 points from the actual exam scores.
    *   **$R^2$ = 0.62**: The model explains 62% of the **variance** in the final exam scores. This means 38% of the variance is due to factors not captured by the model (or random noise). It indicates a moderately strong, but not perfect, fit.

d)  This is **overfitting**. The complex polynomial model fits the training noise. To address this, apply **regularization** (e.g., **Ridge** or **Lasso regression**) which adds a penalty on the size of coefficients, shrinking them and reducing model complexity/variance.
#### Question 5.6 (2 marks)
In the context of a **binary classification** problem, the **confusion matrix** shows:
*   True Positives (TP) = 80
*   False Positives (FP) = 20
*   True Negatives (TN) = 50
*   False Negatives (FN) = 10

Calculate the **Precision** and **Recall** of the model.
#### Model Solution for 5.6:
**Precision** = TP / (TP + FP) = 80 / (80 + 20) = **80 / 100 = 0.80 (or 80%)**
**Recall** = TP / (TP + FN) = 80 / (80 + 10) = **80 / 90 ≈ 0.889 (or 88.9%)**

*   **Explanation**: **Precision** answers: "Of all predicted positives, how many were correct?" **Recall** answers: "Of all actual positives, how many did we find?"
### Section 6: Dimensionality Reduction & PCA (Lectures 9 & 10)
#### Question 6.1 (1 mark)
What is the primary goal of **Principal Component Analysis (PCA)**?
a) To classify data into distinct groups.
b) To predict a target variable using linear combinations of features.
c) To reduce the dimensionality of data while preserving as much variance as possible.
d) To identify and remove all correlated features from a dataset.
#### Model Solution for 6.1:
**c) To reduce the dimensionality of data while preserving as much variance as possible.**
*   **Explanation**: **PCA** is an **unsupervised**, linear **dimensionality reduction** technique. It finds a new set of orthogonal axes (**principal components**) ordered by the amount of variance they capture from the data. The first PC captures the most variance.
#### Question 6.2 (2 marks - Select Two)
Which of the following are **essential steps** in performing PCA using Singular Value Decomposition (SVD)?
a) **Centering** the data by subtracting the mean of each feature.
b) **Standardizing** the data by scaling to unit variance (if features are on different scales).
c) Adding polynomial features to capture non-linearity.
d) Performing **k-means clustering** on the result.
e) Discarding the principal components with the **largest** singular values.
#### Model Solution for 6.2:
**a) **Centering** the data by subtracting the mean of each feature.** and **b) **Standardizing** the data by scaling to unit variance (if features are on different scales).**
*   **Explanation**: **Centering** (subtracting the mean) is **mandatory** for PCA, as it ensures the first PC describes the direction of maximum variance *around the mean*. **Standardization** (scaling) is **highly recommended** when features have different units/scales; otherwise, high-variance features will dominate PC1. We discard components with the **smallest** singular values (**e** is false).
#### Question 6.3 (3 marks)
After performing PCA on a 10-dimensional dataset, you obtain the singular values: $\Sigma = [45.2, 20.1, 8.5, 2.1, 0.9, 0.3, ...]$.

a) What is the **minimum** possible dimensionality (rank) of the original data? (1 mark)
b) Approximately what percentage of the total variance is captured by the **first two** principal components? (Show your reasoning) (2 marks)
#### Model Solution for 6.3:
a) The **minimum possible rank** is the number of non-zero singular values. Based on the sharp drop, the rank is likely 5 or 6 (values 0.9 and 0.3 might be non-zero). However, the *given information* shows at least 6 non-zero values. A safe answer is **at least 6**.

b) **Variance Calculation**:
    The variance captured by the $i$-th PC is proportional to $\sigma_i^2$.
    *   Variance from PC1: $(45.2)^2 = 2043.04$
    *   Variance from PC2: $(20.1)^2 = 404.01$
    *   **Total variance** for the first two: $2043.04 + 404.01 = 2447.05$
    *   To estimate *total* variance, sum squares of all singular values. The first few dominate. A rough estimate using the first 6: $2043.04 + 404.01 + 72.25 + 4.41 + 0.81 + 0.09 \approx 2524.6$.
    *   **Percentage** = $(2447.05 / 2524.6) * 100 \approx **96.9%**.
    *   **Conclusion**: The first two PCs capture **approximately 97%** of the total variance, indicating the data is essentially 2D.
#### Question 6.4 (1 mark)
How does a **Scree Plot** aid in the PCA process?
a) It visualizes the original data points in the space of the first two principal components.
b) It plots the cumulative variance explained against the number of principal components.
c) It plots the individual **variance explained** (or singular values) of each principal component in descending order.
d) It shows the correlation between original features and principal components.
#### Model Solution for 6.4:
**c) It plots the individual **variance explained** (or singular values) of each principal component in descending order.**
*   **Explanation**: The **Scree Plot** helps choose the number of components $k$ to retain. One looks for an **"elbow"** – a point where the explained variance drops sharply and levels off. Components after the elbow contribute little variance and are likely noise.
#### Question 6.5 (5 marks - Open Ended)
You have a dataset of customer profiles with 50 features, including age, annual spend on 20 different product categories, and 30 survey response scores. You want to segment customers for targeted marketing.

a)  Why might you apply **PCA** as a preprocessing step before a clustering algorithm like k-means? (2 marks)
b)  After PCA, you find that you need 8 principal components to explain 90% of the variance. Is it guaranteed that these 8 PCs will lead to better customer segments than using all 50 original features? Explain. (2 marks)
c)  What is a **key disadvantage** of using PCA in this analysis? (1 mark)
#### Model Solution for 6.5:
a)  PCA is useful before clustering for two main reasons:
    1.  **Curse of Dimensionality**: High-dimensional spaces are sparse, making distance metrics less meaningful. PCA reduces noise and focuses on the most informative directions, leading to more stable and interpretable clusters.
    2.  **Decorrelation**: k-means uses Euclidean distance, which can be misled by highly correlated features (e.g., spending on similar product categories). PCA produces **orthogonal** (uncorrelated) components, ensuring each dimension contributes independent information.

b)  **No, it is not guaranteed.** PCA finds components that maximize **global variance**, which may not align with the **cluster structure** relevant for segmentation. The variance most important for distinguishing customer groups might be captured in lower-variance PCs that are discarded. PCA is an **unsupervised** compression, not optimized for any specific downstream task like clustering.

c)  **Loss of Interpretability**. The principal components are linear combinations of all original features (e.g., `0.3*Age + 0.05*Spend_Books - 0.4*Spend_Electronics ...`). It is very difficult to explain what a cluster defined by high "PC2 Score" means in business terms, unlike a cluster defined by "high spend on electronics, young age".
#### Question 6.6 (2 marks)
Which of the following statements about the outputs of SVD ($X = U \Sigma V^T$) in the context of PCA is **correct**?
a) The columns of $V$ (rows of $V^T$) are the **principal component directions** in the original feature space.
b) The matrix $U$ contains the original data projected onto the principal components.
c) The product $U \Sigma$ gives the **principal component scores** for each data point.
d) All of the above.
#### Model Solution for 6.6:
**d) All of the above.**
*   **Explanation**: For centered data $X$: (a) The columns of $V$ are the eigenvectors (PC directions). (b) $U$ contains the coordinates of the data in the orthonormal basis defined by $V$. (c) $U \Sigma$ scales these coordinates by the importance (singular values) of each PC, yielding the final PC scores. Therefore, all statements are correct.

### Section 7: Ethics & Data Governance (Lecture 11)
#### Question 7.1 (2 marks)
According to UK-GDPR, what is the key distinction between a **Data Controller** and a **Data Processor**?
a) The Controller collects the data, the Processor analyzes it.
b) The Controller determines the **purposes and means** of processing, the Processor acts **on behalf of** the Controller.
c) The Processor is always an external company, the Controller is always internal.
d) The Controller is responsible for data security, the Processor is responsible for data accuracy.
#### Model Solution for 7.1:
**b) The Controller determines the **purposes and means** of processing, the Processor acts **on behalf of** the Controller.**
*   **Explanation**: This is the legal definition. The **Data Controller** is the entity that decides *why* and *how* personal data is processed. The **Data Processor** follows the controller's instructions to carry out the processing. The same entity can be both for different data activities.
#### Question 7.2 (1 mark - Select Two)
Which of the following are classified as **Special Category Data** under UK-GDPR?
a) A person's home address.
b) Data revealing a person's **religious beliefs**.
c) **Biometric data** used for identification.
d) A record of product purchases.
e) An individual's job title.
#### Model Solution for 7.2:
**b) Data revealing a person's **religious beliefs**.** and **c) **Biometric data** used for identification.**
*   **Explanation**: **Special Category Data** is sensitive personal data that merits higher protection. It includes data revealing racial/ethnic origin, political opinions, religious/philosophical beliefs, trade union membership, genetic data, biometric data (for ID), health data, sex life, and sexual orientation. An address or purchase history is personal data, but not *special category*.
#### Question 7.3 (3 marks)
The UK-GDPR outlines principles for processing personal data. Name **three** of these seven principles.
#### Model Solution for 7.3:
(Any three of the following are correct)
1.  Processed **lawfully, fairly and transparently**.
2.  Collected for **specified, explicit and legitimate purposes** (purpose limitation).
3.  **Adequate, relevant and limited** to what is necessary (data minimisation).
4.  **Accurate** and kept up to date.
5.  Kept for no longer than **necessary** (storage limitation).
6.  Processed securely (**integrity and confidentiality**).
7.  The controller is **accountable** for compliance.
#### Question 7.4 (2 marks)
In the context of data ethics, **valid consent** requires more than a user ticking a box. What are **two** of the three key conditions for consent to be valid?
#### Model Solution for 7.4:
(Any two of the following are correct)
1.  **Informed**: The data subject must understand what they are consenting to (how their data will be used).
2.  **Autonomous**: Consent must be freely given, without coercion or undue pressure.
3.  **Ongoing**: Consent should not be considered permanent; subjects must be able to **withdraw** it easily at any time.
#### Question 7.5 (5 marks - Scenario Based)
A city council deploys a **predictive policing** algorithm. The algorithm uses historical crime data (location, time, type) to predict areas with high future crime risk, directing police patrols accordingly. A civil rights group raises an ethical concern that the algorithm is **biased**, leading to over-policing in predominantly minority neighbourhoods.

a)  What type of **bias** is most likely present in the training data for this algorithm? (1 mark)
b)  Explain **how** this historical bias can lead the algorithm to produce discriminatory outcomes, even if the variable 'race' is not an input feature. (2 marks)
c)  From a data governance perspective, does the council likely need to obtain **explicit consent** from citizens to process their data in this system? Justify your answer based on a UK-GDPR principle. (2 marks)
#### Model Solution for 7.5:
a)  **Historical Bias** (or **Societal Bias**). The training data reflects past policing practices, which may have been disproportionately focused on certain neighbourhoods due to conscious or unconscious bias, leading to more recorded crimes there.

b)  This happens through **proxy discrimination**. Even if 'race' is excluded, the algorithm may use highly correlated **proxy variables** such as postcode/zip code, socio-economic data, or even reported noise complaint frequency. If these proxies are systematically linked to race due to residential segregation and historical inequality, the algorithm will learn and perpetuate the biased pattern, assigning higher risk scores to minority neighbourhoods based on the biased historical data.

c)  **No, explicit consent is likely not required** in this specific case.
    *   **Justification**: Processing for **law enforcement** or **tasks in the public interest** (like crime prevention) is a separate lawful basis under UK-GDPR, distinct from consent. The council would likely rely on the principle of processing for a **"task carried out in the public interest"** or **"substantial public interest"**. However, they must still comply with all other principles, especially **transparency** (informing the public about the system) and ensuring **fairness** by actively mitigating the identified bias.
#### Question 7.6 (2 marks)
The **Facebook Emotional Contagion Experiment** (2014) manipulated users' news feeds. Which **ethical principle** was most clearly violated by conducting this experiment without users' explicit knowledge?
a) **Data Minimisation**
b) **Informed Consent**
c) **Storage Limitation**
d) **Fairness**
#### Model Solution for 7.6:
**b) **Informed Consent****
*   **Explanation**: The core ethical failure was that users were subjects of a psychological experiment without their knowledge or agreement. Burying the possibility of such research in the general Terms of Service does not constitute **valid consent**, which must be specific, **informed**, and unambiguous. This violates the ethical and legal requirement for **autonomy** and respect for persons.
### Final: Composite & Scenario-Based Questions (Multiple Topics)
#### Question C.1 (10 marks - Major Scenario)
*Scenario:* "Books&Beans", a subscription book and coffee box service, wants to improve customer retention. They have a customer database with: `customer_id`, `signup_date`, `total_boxes_purchased`, `avg_rating`, `days_since_last_order`, `marketing_emails_opened`, `preferred_genre` (Fantasy, Romance, etc.), and `churned` (1 if cancelled, 0 if active). The `churned` column was added 6 months ago when they started tracking this.

**Task:** Design a data science project to predict which active customers are at high risk of churning in the next month.

a)  **Problem Framing & Data (2 marks):**
    i) What is the **machine learning task**? (Classification/Regression/Clustering)
    ii) What is the **target variable** (output)? What are the **features** (inputs)? Give examples.
    iii) A colleague suggests using all historical data since the company started 5 years ago. What is a potential **data quality issue** with this regarding the target variable?

b)  **Modeling & Evaluation (4 marks):**
    i) The dataset is imbalanced (only 10% churned). Why is **accuracy** a poor metric? What **two better metrics** should be reported, and why?
    ii) You decide to try a **Decision Tree** and a **Random Forest**. Explain **one key advantage** of Random Forest over a single Decision Tree in this context.
    iii) How would you use **cross-validation** to fairly compare these two model types?

c)  **Ethics & Deployment (4 marks):**
    i) The marketing team wants to use the model to offer a 20% discount *only* to customers predicted to churn. What **ethical risk** might this pose if the model has even a small false positive rate? Relate this to a **UK-GDPR principle**.
    ii) To improve the model, an engineer suggests adding a new feature: `estimated_income` (inferred from postcode data). What **special consideration** does this introduce under data protection law?
#### Model Solution for C.1:
**a) Problem Framing & Data:**
i)  This is a **binary classification** task. We are predicting a categorical label: `will_churn` (Yes/No or 1/0).
ii) **Target Variable**: `churned` (or a derived `will_churn_next_month`).
    **Features (Examples)**: `total_boxes_purchased` (numerical), `avg_rating` (numerical), `days_since_last_order` (numerical), `marketing_emails_opened` (numerical), `preferred_genre` (categorical).
iii) **Potential Data Quality Issue: Lack of Labels / Temporal Faithfulness**. The `churned` label only exists for the last 6 months. For data older than that, we do not know who churned and when. Using old data without a reliable target variable would introduce **label noise** and make the model learn incorrect patterns. The **scope** and **temporality** of the data are misaligned with the problem.

**b) Modeling & Evaluation:**
i)  **Accuracy is misleading** because a model that simply predicts "will not churn" for everyone would be 90% accurate, yet useless (0% **recall** for the churn class).
    **Two Better Metrics**:
    1.  **Recall (Sensitivity)**: Crucially important. It measures the proportion of *actual churners* we correctly identify. Missing a churning customer (false negative) is a direct loss.
    2.  **Precision**: Also important. It measures the proportion of *predicted churners* who actually churn. Low precision means many false alarms, leading to wasted discounts and potential customer annoyance.
    *   The **F1 Score** (harmonic mean of precision and recall) is a good single summary metric for this imbalance.
ii) **Key Advantage of Random Forest**: **Superior Generalization / Reduced Overfitting**. A single Decision Tree is prone to **high variance** and will likely **overfit** the training data, capturing noise specific to it. The **Random Forest** ensemble, through **bagging** and random feature selection, **averages out** this variance, leading to a model that performs more consistently on unseen active customers.
iii) **Using Cross-Validation**: Use **Stratified k-Fold Cross-Validation** (e.g., 5-fold) on the *training set*.
    *   Ensure folds preserve the class imbalance ratio.
    *   For each model type (Decision Tree, Random Forest), train on `k-1` folds and evaluate on the held-out fold using the chosen metrics (F1, Recall, Precision).
    *   Calculate the **average** performance across all `k` folds for each model and metric.
    *   Compare the average scores. This gives a robust estimate of how each model type generalizes, controlling for the randomness of a single train-test split.

**c) Ethics & Deployment:**
i)  **Ethical Risk & GDPR Principle**: The risk is **unfair treatment**. Customers wrongly flagged as churners (false positives) would **unjustly receive a discount**, while loyal customers (true negatives) would not. This could be seen as arbitrary and unfair, potentially violating the UK-GDPR principle of processing data **"fairly and transparently"**. It could also breach the principle of **"accuracy"**, as decisions are based on an inaccurate prediction.
ii) **Special Consideration**: `estimated_income` inferred from postcode is likely to be considered **Special Category Data** or data requiring careful handling. Postcode can be a strong **proxy** for socio-economic status, and by extension, could indirectly reveal sensitive information. Processing this data requires a stronger lawful basis and explicit consideration of the **discrimination risks** (bias against/for certain postcodes). It increases the need for **bias auditing** of the model and heightened **transparency** measures.
#### Question C.2 (5 marks - Open Ended)
You are given a **high-dimensional** dataset (p = 500 features, n = 2000 samples) for a classification problem. Discuss the **challenges** this poses and outline a **step-by-step analytical plan** to build a robust model, mentioning specific techniques you would use at each stage.
#### Model Solution for C.2:
**Challenges:**
1.  **Curse of Dimensionality**: Distance-based algorithms (like k-NN) become ineffective. Feature space is sparse, making patterns hard to find.
2.  **Overfitting Risk**: With many features relative to samples, models can easily memorize noise.
3.  **Computational Cost**: Training can be slow.
4.  **Multicollinearity**: Many features may be highly correlated, introducing instability.

**Analytical Plan:**
1.  **EDA & Profiling**: Use the **structured workflow** from the profiling guide. Check for **missing values**, **faithfulness**, and feature distributions. Use visualizations like **pairplots** (on a feature subset) and **correlation matrices**.
2.  **Data Preprocessing**:
    *   Handle missing data (imputation or deletion).
    *   **Standardize** all features (crucial for any distance/PCA/regularized model).
3.  **Dimensionality Reduction / Feature Selection**:
    *   Apply **PCA** to reduce dimensions while preserving variance. Examine the **scree plot** to choose components.
    *   OR use **regularization-based feature selection** (e.g., **Lasso Regression**) which shrinks coefficients of irrelevant features to zero.
4.  **Modeling with Regularization**:
    *   Choose algorithms inherently resistant to high dimensions: **Regularized Logistic Regression (L1/L2)**, **Random Forest** (which performs implicit feature selection), or **Linear SVM**.
    *   **Do not use** standard k-NN or unregularized linear models.
5.  **Robust Evaluation**:
    *   Use **Stratified K-Fold Cross-Validation** to evaluate model performance reliably.
    *   For hyperparameter tuning (e.g., `C` in SVM, `max_depth` in RF), use **GridSearchCV** or **RandomizedSearchCV** *within* the cross-validation loop to avoid data leakage.
6.  **Bias & Fairness Check**: Before deployment, audit the final model for **bias** across sensitive attributes if applicable.
#### Question C.3 (3 marks - MCQ Trio)
i)  **Select one.** The **Gini impurity** in a decision tree node is zero when:
    a) The node is perfectly balanced between two classes.
    b) The node contains samples from **only one class**.
    c) The node has the maximum possible entropy.

ii) **Select one.** In **Agglomerative Clustering**, which linkage method minimizes the increase in total within-cluster variance?
    a) Single
    b) Complete
    c) Average
    d) Ward

iii) **Select two.** Which of these are **required** for **valid consent** under ethical and GDPR standards?
    a) It must be given via a handwritten signature.
    b) It must be **informed**.
    c) It can never be withdrawn once given.
    d) It must be **freely given** (autonomous).
    e) It must be specific to the described processing.
#### Model Solution for C.3:
i)  **b) The node contains samples from **only one class**.** (A pure node has 0 impurity).
ii) **d) Ward** (Ward's method merges clusters to minimize variance increase).
iii) **b) It must be **informed**.** and **d) It must be **freely given** (autonomous).** (Also, it should be specific and withdrawable).