## 1. Introduction to Distributed Systems Pt.1 - *Chapter 1*
### Definition of a Distributed System (DS)
> (1) A collection of **autonomous computing elements (nodes)** that appears to its **users as a single coherent system**.

**Nodes** are hardware devices or software processes (computers, sensors) that collaborate. **Autonomy** means each node has its own **local clock**; there is no global clock, making synchronization a core challenge. Nodes must **communicate via a network**.

> (2) A system in which components located at **networked computers** **communicate** and coordinate their actions only by **passing messages**.

The collection operates uniformly regardless of *where, when, or how* the user interacts with it. This **hides distribution** from the user (e.g., they don't know where computation happens or data is stored).

**(EXAMPLEs) 
- The **Internet**, the **World Wide Web (WWW)**, **cloud computing** platforms, **cellular networks**.
- **Distributed Applications** built on top: Netflix, Spotify, online banking.
- *"You know you have a DS when the crash of a computer you’ve never heard of stops you from getting any work done" – Leslie Lamport*
#### Distributed versus Decentralised Systems
Two perspectives exist:
1.  **Integrative View:** Connecting existing systems into a larger one.
2.  **Expansive View:** Extending an existing system with more computers.
![[centralised-decentralised-distributed-system.png\|400]]
> A **Distributed System** is a networked computer system in which processes and resources are **sufficiently** spread across multiple computers (expansive view). The distribution is done for practical reasons like performance, but could theoretically be centralised.

> A **Decentralised System** is a networked computer system in which processes and resources are **necessarily** spread across multiple computers (integrative view). The distribution is fundamental to its operation.

When data needs to stay within the perimeter of an org (e.g. security reasons), training is brought to the data. The result is known as **federated learning**.

**(EXAMPLE 1) Blockchain (Decentralised System)**
A **blockchain** or distributed ledger is decentralised because participating parties **do not trust each other**. Instead of a central authority, transactions are validated by participants and recorded in a public ledger replicated across all nodes. The spread is **necessary** due to the lack of trust.

**(EXAMPLE 2) Geographically Dispersed Monitoring (Decentralised System)**
Systems monitoring power plants, satellites, or traffic are **necessarily geographically dispersed**. The sensors/controllers must be at the physical location, while decision-making systems may be elsewhere. The spread is dictated by **spatial requirements**.

**(EXAMPLE 3) Content Delivery Network (CDN) (Distributed System)**
A CDN **copies website content** to servers worldwide. A user is redirected to a nearby server for better performance. Content is replicated **sufficiently** (where it makes sense for latency/load), not necessarily everywhere. The distribution is for **performance and dependability**.

**(EXAMPLE 4) Network-Attached Storage (NAS) (Distributed System)**
A domestic NAS acts as a central file server for multiple users. Files are stored in one place but appear integrated into each user's local system. Resources are **sufficiently** spread to enable easy sharing, but could be more centralised. The distribution provides **transparent access**.

### Goals and challenges & Sharing of resources
A DS aims for four primary goals:
(1) **Sharing of Resources** (2) **Distribution Transparency** (3) **Openness** (4) **Scalability**

Key challenges to achieve these goals involve:
* **Architecture:** Common organisational styles.
* **Processes:** Their types and relationships.
* **Communication:** Data exchange facilities (RPC, messaging).
* **Coordination:** Synchronisation algorithms.
* **Naming:** Identifying resources uniquely.
* **Consistency & Replication:** Managing copies of data.
* **Fault Tolerance:** Handling partial failures.
* **Security:** Ensuring authorised access.
Sharing resources (storage, compute, data) is economically efficient and facilitates collaboration (e.g., cloud storage, groupware, the Internet itself). It allows geographically dispersed groups to work together seamlessly.
### Transparency
>The phenomenon by which a DS attempts to *hide* that its processes and resources are *physically distributed* across *multiple computers*, possibly separated by large distances.

This is primarily achieved by **middleware**, a software layer that sits between applications and operating systems, masking heterogeneity and distribution. Its the glue *between* apps and OSs, extending over multiple machines; contains commonly used components and functions that need not be implemented by apps separately.
![[middleware-layer-ex.png\|400]]
**(TYPES OF TRANSPARENCY)** Different aspects can be hidden:

| Transparency | Description |
| :--- | :--- |
| **Access** | Hide differences in data representation/access methods. |
| **Location** | Hide where an object is physically located. |
| **Migration** | Hide that an object may move. |
| **Relocation** | Hide that an object may move *while in use*. |
| **Replication** | Hide that multiple copies of an object exist. |
| **Concurrency** | Hide that an object is shared by multiple users. |
| **Failure** | Hide the failure and recovery of an object. |
**Note:** Achieving full transparency can be costly and may conflict with performance or simplicity.
### Openness
> An **open DS** is built from components with well-defined, standard interfaces, allowing them to easily interoperate with components from other systems, supporting extensibility and portability.
![[middleware-openness-ex.png\|200]]
 Open systems rely on agreed-upon **interfaces** and **communication protocols**, allowing services from different vendors or domains to interact.
### Scalability Issues
A DS is scalable if it remains effective when scaled in:
1.  **Size:** Number of users/processes.
2.  **Geography:** Distance between nodes.
3.  **Administrative Domains:** Number of independent managing organizations.

**(SCALING SIZE)** Scalability for the internet was effortless because information is organised hierarchically rather than linearly: $O(\log(n))$. Poor scalability is when the cost of supporting $n$ users is worse than $O(n)$.

**(SCALING GEOGRAPHICALLY - THE LATENCY & BANDWIDTH PROBLEM)**
Techniques that work in a Local Area Network (LAN) often fail in a Wide Area Network (WAN).
* **Latency:** WAN communication (~100s ms) is **orders of magnitude slower** than LAN communication (~100s µs). Synchronous communication (client blocks for a reply) becomes a major bottleneck.
* **Bandwidth & Reliability:** WANs have limited bandwidth and are less reliable. Solutions like efficient video streaming designed for LANs fail over WANs.
* **Discovery:** LANs often use efficient **broadcasting** for service discovery. WANs lack this, requiring scalable, separate directory services.

**(SCALING ADMINISTRATIVELY - THE TRUST & POLICY PROBLEM)**
Expanding across administrative domains (e.g., different companies) introduces:
* **Conflicting Policies:** Different rules for resource usage, payment, and management.
* **Security Challenges:**
  *   The DS must **protect itself** from malicious attacks from the new domain.
  *   The new domain must **protect itself** from malicious code or actions originating from the DS (e.g., untrusted downloaded applets).

**Examples:** A scientific **computational grid** spanning universities must enforce access controls. A **peer-to-peer (P2P)** file-sharing network like BitTorrent bypasses administrative control, scaling easily because end-users, not administrators, collaborate to run the system.
### Scaling Techniques
> SCALING UP is a solution of capacity improvement through memory increases, CPU upgrades, or network module replacement

> SCALING OUT is a solution of increasing machine deployment via hiding communication latencies, work distribution and replication.

Three fundamental techniques address scalability:
#### (SOLUTION 1) Hiding Communication Latencies
Improve perceived performance by avoiding blocking waits, especially important for **geographic scalability**. This approach is about making better use of response times.
![[hiding-comm-latencies-ex.png\|500]]
* **Use Asynchronous Communication:** The client sends a request and continues other work, processing the reply later (via callbacks or separate threads).
* **Move Computation to Client:** Instead of many client-server messages, ship code (e.g., a form-validation script) to the client. The client returns only the final result. This is widely used on the **Web (JavaScript, WebAssembly)**.
#### (SOLUTION 2) Partitioning and Distribution
Split a large component into smaller parts and distribute them across the system.
**(EXAMPLE - DOMAIN NAME SYSTEM (DNS))**
The DNS namespace is partitioned into hierarchical, non-overlapping **zones**. Each zone is managed by a separate **name server**. Resolving a name like `flits.cs.vu.nl` involves querying servers up the hierarchy.
![[zones-dns.png\|300]]
This **distribution** and **decentralised administration** allows DNS to scale to the entire Internet. Lookup time grows **logarithmically**, not linearly, with the number of hosts.

RESOLVING a name means returning the network address of the associated host.
E.g. `flits.cs.vu.nl` $R(\text{name}, Z_i) = \text{address}$ be the resolving of a name to a zone $i$ to an address. 
$$\begin{align} 
R(\text{flits.cs.vu.nl}, Z_1) = Z_2) \\ 
R(\text{flits.cs.vu}, Z_2) = Z_3) \\
R(\text{flits.cs}, Z_3) = \\
\text{host address} \\
\end{align}$$
#### (SOLUTION 3) Replication
Scalability problems often appear in the form of *performance degradation*, the solution is to create copies of components (data, services) across the DS.
* **Benefits:** Improves **performance** (load balancing), **availability** (fault tolerance), and can **hide latency** (place copy near user).
* **The Major Drawback: Consistency.** Keeping all replicas identical requires **global synchronisation** on every update, which is costly and hinders scalability. Often, consistency must be relaxed to achieve scale (e.g., eventual consistency).
### Pitfalls (9)
A common source of complexity in DS design comes from mistaken assumptions:
The network is: **(1)** **reliable**, **(2)** **secure**, **(3)** **homogeneous**.
**(4)** Topology **doesn't** change. **(5)** Latency is **zero**. **(6)** Bandwidth is **infinite**.
**(7)** Transport cost is **zero**. **(8)** There is **one** admin. **(9)** All clocks are **synchronised**.

These properties are **unique challenges of distributed systems** and must be explicitly considered; they do not appear in centralized system design.

**Check of Understanding**
> **Question:** The DNS is a critical, globally scalable distributed system. Explain how its design employs **two** of the three core scaling techniques (Hiding Latency, Partitioning, Replication) to achieve scalability. Provide a specific detail of how DNS implements each chosen technique.
>
> **Answer:** DNS employs **Partitioning** and **Replication**.
> 1. **Partitioning (and Distribution):** The DNS namespace is **hierarchically partitioned** into zones (e.g., `.com`, `ac.uk`, `google.com`). Authority for each zone is **delegated** to different, independent name servers. This distributes the management and query load across countless servers worldwide, preventing any single server from becoming a bottleneck. A query for `www.example.ac.uk` is resolved by traversing this hierarchy, contacting different servers at each level.
> 2. **Replication:** Within each zone, multiple **replica name servers** are typically deployed. For example, the `google.com` zone is served by many geographically dispersed servers. This **replication** provides **fault tolerance** (if one fails, others can answer) and **improves performance** via load balancing and reduced latency for users near a replica. DNS clients and intermediate resolvers also **cache** (a form of replication) responses to reduce load on authoritative servers and speed up subsequent lookups.
## 2. Types of Distributed Systems Pt.2 - *Chapter 1*
There are three types of DS that serve distinct purposes.
### 1. High performance distributed computing systems (HPDCS)
These systems focus on delivering massive computational power for complex tasks like scientific simulations and large-scale data processing.
#### Distributed Shared Memory systems
Early HPDCS aimed to mimic the programming ease of multiprocessors (shared memory) across multiple independent computers.
- **Concept:** Create a single, virtual shared address space across physically separate machines using virtual-memory page-faulting techniques. If a process accesses a "page" located on another machine, the system fetches it transparently.
- **Outcome:** This model largely failed to compete with true multiprocessors on performance and did not meet programmer expectations, leading to its decline.
#### Cluster Computing
Cluster computing became popular by connecting many **off-the-shelf, commodity computers** with a high-speed network to create a cost-effective supercomputer.
- **Characteristics:** **Homogeneous** hardware/OS, controlled by a single **master node**, used primarily for **parallel programming** (running a single program split across many nodes).
- **Architecture:** A **master node** manages job scheduling, a batch queue, and user access. **Compute nodes** run the actual workloads. The master runs the coordinating **middleware**.  
![[cluster-computing-system-ex.png|400]]
- **Example:** **Linux-based Beowulf clusters**.
#### Grid Computing
Grid computing federates resources (computers, storage, special devices) from **different administrative domains** (e.g., universities, labs) into a **Virtual Organization (VO)**.
- **Goal:** Enable flexible, secure, coordinated resource sharing across institutional boundaries.
- **Challenge:** Managing heterogeneity and differing policies.
- **Architecture:** Built around a layered **Grid Middleware** that provides services for resource access, security, and management across sites. It evolved towards **service-oriented architectures** like the **Open Grid Services Architecture (OGSA)** based on web services.  
![[grid-computing-systems-proposed-arch-ex.png|200]]
#### Cloud Computing
Cloud computing is the logical evolution, outsourcing the entire infrastructure and providing resources as an on-demand, pay-per-use **utility**.

>**(1)** An IT infrastructure where computing resources are **virtualised** and accessed as a **service**.  
>**(2)** Characterised by an easily usable pool of **virtualised resources** that can be **dynamically reconfigured (elastic)** and accessed via a **pay-per-use** model with **Service Level Agreements (SLAs)**.

**(THE FOUR LAYERS OF A CLOUD)**  
![[the-organisation-of-clouds.png|400]]
1. **Hardware:** The physical data center (servers, routers, cooling) – invisible to users.
2. **Infrastructure:** Virtualised compute and storage resources (e.g., virtual machines). Provides **Infrastructure-as-a-Service (IaaS)**.
3. **Platform:** Higher-level abstractions, APIs, and services for developers to build and deploy applications. Provides **Platform-as-a-Service (PaaS)**.
4. **Application:** The actual software applications running in the cloud (e.g., Gmail, Office 365). Provides **Software-as-a-Service (SaaS)**.
**(OBSTACLES)** Include **vendor lock-in**, **security/privacy** concerns, and **dependency** on provider availability.
### 2. Distributed information systems
These systems focus on integrating applications and data across an enterprise, evolving from simple client-server models to complex, interoperable middleware.
**(CONTEXT)** Organisations had many independent, networked applications ("islands of automation") that needed to work together, leading to **Enterprise Application Integration (EAI)**. The goal was to achieve **interoperability** between different apps and their data.
#### Transactions
A fundamental concept for ensuring reliable operations across multiple components is the **transaction**.
> A sequence of operations (e.g., `READ`, `WRITE`, `BEGIN_TRANSACTION`) that must **all succeed or all fail** (the "all-or-nothing" property).
 
**ACID Properties:** Transactions guarantee:
- **Atomicity:** Indivisible execution.
- **Consistency:** Leaves the system in a valid state.
- **Isolation:** Concurrent transactions don't interfere.
- **Durability:** Committed changes are permanent.

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
**(SUB-TRANSACTIONS & NESTING)** Transactions can be nested to distribute work across machines. A top-level transaction can spawn parallel subtransactions. If a parent aborts, **all** its committed children must also be undone—**permanence** applies only to **top-level commits**. This provides a clean model for distributed work.
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire sys for it to manipulate_ as it wishes. 
- If it aborts, its private universe vanishes. 
- If it commits, its private universe replaces the parent’s universe. 
- Thus, if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.

**(TRANSACTION PROCESSING (TP) MONITORS)** When transaction data is distributed across servers, a **TP Monitor** is the middleware component that coordinates the execution. It ensures the ACID properties are maintained using protocols (like two-phase commit) across all participating servers.  
![[tpm-in-a-ds.png|400]]
- **Role:** The TP monitor relieves application developers from implementing complex coordination logic.

**(MIDDLEWARE FOR EAI)** Several middleware communication paradigms facilitate integration:  
![[middleware-in-eai.png|300]]
- **Remote Procedure Call (RPC) / Remote Method Invocation (RMI):** Allow a component to call a *procedure*/*method* on a remote component as if it were local. Leads to **tight coupling** (both parties must be running and know each other's exact location). Procedure calls to remote servers that are encapsulated in a transaction, are **transactional RPCs**.
- **Message-Oriented Middleware (MOM) / Publish-Subscribe:** Provides **loose coupling**. Producers **publish** messages to a logical topic/queue. Consumers **subscribe** to topics of interest. The middleware ensures delivery. This decouples components in time and space.
#### Coupling in Distributed Systems: Space vXs. Time

| Coupling Dimension                                | Description                                                                                                                                               | System Characteristic                                                                                                       | Example                                                                                                                                                                                                          | Typical Architecture/Pattern                                            |
| :------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------- |
| **Space (Referential) Coupling**<br><br>**Tight** | Components must have **explicit knowledge of each other's identity/location** (e.g., IP address, object reference) to communicate.                        | Components are **directly dependent** on each other's specific interfaces and network addresses. Changes require updates.   | **Client-Server RPC/RMI:** A client must know the server's exact endpoint. **Direct socket communication.**                                                                                                      | Traditional 2/3-Tier, RPC, RMI                                          |
| **Space (Referential) Coupling**<br><br>**Loose** | Components communicate **indirectly** and need **no explicit knowledge** of each other's identity. They refer to a logical identifier, topic, or channel. | Components are **independent**. The system can add, remove, or replace components without disrupting others.                | **Publish/Subscribe:** A publisher sends a message to a "news" topic; subscribers listen to that topic without knowing the publisher. **Message Queues:** A producer sends to a queue; a consumer takes from it. | Message-Oriented Middleware (MOM), Event-Driven, Shared Data Space      |
| **Time Coupling**<br><br>**Tight**                | The sender and receiver **must both be active and available at the same time** for communication to succeed. Communication is **synchronous**.            | Communication is **blocking**. System is sensitive to latency and failures of either party.                                 | **Phone Call, HTTP Request/Response, Synchronous RPC:** The caller blocks and waits for an immediate reply.                                                                                                      | Request-Reply, Synchronous Client-Server                                |
| **Time Coupling**<br><br>**Loose**                | The sender and receiver **do not need to be active simultaneously**. Messages can be **intermediated and stored**.                                        | Communication is **asynchronous** and **non-blocking**. Systems are more resilient to temporary failures and variable load. | **Email, Message Queue, Persistent Pub/Sub:** A user sends an email; the recipient reads it hours later.                                                                                                         | Message Queuing, Asynchronous Messaging, Store-and-Forward (e.g., DTNs) |
**(INTERSECTION)** of these two dimensions creates four classic coordination models:

|                     | Temporally Tight                                                                                                                                                                                                    | Temporally Loose                                                                                                                                                                                                                                                        |
| :------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Spatially Tight** | **1. Direct Communication**<br>• **Example:** Phone call, synchronous RPC.<br>• **Model:** Tightest coupling. Common in basic client-server architectures.                                                          | **2. Mailbox Coordination**<br>• **Example:** Email, traditional MPI send/receive to a known process.<br>• **Model:** Referentially coupled (need the mailbox ID), but decoupled in time.                                                                               |
| **Spatially Loose** | **3. Event-Based Coordination**<br>• **Example:** **Publish-Subscribe** with immediate delivery (subscriber must be running).<br>• **Model:** **Referentially decoupled** (use topics), but **temporally coupled**. | **4. Shared Data Space**<br>• **Example:** **Tuple spaces** (e.g., Linda), distributed blackboards, some persistent pub/sub.<br>• **Model:** **Fully decoupled.** Processes communicate by reading/writing to a shared, persistent space without knowing of each other. |
**(TRADE-OFFS)**
* **Tight Coupling (Space & Time):** Leads to systems that are often **simpler to reason about** but are **less flexible, scalable, and fault-tolerant**. They have clear dependencies but single points of failure.
* **Loose Coupling (Space & Time):** Leads to systems that are **more scalable, resilient, and flexible** (components can evolve independently) but are **more complex to design, debug, and ensure consistency** in. This is the dominant trend for large-scale, modern distributed systems (microservices, event-driven architectures, serverless).

The evolution of DS design often involves moving from the **top-left quadrant (Direct Communication)** -> **bottom-right quadrant (Shared Data Space)** to achieve greater scalability and resilience, accepting the complexity that comes with loose coupling.
### 3. Distributed Systems for Pervasive computing (DPS)
These systems are characterized by small, mobile, and embedded devices that blend naturally into the user's environment, often interacting implicitly. The **Internet of Things (IoT)** is a key part of this domain.
#### Ubiquitous Computing Systems (UCS)
Systems that are continuously present and interact unobtrusively with users.
**Core Requirements:**
1. **Distribution:** Networked, accessible devices.
2. **Interaction:** Highly unobtrusive, often **implicit** (the system infers intent from user actions, like adjusting a car seat).
3. **Context Awareness:** The system is aware of the user's situation (**where, who, when, what**) using sensor data (e.g., mapping GPS to a shop location).
4. **Autonomy:** Self-managed with minimal human intervention (e.g., auto IP address allocation via DHCP, device discovery via UPnP, automatic software updates).
5. **Intelligence:** Uses AI techniques to handle dynamic, unpredictable environments and incomplete input.
#### Mobile computing systems
A subset of DPS where devices change location and network attachment (mobile).
- **Key Challenges:** **Dynamic Location**, **Service Discovery**, **Intermittent Connectivity**.
**Networks:**
- **MANETs (Mobile Ad-hoc Networks):** Self-forming wireless networks without infrastructure; require dynamic routing protocols.
- **DTNs (Disruption-Tolerant Networks):** Assume no continuous path; use **store-carry-forward** routing, where messages are physically carried by mobile nodes until they can be forwarded.  
![[passing-messages-disruption-tolerant-network.png|300]]
- **Example Protocol:** **Mobile IP** allows a device to keep a permanent "home address" while using a temporary "care-of address" when roaming.

**(ROUTING STRATEGIES)**
* **Flooding-Based (e.g., Epidemic Routing):** Messages spread redundantly. High delivery chance but high resource cost.
* **Selective Forwarding:** More efficient. Decisions based on:
  * **Utility/Probability:** Forward to nodes with higher historical probability of contacting the destination.
  * **Social Patterns:** Forward to nodes belonging to the same "community" (**social-based routing**).
  * **Connectivity:** Forward to **well-connected nodes** (central hubs in the mobility pattern).
* **Technical Example:** A wildlife tracker on a zebra (Node A) needs to send data to the ranger station (Node D). There's no direct path. The zebra moves and encounters a vulture's tracker (Node B). Using a **utility-based** protocol, Node A forwards the data to Node B. The vulture flies and later lands near a fixed sensor node (Node C) connected to the station, finally delivering the data. The network tolerated long delays and disconnections.
#### Sensor (and actuator) networks
Massive networks of small, resource-constrained devices for collaborative sensing and actuation. Typically consisting of thousands of relatively small nodes.
- **Key Characteristics:** Severe **resource constraints** (battery, CPU, bandwidth), **application-specific design**, and **collaborative in-network processing** to save energy.
- **Wireless Communication & Ad-hoc Deployment.**
- **Programming Models:** Include **database abstractions** (e.g., TinyDB, where queries are aggregated up a tree) and **region-based programming** (nodes collaborate with neighbours).
- **Architecture Shift:** Represents a move from centralized cloud processing to **edge intelligence**, where data is processed within the network itself.  
![[cloud-edge-continuum.png|400]]
**(DATABASE VIEW & DATA ACCESS)** As another related example, consider a sensor network as implementing a distributed db. This view is common as many networks are deployed for measurement and surveillance, where an operator extracts information by issuing queries.

**(DATA PROCESSING ARCHITECTURES)** To organise a sensor network as a distributed db, there are essentially two extremes:
1.  **Centralised Processing:** Sensors send all raw data to a central operator's site. This wastes network resources and energy.
2.  **Purely Edge Processing:** Queries are sent to sensors, each computes an answer, and the operator aggregates all responses. This wastes the opportunity for sensors to aggregate data locally, reducing the volume of traffic.

**(GOOGLE SPANNER - A CASE STUDY IN GLOBAL CONSISTENCY)**  
Spanner is a globally distributed database that uniquely combines **SQL, ACID transactions, and horizontal scalability**.

- **Core Innovation:** **TrueTime**, a globally synchronized clock API using atomic clocks/GPS, which enables **external consistency**.
- **Consistency Model:** Provides **Strict Serializability** (transactions appear serialized) and **External Consistency** (the serial order respects real-time observed commits).
- **ACID Implementation:** Uses **synchronous Paxos-based replication** for durability and **Multi-Version Concurrency Control (MVCC)** with TrueTime timestamps for isolation.
- **Trade-off:** Also offers a **Repeatable Read** isolation level for performance, which is susceptible to **write skew** and requires careful application design.

**Check of Understanding**
> **Question:** Compare and contrast **Cluster Computing**, **Grid Computing**, and **Cloud Computing** across three dimensions: (1) Primary ownership/control of resources, (2) Degree of hardware/software homogeneity, and (3) The main economic model for users. Provide a real-world use case that would be a _poor_ fit for Grid Computing but a _good_ fit for Cloud Computing, and explain why. **Answer:**

| Dimension             | Cluster Computing                                             | Grid Computing                                               | Cloud Computing                                                              |
| --------------------- | ------------------------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------------------------- |
| **Ownership/Control** | Single organization (e.g., a university lab).                 | Multiple independent organizations (a Virtual Organization). | Third-party provider (e.g., AWS, Google).                                    |
| **Homogeneity**       | High. Uses identical, commodity hardware and software.        | Low. Heterogeneous resources from different admin domains.   | High (within a provider's data centers). Hidden from user by virtualization. |
| **Economic Model**    | Capital expenditure (buying the cluster). Often grant-funded. | Resource sharing/bartering within the VO.                    | Operational expenditure (pay-per-use utility).                               |
**Use Case:** A startup developing a new mobile app that needs to handle unpredictable, rapid growth in user traffic.
> - **Poor for Grid:** A Grid is ill-suited because the startup has no established partnerships in a research VO to "share" resources. Grids are designed for coordinated, pre-planned resource sharing among trusted partners, not for dynamic, commercial, on-demand scaling.
> - **Good for Cloud:** The Cloud is perfect because the startup can use **IaaS/PaaS** to instantly provision resources, scale elastically with user demand, and only pay for what they use. This aligns with the **pay-per-use** model and removes the need for large capital investment or managing physical hardware.
## 3.  Architectures - *Chapter 2*
### Understand the different ways on how to view the org of a DS
An architecture defines the sys through **components** (replaceable units with interfaces), their **connectors**, the **data** they exchange, and how they are **configured** into a whole.
![[machine-interfaces.png\|300]]
### Architectural styles
#### Layered
Components are organised in hierarchical layers. A component in layer `Lj` can make a **downcall** to a lower layer `Li` (`i<j`) and expects a response. Upcalls to higher layers are rare.
![[layered-architectures.png\|400]]
**(LAYERED COMMUNICATION PROTOCOL)** A classic example is a protocol stack (e.g., TCP/IP). Each layer offers a **service** through a specific **interface** and implements that service using a **protocol** (rules for data exchange). For instance, **TCP** offers a reliable, connection-oriented service via a `socket` interface, governed by a protocol managing connections and data ordering.
![[layered-communication-protocol-ex.png\|300]]
**(APPLICATION LAYERING: THE PAD MODEL)** Many client-server apps use three logical layers:
1.  **Presentation:** User interface.
2.  **Application:** Core processing logic.
3.  **Data:** Persistent data management.

| PAD Search Engine                  | Cloud Example                 |
| ---------------------------------- | ----------------------------- |
| ![[pad-search-engine-ex.png\|300]] | ![[cloud-search-ex.png\|300]] |
*Example:* In a cloud search engine, a user's query (Presentation) is routed by a load balancer to an app server (Application), which processes it by querying an index (Data).
#### Object-based
The sys is structured as a collection of loosely coupled **objects** (or **components**), where each object encapsulates its state (data) and behaviour (methods). Interaction occurs via **Remote Method Invocation (RMI)**.
Objects are accessed through interfaces, hiding their implementation. In a distributed object, the interface can be placed on a client machine via a **proxy** (client stub), while the object itself resides on a server. The proxy handles communication (marshalling/unmarshalling), forwarding requests to the server-side **skeleton**.
![[object-based-arch-remote-obj-client-side-proxy.png\|400]]
*Example:* A Java RMI app where a client-side proxy object forwards method calls to the actual object on the server.
This style, promoting clear encapsulation, forms the foundation for **Service-Oriented Architectures (SOA)**, where systems are built by composing independent, often remotely hosted, services (e.g., an online shop using a third-party payment service).
#### Resource-centred
This style, exemplified by **REST (Representational State Transfer)**, models a sys as a collection of uniquely named **resources**. All services offer the same, minimal interface and interactions are **stateless**.
* **Key Principles:**
    1.  Resources have unique identifiers (e.g., URIs).
    2.  Uniform interface (typically **CRUD** operations).
    3.  Messages are self-descriptive (e.g., HTTP headers).
    4.  Stateless execution (server forgets client after request).
* **CRUD Operations:** `POST` (Create), `GET` (Retrieve), `PUT` (Update), `DELETE`.
* *Example:* **Amazon S3**. A file (object) in a bucket (directory) is accessed via a URI (`http://BucketName.s3.amazonaws.com/ObjectName`). Creating the object uses `PUT` on that URI; listing bucket contents uses `GET`.

#### Event-based (Publish-Subscribe)
Components interact by generating and reacting to events, leading to **loose coupling** between producers (publishers) and consumers (subscribers).
Coordination models vary based on temporal and referential coupling:
![[event-based-vs-shared-data-space-arch-ex.png\|400]]
* **Event-Based Coordination:** Subscribers express interest in event types (**topics** or **content**). When a publisher generates an event, the middleware (**event bus**) delivers it to all matching subscribers. Processes are **referentially decoupled** (don't know each other) but often **temporally coupled** (subscriber must be running).
* **Shared Data Space:** Processes communicate by writing and reading **tuples** (structured data records) to/from a shared, associative store. This is both **referentially and temporally decoupled**. A process can subscribe to tuples matching a pattern and be notified when one is published.
* *Example:* A sensor network where a motion sensor (publisher) emits an "occupancy" event. A security service (subscriber), which registered for this event, receives the notification and checks if the door lock status (another event) is "unlocked" to trigger an alert.
### System architecture
#### Centralised
Based on the **client-server model**, where **servers** (providers) and **clients** (users) of services are distinct roles, communicating via a request/reply model.
![[centralised-org-arch-ex.png\|150]]

| 2-Tier                         | 3-Tier                       |
| ------------------------------ | ---------------------------- |
| ![[2-tiered-arch-ex.png\|400]] | ![[3-tier-arch-ex.png\|250]] |
* **2-Tiered:** Direct client-to-server communication. The client may handle presentation and some app logic (fat client) or be very thin.
* **3-Tiered:** A more scalable architecture with distinct presentation, app logic, and data tiers. Servers in one tier can act as clients to the next (e.g., an app server requesting data from a db server).
#### Decentralised (Peer-to-Peer - P2P)
All processes (**peers**) are equal, acting as both client and server (**servant**). Functions are distributed across all peers.
* **Unstructured P2P:** Peers connect in an ad-hoc manner, forming a random overlay network. Data location is non-deterministic.
  * **Search by Flooding:** A query is sent to all neighbours, who propagate it further (limited by a TTL). High network cost but fast.
  * **Search by Random Walk:** A query is forwarded to one randomly chosen neighbour at a time. Lower cost but slower; multiple parallel walks speed it up.
* **Structured P2P:** Peers organise into a specific, deterministic overlay (e.g., a ring, tree, or hypercube). Data items are assigned unique **keys** via hashing and stored in a **Distributed Hash Table (DHT)**. Lookups are routed efficiently through the overlay to the peer responsible for that key.
![[p2p-arch-as-4d-hyper-cube-ex.png\|300]]
* *Example (Structured P2P):* In the 4D hypercube above, to find data with key `14` (`1110` in binary), node `0111` would route the request through connected neighbours, following the overlay protocol, until it reaches the node responsible for key `1110`.
#### Hybrid
Combines elements of centralised and decentralised architectures.
A prime example is the **edge-server sys**. Servers are placed at the network's **edge** (e.g., at an Internet Service Provider). An origin server in a central data centre replicates content to these edge servers. End-users connect to a nearby edge server for low-latency access, while the sys as a whole is managed centrally.
![[internet-as-edge-servers-ex.png\|400]]
This model extends to **fog/edge computing**, where computation and storage are distributed between the central cloud, edge servers, and even end-user devices for ultra-responsive services.
## 4. Communication - *Chapter 4*
At its core, communication in a DS involves **processes on different machines exchanging information** via **low-level message passing** over a network. Higher-level models (like RPC) are abstractions built on this fundamental mechanism.
### Latency and Bandwidth
Two critical performance metrics for any communication channel.
**(LATENCY)** The **delay** or time it takes for a single message (or the first bit of data) to travel from source to destination. Limited by the speed of light and processing delays in network hardware.
* *Example:* Datacenter (1ms) vs. Intercontinental (100ms) vs. "Sneakernet" in a van (1 day).

**(BANDWIDTH)** The **capacity** or volume of data that can be transmitted per unit of time.
Example:* 5G (~5-20Gbps) vs. Broadband (~300Mbps) vs. Van of Hard Drives (~1 Gbps effective).

**Key Relationship:** The total time to transfer a message is the sum of the **latency** and the time to push all the data through the pipe, which is the message size divided by the **bandwidth**.

**Check of Understanding**
>**Question:** How long does it take to send a 10 MB (80 Mbit) file over a link with 100 ms latency and 1 Gbps (1000 Mbit/s) bandwidth?
>**Answer:** Time = Latency + (Size/Bandwidth) = $0.1s + {80 M\text{bit} \over 1000 M\text{bit}/s} = 0.1s + 0.08s = 0.18 \text{seconds}.$ Latency is the dominant factor. The bandwidth term would dominate for a 10 GB file.
### Layered Protocols
Communication is organised into layers, each providing a service to the layer above and using the service of the layer below. This modularity hides complexity.

The classic **OSI model** has seven layers, but practical systems like **TCP/IP** use fewer.
![[layered-protocols.png\|300]]

**(MIDDLEWARE PROTOCOLS)** This layered model is extended in DSs by **Middleware**, which sits between the OS/transport layers and the app. It provides common services like secure communication, data marshalling, and naming, freeing the app developer from re-implementing them.

| Protocols                          | Adapted Layering Scheme                          |
| ---------------------------------- | ------------------------------------------------ |
| ![[middleware-protocols.png\|300]] | ![[middleware-adapted-layering-scheme.png\|300]] |
### Types of Communication
Communication can be characterised along two key dimensions:
**(SYNCHRONOUS vs. ASYNCHRONOUS)** This concerns the **timing and blocking behaviour** of the sender and receiver.
* **Synchronous:** The sender blocks and waits until the receiver is ready to accept the message (or a reply is received).
* **Asynchronous:** The sender continues execution immediately after issuing the send; the message is stored (buffered) until the receiver can process it.
![[communication-sync-ex.png\|300]]
**(TRANSIENT vs. PERSISTENT)** This concerns the **lifetime and storage** of the message in the communication sys.
* **Transient:** A message is only stored *en route* while travelling to the receiver. If it cannot be delivered immediately (e.g., the receiver is down), it is **discarded**.
* **Persistent:** A message is stored in a **queue** (e.g., at a message server) until it can be successfully delivered to the receiver, even if the receiver is temporarily unavailable.

**Common Combinations:**
* **Client-Server / RPC:** Typically uses **transient, synchronous** communication (request-reply). Both parties must be active.
* **Message-Oriented Middleware:** Typically uses **persistent, asynchronous** communication. Provides decoupling and fault tolerance.

**Check of Understanding**
> **Question:** What is a major drawback of the transient, synchronous model used in classic client-server systems?
> **Answer:** It requires **both client and server to be running simultaneously**. If the server crashes after the client sends a request, the client may be left hanging (blocked) indefinitely, leading to poor reliability and scalability.
### Remote Procedure Call (RPC)
RPC is an abstraction that makes a remote function call appear like a **local procedure call** to the programmer, hiding all network complexity.

**(BASIC OPERATION)** A **client stub** on the caller's machine masquerades as the actual server procedure. It **marshals** parameters into a network message, sends it, waits for the reply, **unmarshals** the result, and returns it.
![[rpc-operation-ex.png\|400]]
```java
// `processPayment` executes on a remote server.
Result result = paymentsService.processPayment(cardDetails, amount);
```
**(PARAMETER PASSING & MARSHALLING)** A core challenge is transforming data (parameters, results) from a machine-specific in-memory format into a neutral network format. This involves:
* **Byte Order (Endianness):** Converting between big-endian and little-endian integer representations.
* **Data Representation:** Agreeing on formats for floats, strings (ASCII/EBCDIC), and complex structures.
* **Serialisation:** Flattening an object's state into a byte stream.
![[parameter-passing.png\|200]]

**(ASYMPTOTOIC RPC)** To avoid clients blocking, RPC can be made asynchronous. The client sends a request and continues, later collecting the result via a separate call or a callback.
![[async-rpc-ex.png\|300]]

**(REMOTE METHOD INVOCATION - RMI)** RMI is the object-oriented extension of RPC, allowing invocation of methods on remote objects. It uses client-side **proxy objects** that represent the remote object.
![[rmi-ex.png\|400]]

**Check of Understanding**
> **Question:** What is the key technological difference between early RPC systems (e.g., SunRPC) and modern ones like gRPC?
> **Answer:** While the core concept is the same, modern RPC frameworks like **gRPC** typically use **HTTP/2** as the transport for multiplexing and efficiency, and **Protocol Buffers** (a binary, language-neutral interface definition language) for highly efficient, version-tolerant marshalling/serialisation, as opposed to older textual or more verbose formats like XML.
### Message Oriented Middleware
MOM provides **persistent, asynchronous communication** via messages, leading to loosely coupled, reliable systems.

**(MESSAGE-QUEUING MODEL)** The core abstraction is a **queue**. Senders place messages into a queue, and receivers take messages from it. Queues decouple the communicating processes in time (asynchronous) and space (they don't need to know each other's address).

| MPI Message Queue Model               | Message Queue System                    |
| ------------------------------------- | --------------------------------------- |
| ![[mpi-message-queue-model.png\|200]] | ![[message-queuing-system-ex.png\|200]] |
**(THE MESSAGE-PASSING INTERFACE - MPI)** MPI is a standard API for **transient messaging** primarily used in High-Performance Computing (HPC). It assumes a known, running group of processes.
*   It provides a rich set of primitives for point-to-point (`MPI_Send`, `MPI_Recv`) and collective communication (broadcast, scatter/gather).

| Advanced Transient Messaging                  | Example Primitives                                                                                                                                                                                                                                                                 |
| --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![[advanced-transient-messaging-ex.png\|200]] | `MPI_Send(buf, count, type, dest, tag, comm)`: <br>Sends a message.<br><br>`MPI_Recv(buf, count, type, source, tag, comm, status)`: <br>Receives a message (blocks).<br><br>`MPI_Isend()` / `MPI_Irecv()`: <br>Non-blocking versions for overlap of communication and computation. |
**(ADVANCED MESSAGE QUEUING PROTOCOL - AMQP)** AMQP is an open standard app-layer protocol for MOM, defining how messages are formatted and transmitted between systems (e.g., between a publisher and a queue, or a queue and a consumer).
* *Example systems:* **RabbitMQ** (a widely-used AMQP broker), **Apache ActiveMQ**. They are used for enterprise integration, task distribution, and in cloud platforms.
* *Use Case - OpenStack:* Uses a message queue (like RabbitMQ) as the central nervous sys. All service requests (e.g., "launch a VM") are placed on queues, and worker processes listen to these queues and execute the tasks, enabling a scalable, decoupled control plane.

**Check of Understanding**
> **Question:** A financial trading sys needs to ensure that a "sell order" message is never lost, even if the trading engine process crashes momentarily. Which communication paradigm and characteristic (transient/persistent) is essential here?
> **Answer:** This requires **Message-Oriented Middleware** with **persistent communication**. The "sell order" message would be placed into a durable queue. The trading engine, upon restarting, would retrieve the message from the queue and process it, guaranteeing "at-least-once" delivery.
## 5. Service Oriented Architectures
### Conceptual Design of Software systems
At a conceptual level, software systems are broken into layers, leading to tiered architectures.
![[1-2-3-n-tier-architectures-ex.png\|700]]
### Architectures
**(1-Tier)** All components (presentation, logic, data) are tightly integrated into a single, monolithic app and deployment unit. Example: A standalone desktop app with its own embedded db.

**(2-Tier)** The classic client-server model. A **client** (handling presentation and some logic) communicates directly with a **server** (typically a db). Example: A desktop app that connects to a central db server.

**(3-Tier)** Introduces a dedicated middle tier, separating concerns:
* **Presentation Tier:** User interface.
* **Application/Logic Tier:** Core business rules and processes.
* **Data Tier:** Persistent data storage.
This provides better scalability, security (the db is not directly exposed), and maintenance. Example: A web app (Presentation) using an app server (Logic) to query a db (Data).
#### N-Tier
A generalisation of the 3-tier model, where any tier can be subdivided or services can be integrated, creating a more complex, distributed architecture. This often arises from:
1.  Integrating full legacy systems (which are themselves 2 or 3-tier) as resources.
2.  Adding dedicated web or API gateway servers as an extra presentation tier.
**Challenge:** N-tier architectures face significant complexity in **integration** due to a lack of universal standards, requiring extensive custom middleware to connect disparate systems.
### Emergence of SOAs
SOA emerged as a direct response to the challenges of N-tier and enterprise app integration. The trend was toward integrating complex, decoupled systems, often from different orgs. SOA facilitates this by treating app **functionality as reusable, loosely-coupled services**.
### Vision
SOA is an architectural style for building apps from **independent services** that communicate via messages. It represents a paradigm shift from tightly-coupled object-oriented systems to **message-oriented**, platform-independent integration. ARCH MODEL:
Top-bottom: Services/APIs, App, App server, Managed Runtime Env, OS, Hypervisor, Storage, Network, Hardware.
The SOA vision is built on integrating **Resources** (data, compute), **Communities** (procedures), and **Technologies** through standardised **Services** and **Connectivity**.
#### What is a ‘Service’?
> A *reusable*, *self-contained* *software component* that implements a *discrete business function* (e.g., "get user data," "process payment"). It is accessed strictly through its defined, message-based interface. Examples include weather forecasts, credit checks, or translation engines.
#### Architecture Model
The SOA model defines three core roles and three core operations:
![[soa-architecture-ex.png\|400]]
* **Roles:** Service Provider (hosts), Service Registry (finds), Service Consumer (uses).
* **Operations:** Publish (provider lists service), Find (consumer locates service), Bind (consumer connects to and uses the provider).
#### Service Provision and Consumption
![[service-provision-and-consumption.png\|400]]
This illustrates the dynamic interaction: a service is implemented and published by a provider. A consumer discovers it via the registry and subsequently binds to the provider to execute it.
#### Differences between SOAs and traditional n-tier architectures
1.  **Decentralised Middleware:** In SOA, communication logic (middleware) is embedded within each service's environment, not centralised in a single app server tier.
2.  **Strong Emphasis on Loose Coupling:** Services have minimal dependencies on each other's internal implementation, communicating via standardised messages. This contrasts with the tighter integration often found in N-tier layers.
3.  **Adherence to Well-Supported Standards:** SOA relies heavily on global, platform-neutral standards (e.g., XML, SOAP, WSDL) to enable interoperability across orgal boundaries, unlike the often proprietary or bespoke interfaces in N-tier systems.

**Check of Understanding**
> **Question:** A company has a monolithic 1-tier app for customer orders. They want to expose the "check inventory" and "process payment" functions to new mobile and web apps, and also to partner companies. Explain why moving to an SOA style would be more effective than simply creating a 3-tier version of their existing sys.
> **Answer:** A 3-tier architecture would primarily separate logic internally for scalability but would likely still result in a single, integrated app with a single point of access. SOA, by contrast, would decompose "check inventory" and "process payment" into **independent, reusable services**. These services could then be:
> *  **Discovered and consumed separately** by the new mobile app, web app, and partner systems.
> * **Scaled independently** based on demand.
> * **Updated or replaced** without affecting the entire order sys, thanks to **loose coupling** via standardised interfaces. This provides the agility and openness needed for multi-channel and inter-organisational integration that a standard 3-tier refactor would not.
## 6. Web Services and REST
### Why do we need Web services?
The evolution of distributed computing has moved from linking machines (Internet/TCP/IP) to linking documents (WWW/HTTP/HTML) to linking **applications**. Web services fulfill this third stage, enabling programmatic, reliable interaction between disparate apps over networks, moving beyond unreliable "screen scraping" of websites. They support industry needs for standardised data exchange using formats like **XML** and **JSON**.
### What are web services?
A **Web Service** is a unit of business logic accessible over a network (Internet/intranet) using standard protocols (HTTP, SMTP) and data formats (XML, JSON). It provides a **coarse-grained** way to expose functionality from existing platforms (JEE, .NET) within a **Service-Oriented Architecture (SOA)**, promoting large-scale, **loosely-coupled** systems without mandating specific implementation technologies.

**(SOA ROLES & EXAMPLE)** In an SOA with web services, there are three core roles: the **Provider** (publishes the service), the **Broker** (registry/directory like a "Yellow Pages"), and the **Requestor** (finds and uses the service). This enables dynamic discovery and integration.
![[soa-web-services.png\|400]]
*Example:* An airline's booking sys (on Oracle Solaris) and a car rental website (on Linux) can be integrated via web services, allowing seamless combined bookings despite different underlying hardware and software.

**(KEY FEATURES)**
* **Standardised Data Packaging:** Uses XML/JSON over standard transports (HTTP, SMTP).
* **Interoperability Abstraction:** Provides a web-friendly layer over traditional RPC/RMI.
* **Self-Describing:** Described using languages like WSDL.
* **Discoverable:** Published to and discoverable via service registries.
### What is REST?
**REST (Representational State Transfer)** is an **architectural style** (not a standard) for building DSs, particularly web services. It views a sys as a collection of **resources** (each identified by a **URI**) and uses a **uniform interface** (HTTP verbs) to manipulate them. It is the predominant style for modern web APIs.
### What does it consist of?
**(RESOURCE-BASED ARCHITECTURE)** The core REST principles are:
1.  **Resources identified by URIs:** Every piece of data is a resource with a unique address (e.g., `/parts/00345`).
2.  **Uniform Interface:** A constrained set of well-defined operations using HTTP verbs: **POST** (Create), **GET** (Retrieve), **PUT** (Update), **DELETE** (Delete).
3.  **Self-Descriptive Messages:** Requests and responses contain all information needed to be understood (via HTTP headers and body).
4.  **Stateless Interactions:** The server does not retain client context between requests. Each request contains all necessary information.
![[rest-triangle.png\|200]]
**(REPRESENTATIONAL STATE TRANSFER)** The name derives from how clients interact with the sys: a client retrieves a **representation** (e.g., HTML, JSON, XML) of a resource, which places the client in an app state. By following hyperlinks (URIs) within that representation, the client initiates a new request, transferring to a new state.
![[rest-boeing-ex.png\|200]]
**(PRACTICAL EXAMPLE)** Consider a parts depot web service.
* **GET list of parts:** `GET http://www.parts-depot.com/parts` returns an XML list with hyperlinks to each part.
* **GET part details:** `GET http://www.parts-depot.com/parts/00345` returns detailed XML for part 00345, which may contain further links (e.g., to its specification).
* **Submit a Purchase Order (PO):** `POST http://www.parts-depot.com/orders` with an XML PO document in the request body creates a new order.

| REST PO example |     |
| --------------- | --- |
|                 |     |

![[rest-purchase-order-ex.png\|300]]
![[web-service-pruchase-order.png\|200]]
#### Claimed benefits
Proponents of REST highlight several advantages:
* **Improved Performance:** Native support for HTTP caching improves response times.
* **Scalability:** Statelessness allows requests to be handled by any server, simplifying scaling.
* **Simplicity & Interoperability:** Relies on well-understood, ubiquitous web standards (HTTP, URI), reducing vendor lock-in.
* **Discoverability:** Hyperlinks embedded in representations guide clients through the API, acting as a built-in discovery mechanism.
### HTTP
HTTP is the primary protocol for RESTful interactions. Understanding its mechanics is crucial.

**(ANATOMY OF HTTP REQUEST)** A client sends a request with:
* **Request Line:** `<VERB> <URI> HTTP/<version>` (e.g., `GET /parts/00345 HTTP/1.1`).
* **Headers:** Metadata (e.g., `Content-Type: app/json`, `Authorisation: Bearer ...`).
* **Body (optional):** The payload data (e.g., JSON or XML for POST/PUT).
![[anatomy-of-http-req.png\|200]]
![[get-post-req-ex.png\|400]]
**(ANATOMY OF HTTP RESPONSE)** The server replies with:
* **Status Line:** `HTTP/<version> <Status Code> <Reason>` (e.g., `HTTP/1.1 200 OK`).
* **Headers:** Metadata about the response.
* **Body (optional):** The requested representation (e.g., HTML, JSON).
![[anatomy-of-http-resp.png\|200]]
**(HTTP STATUS CODES)** Key categories include:
* **2xx Success:** `200 OK` (request succeeded), `201 Created` (resource created).
* **3xx Redirection:** `301 Moved Permanently`.
* **4xx Client Error:** `400 Bad Request`, `401 Unauthorised`, `404 Not Found`.
* **5xx Server Error:** `500 Internal Server Error`.
**(ADDITIONAL HTTP VERBS)** Beyond CRUD, HTTP defines other verbs like `HEAD` (get headers only), `PATCH` (partial update), and `OPTIONS` (discover allowed operations).

**Check of Understanding**
> **Question:** You are designing a web API for a university's course registration sys. You need operations to: a) retrieve a list of all courses for a term, b) allow a student to enroll in a specific course, and c) allow an admin to update a course's maximum capacity. Design the RESTful endpoints (URIs and HTTP methods) for these operations, justifying your choices.
> **Answer:**
> a) **Retrieve course list:** `GET /terms/{termId}/courses`
> * Justification: `GET` is for safe retrieval. The URI is hierarchical, listing courses as sub-resources of a specific term.
> b) **Student enrollment:** `POST /courses/{courseId}/enrollments`
> * Justification: `POST` is for creating a new subordinate resource. The enrollment is created as a new sub-resource under the specific course. The student's ID would be in the request body.
> c) **Update course capacity:** `PUT /courses/{courseId}`
> * Justification: `PUT` is for complete replacement/update of a resource. The request body would contain the full or partial updated course representation, including the new `maxCapacity` field. Alternatively, `PATCH` could be used for a partial update.
## 7. Programming RESTful Web Services
**(REST: Quick Recap)**
* **Architectural Style:** Built on web standards (HTTP, URI).
* **Core Concepts:** Everything is a **resource** identified by a **URI**. Resources are manipulated via a **uniform interface** of HTTP methods (GET, POST, PUT, DELETE).
* **Client-Server:** REST client accesses/modifies resources on a REST server.
* **Multiple Representations:** A single resource can have different representations (JSON, XML, text), requested via HTTP content negotiation.
* **Stateless:** Each request contains all necessary context.

**(REST APIs: Examples)**
Major platforms provide RESTful APIs for programmatic access:
* **Google APIs:** Custom Search, Maps, YouTube (e.g., `GET https://www.googleapis.com/customsearch/v1?key=API_KEY&q=query`).
* **X (Twitter) API:** For reading and writing tweet data.
* **Amazon Web Services (AWS):** Many services like Amazon S3 expose RESTful endpoints.
* **OpenAI API:** For accessing AI models like GPT via HTTP requests.
### Reference implementations:
#### Python (Flask Restful)
**Flask-RESTful** is a lightweight Python extension for Flask that simplifies building REST APIs.
**(BASIC EXAMPLE)** A minimal API that returns JSON.
```python
from flask import Flask
from flask_restful import Resource, Api
app = Flask(__name__)
api = Api(app)
class HelloWorld(Resource):
    def get(self):
        return {'hello': 'world'}
api.add_resource(HelloWorld, '/')
if __name__ == '__main__':
    app.run(debug=True)
```
Running this (`python api.py`) and calling `curl http://127.0.0.1:5000/` returns `{"hello": "world"}`.
**(CRUD RESOURCE EXAMPLE)** A simple in-memory "todo" API demonstrating HTTP methods.
```python
from flask_restful import Resource, reqparse
todos = {}
class TodoSimple(Resource):
    def get(self, todo_id):
        return {todo_id: todos[todo_id]} # GET to retrieve
    def put(self, todo_id):
        todos[todo_id] = request.form['data'] # PUT to create/update
        return {todo_id: todos[todo_id]}
api.add_resource(TodoSimple, '/<string:todo_id>')
```
* **`PUT /todo1 -d "data=Remember the milk"`** creates/updates the item.
* **`GET /todo1`** retrieves it.
#### Java (Jersey)
**Jersey** is the reference implementation for **JAX-RS (Java API for RESTful Web Services)**, a Java specification. It uses annotations to map Java classes to REST resources.
**(JAX-RS ANNOTATIONS)** Key annotations for defining REST endpoints:

| Annotation | Description |
| :--- | :--- |
| `@Path("/path")` | Defines the URI path for the resource class or method. |
| `@GET`, `@POST`, `@PUT`, `@DELETE` | Specifies the HTTP method the method responds to. |
| `@Produces(MediaType.TEXT_PLAIN)` | Declares the MIME type(s) the method returns (e.g., `"application/json"`). |
| `@Consumes(MediaType.APPLICATION_JSON)` | Declares the MIME type(s) the method accepts in the request body. |
| `@PathParam("id")` | Binds a URI path segment to a method parameter. |

**(EXAMPLE: SIMPLE CALCULATOR)** A REST service exposing `add` and `subtract` operations.
**1. Create the RESTful Web Service:**
```java
import javax.ws.rs.*;
@Path("/calc") // Base path for this resource
public class CalcREST {
    // Handles GET /calc/add/{a}/{b} and returns plain text
    @GET @Path("/add/{a}/{b}")
    @Produces(MediaType.TEXT_PLAIN)
    public String addPlainText(@PathParam("a") double a, 
                               @PathParam("b") double b) {
        return (a + b) + "";
    }
    // Handles same path but returns XML if client accepts it
    @GET @Path("/add/{a}/{b}")
    @Produces(MediaType.TEXT_XML)
    public String addXML(@PathParam("a") double a, 
                         @PathParam("b") double b) {
        return "<?xml version=\"1.0\"?><result>" + (a + b) + "</result>";
    }
    // Similar method for @Path("/sub/{a}/{b}")
}
```
**2. Publish the Service:** A main class to start an embedded HTTP server.
```java
public class CalcRESTStartUp {
    static final String BASE_URI = "http://localhost:9999/calcrest/";
    public static void main(String[] args) throws IOException {
        HttpServer server = HttpServerFactory.create(BASE_URI);
        server.start();
        sys.out.println("Server running. Press Enter to stop.");
        sys.in.read();
        server.stop(0);
    }
}
```
**3. Create a Client:** Java code to consume the service.
```java
Client client = Client.create();
WebResource addResource = client.resource("http://localhost:9999/calcrest")
                                 .path("calc/add/10/5");
// Request plain text
String textResult = addResource.accept(MediaType.TEXT_PLAIN).get(String.class);
// Request XML
String xmlResult = addResource.accept(MediaType.TEXT_XML).get(String.class);
```
**(OTHER FRAMEWORKS)** Other popular implementations include:
* **Django REST Framework (Python):** A powerful, feature-rich toolkit for building Web APIs in Django.
* **RESTEasy (Java):** A JBoss project, another full JAX-RS implementation.
### Data encoding and RPC
While REST over HTTP/JSON is dominant, other efficient data encoding formats are used, particularly in **RPC (Remote Procedure Call)** frameworks:
![[data-encoding-rpe-ex.png\|100]]
* **Protocol Buffers (protobuf):** Google's language-neutral, platform-neutral mechanism for serializing structured data. It is smaller and faster than XML/JSON. Used extensively in **gRPC**, a modern high-performance RPC framework.
* **Apache Thrift:** A scalable cross-language service development framework, combining a software stack with a code generation engine.
* **Apache Avro:** A data serialisation sys providing rich data structures, a compact binary format, and direct code generation.
* **Gson:** A Java library from Google to convert Java Objects into JSON and vice-versa.

**Check of Understanding**
> **Question:** You need to build a high-throughput internal microservice for real-time financial trade matching. The service requires very low latency, strict interface contracts, and must support streaming updates. Would you choose a REST/JSON approach using Flask or Jersey, or a gRPC/protobuf approach? Justify your choice based on the technical characteristics discussed.
> **Answer:** Choose **gRPC/protobuf**.
> * **Performance & Latency:** gRPC uses **HTTP/2** (multiplexing, header compression) and **Protocol Buffers** (binary, efficient serialisation), leading to much lower latency and higher throughput than REST/JSON over HTTP/1.1, which is critical for real-time trading.
> * **Interface Contracts:** Protobuf `.proto` files provide strict, forward/backward compatible interface definitions and enable automatic, type-safe client/server code generation. REST/JSON interfaces are looser and require manual validation.
> * **Streaming:** gRPC has first-class support for **bidirectional streaming**, perfect for pushing continuous trade updates. REST would require inefficient polling or bespoke solutions like WebSockets.
> While Flask/Jersey REST is excellent for public, web-centric APIs, gRPC/protobuf is optimised for high-performance, contract-first internal service communication.
## 8. Microservices, Nanoservices and Serverless
### Recap: SOAs
SOAs established the foundational principles of **decentralised middleware**, a **strong emphasis on loose coupling** between components, and **adherence to well-supported standards** for cross-system communication.
### Microservices
Microservices are a specific, fine-grained implementation of the SOA pattern. An app is decomposed into a suite of **small, independent, and loosely coupled services**. Each service runs in its own process, communicates via lightweight mechanisms (often HTTP/REST or messaging), and is built around a specific business capability.
![[microservices-ex.png\|400]]
**(KEY FEATURES)**
* **Specified Functionality:** Each service has a single, well-defined responsibility.
* **Independent Deployment:** Services can be developed, tested, and deployed in isolation, enabling continuous delivery.
* **Organised by Business Domain:** Services are structured around business processes (e.g., "User Service," "Order Service," "Payment Service").
**(ARCHITECTURAL SHIFT)** This represents a move from a monolithic architecture, where all functionality is bundled together, to a DS of collaborating services.
![[monolith-to-microservices.png\|200]]
**(CONTAINERS & MICROSERVICES)** Containers (e.g., Docker) are the ideal deployment vehicle for microservices. They package an app with all its dependencies into a single, lightweight, portable artefact. This provides the isolation and environment consistency that microservices require, without the overhead of a full virtual machine.

| Virtual Machine          | Container                       |
| ------------------------ | ------------------------------- |
| ![[vm-arch-ex.png\|100]] | ![[container-arch-ex.png\|100]] |

![[apps-and-dependencies-deployed-as-artifact.png.png\|300]]
### Nanoservices
Nanoservices take the microservices concept to an extreme level of granularity. A nanoservice is a **single-function service**—often just a few lines of code—that performs one very specific task. While microservices are "small," nanoservices are "tiny." This extreme decomposition can lead to massive operational complexity (managing thousands of services) and is generally considered an anti-pattern unless there is a specific, compelling need for such fine granularity.
### Serverless Computing
Serverless computing is a cloud execution model where the cloud provider dynamically manages the allocation and provisioning of servers. The developer writes **code (functions)** without any concern for the underlying infrastructure. The core principle is **"No Servers to Manage."**
### Function as a Service (FaaS)
FaaS is the specific implementation paradigm at the heart of serverless computing. It allows developers to deploy **individual functions** (blocks of code) that are executed in response to events. The platform automatically scales, runs, and bills for these functions down to the millisecond of execution time.
**(EXECUTION MODEL)** When an event triggers a function, the FaaS platform checks if an instance is already running ("warm"). If not, it performs a **cold start**: loading the function code, provisioning a runtime container, and then executing it. Subsequent invocations may reuse the warm instance for much lower latency.
![[serverless-execution-model-ex.png\|400]]
### Architectural Support
Serverless architectures are inherently **event-driven**. apps are composed of functions triggered by events from various sources (HTTP requests, db changes, message queues, timers).
![[serverless-apps-logic-flow.png\|400]]
**(KEY METRICS & CHALLENGES)**
1.  **Cold Start Latency:** The delay when instantiating a new function instance. This overhead is **not negligible** and is a critical performance consideration.
![[cold-vs-warm-serverless-app.png\|300]]
2.  **Statelessness:** Functions are inherently stateless. Any required state (user session, intermediate data) must be stored in external services (databases, caches), adding complexity and latency.
3.  **Function Composition:** Building complex apps requires orchestrating multiple functions, which can be done through sequential chaining, event-driven flows, or dedicated orchestration services.
**(EVOLUTION OF GRANULARITY)** The trend moves from large monoliths, to decomposed microservices, and further to ephemeral, event-triggered functions.
![[monolith-to-microservices-to-nanoservices.png\|200]]
### Solutions
**(COMMERCIAL & OPEN SOURCE FaaS PLATFORMS)**
* **AWS Lambda:** The pioneer. Functions are triggered by events from almost any AWS service (S3, API Gateway, DynamoDB) or custom HTTP endpoints.

| AWS Lambda Example          | AWS Lambda Arch                  |
| --------------------------- | -------------------------------- |
| ![[aws-lambda-ex.png\|400]] | ![[aws-lambda-arch-ex.png\|330]] |
* **Microsoft Azure Functions:** A polyglot platform supporting multiple languages. It integrates deeply with Azure services and offers easy deployment from source control.
    ![[azure-functions-steps-ex.png\|400]]
* **Google Cloud Functions, IBM Cloud Code Engine, Apache OpenWhisk, Knative:** Other major platforms providing similar FaaS capabilities, often with unique strengths in open-source integration or Kubernetes-native operation.

**(BENEFITS)**
* **No Server Management:** Eliminates operational overhead.
* **Continuous, Automatic Scaling:** Scales from zero to thousands of instances seamlessly.
* **Pay-Per-Use Cost Model:** You only pay for the compute time your functions consume, never for idle resources.

**Check of Understanding**
> **Question:** Your team is designing a new photo-sharing app. A core feature requires generating multiple thumbnail sizes whenever a user uploads an image. Should this feature be implemented as part of a monolithic "Upload Service," as a standalone microservice, or as a serverless function? Justify your choice based on architectural concepts.
> **Answer:** The thumbnail generation is an ideal candidate for a **serverless function (FaaS)**. Justification:
> * **Event-Driven:** It is triggered by a specific, infrequent event (image upload).
> * **Variable, Bursty Workload:** Upload frequency is unpredictable. Serverless scales to zero when idle (cost-effective) and instantly handles sudden spikes (e.g., many users uploading at once).
> * **Short-Running, Stateless Task:** The job is compute-intensive but short-lived. It reads from object storage (the original image) and writes back to object storage (the thumbnails), requiring no persistent internal state.
> * **Decoupled & Focused:** Implementing it as a standalone function keeps it completely decoupled from the main upload logic, adhering to the microservices principle of single responsibility, but with the operational simplicity of serverless. A full microservice would require managing a constantly running container for a sporadic task, which is less efficient.
## 9. Naming Pt.1 - *Chapter 5*
### Fundamental concepts
**(DEFINITIONS)** In a DS, an **entity** (resource like a file, service, or host) must have a **name** to be accessed. A name is a string (e.g., `server42`, `/home/file.txt`). To locate and interact with an entity, its name must be **bound** to its attributes (e.g., IP address, port). The process of translating a name into these attributes is called **name resolution**.
![[name-res-ex.png\|200]]
**(ACCESS POINTS AND ADDRESSES)** An **access point** is a special entity used to operate on another entity. Its name is an **address** (e.g., `192.168.1.5:8080`). An entity can have multiple or changing access points. For flexibility, we prefer **location-independent names** (stable identifiers) rather than directly using addresses.
**(FUNCTION OF A NAMING System)** A naming sys manages **bindings** between names and entity attributes. Its primary function is **name resolution**. Secondary functions include creating/deleting bindings, listing names, and organising the **namespace** (the set of all valid names).

Three broad classes exist:
1.  **Flat Naming:** Uses unstructured identifiers (IDs). This lecture's focus.
2.  **Structured Naming:** Uses human-readable, hierarchical names (e.g., file paths, URLs). (Covered in Pt.2).
3.  **Attribute-Based Naming:** Entities are described/searchable by attributes (e.g., "printer, colour, floor 3"). (Covered in Pt.2).
### Namespaces
This section focuses on **flat naming systems**, where entities have non-hierarchical, often meaningless identifiers (IDs). The key challenge is **locating an entity when given only its flat ID**.

**(BROADCASTING)** One simple method is to **broadcast** a query asking "who has this ID?" across the local network.
* *Example: Address Resolution Protocol (ARP)* resolves an IP address to a MAC address by broadcasting "Who has IP `192.168.1.10`?" on the local LAN.
![[arp-pro-ex.png\|200]]
* **Drawback:** Does **not scale** beyond local networks; generates excessive traffic in large systems.

**(FORWARDING POINTERS)** When an entity moves, it leaves a **forwarding pointer** at its old location pointing to the new one. To locate it, a client follows the chain of pointers.
![[naming-pt1-ssp-chains-ex.png\|300]]
* *Example in RMI:* A **client stub** can hold a forwarding pointer to the **server stub** (a **Stub-Scion Pair** or SSP). Shortcuts can be stored to improve future lookups.
![[ssp-chains-a-and-b.png\|400]]
* **Drawbacks:** Long chains are **not fault-tolerant** (a broken pointer breaks resolution) and increase **latency**.

**(HOME-BASED APPROACH)** A central **home location** maintains the current address of a mobile entity.
* *Example: Mobile IP.* A mobile host has a permanent **home address**. A **home agent** on its home network tracks its current **care-of-address**. Packets sent to the home address are **tunnelled** by the home agent to the current location.
![[mobile-ip-world-map-ex.png\|400]]
* **Drawbacks:** The fixed home can become a **single point of failure** and a **performance bottleneck**, especially if the entity and client are far from the home (poor **geographical scalability**).
### Naming graphs
This metaphor describes structures for organising and resolving names on a larger scale.

**(DISTRIBUTED HASH TABLES - DHTs)** A DHT is a decentralised, structured overlay network that provides a hash-table-like interface: `put(key, value)` and `get(key)`. Entities (values) are assigned a unique **key** (via hashing). Responsibility for storing the `(key, address)` binding is distributed across all nodes in the network based on the key's value.
	
| DHT Principle                                  | DHT Example          |
| ---------------------------------------------- | -------------------- |
| ![[distributed-hash-table-principle.png\|400]] | ![[dht-ex.png\|300]] |
* *Example:* In a Chord DHT, nodes are arranged in a logical ring. An entity with key `k` is stored on the node with the smallest ID ≥ `k` (its **successor**). Lookups are routed efficiently around the ring in `O(log N)` hops.
* **Advantages:** Highly **scalable** and **decentralised**; no single point of failure.

**(HIERARCHICAL APPROACHES)** This method organises directory nodes into a **tree** that mirrors the network's hierarchical structure (e.g., countries, orgs, departments). Directories store location information for entities within their subtree.
![[hierarchical-tree-ex.png\|300]]
* **(LOOKUP OPERATION)** To find an entity, start at the local leaf node. If it doesn't know the location, the request moves **up** the tree until a directory that knows about the entity is found (ultimately, the root knows all). The request then moves **down** via pointers to the leaf node holding the address.
![[lookup-operation.png\|300]]
* **(INSERT OPERATION)** To insert a new entity address, the request moves up the tree until finding a node that already knows about the entity. A chain of **forwarding pointers** is then created back down to the leaf node storing the new address.
![[insert-operation-a-b.png\|400]]
* **Trade-off:** More structured and scalable than broadcasting or pure home-based, but insert/lookup operations involve multiple network hops.

**Check of Understanding**
> **Question:** You are designing a peer-to-peer file-sharing sys where millions of nodes join and leave frequently, and files (entities) are referenced by a unique hash of their content (a flat name). Which flat naming resolution scheme discussed would be most suitable and why? What is a key operational challenge you would need to manage?
> **Answer:** A **Distributed Hash Table (DHT)** would be most suitable.
> **Why:** The unique file hash serves as a perfect **key**. A DHT efficiently and **decentralises** the responsibility of storing the binding between this key and the current node(s) storing the file. It scales to millions of nodes and handles churn (nodes joining/leaving) gracefully through its structured overlay and replication mechanisms.
> **Key Challenge:** **Maintaining the DHT's consistency and routing tables** in the face of high churn. As nodes frequently join and leave, the sys must efficiently update successor pointers and replicate data to ensure `get(key)` operations remain reliable and efficient.
## 10. Naming Pt.2 - *Chapter 5*
### Structured naming
Structured names are human-readable and often hierarchical (e.g., `/home/user/file.txt`, `www.example.com`). They are composed from simple names and organised within a **namespace**.
#### Namespaces
A namespace is the set of all valid names recognised by a sys. It can be represented as a **directed graph** with two node types:
* **Leaf Nodes:** Represent named entities (files, hosts, services). They store the entity's attributes or state.
* **Directory Nodes:** Act as catalogs. They store a **directory table** of (`edge-label`, `node-identifier`) pairs, defining the outgoing edges.
Each path through the graph, identified by a sequence of labels (e.g., `L1, L2, ... Ln`), is a structured name.
![[naming-file-sys-ex.png\|200]]
*Example:* A filesystem namespace. A single root node is common.
![[single-root-naming-ex.png\|400]]

#### Name resolution
Name resolution is the process of traversing the naming graph to translate a name into the entity it refers to. The starting point is defined by a **closure mechanism**.
**(CLOSURE MECHANISM)** This is the rule or context that determines the **initial node** (index node) for resolution.
* *Example (Unix Filesystem):* To resolve `/home/steen/mbox`, the closure mechanism provides access to the root directory's inode. The OS finds this via the disk's superblock.
![[closure-mechanism-unix-ex.png\|400]]
* *Other Examples:* `www.distributed-systems.net` starts resolution at a known DNS server; `0031 20 598 7784` starts at the local telephone exchange.
**(path name RESOLUTION)** The process of walking the graph. In a Unix filesystem, every file/directory is an **inode**. A directory is a file mapping names to inode numbers. Resolution walks this chain.

| Inode                  | Path Name Resolution                  |
| ---------------------- | ------------------------------------- |
| ![[inode-ex.png\|300]] | ![[path-name-resolution-ex.png\|350]] |
**(LINKING)** Creates **aliases**, allowing multiple names for one entity.
* **Hard Link:** A direct directory entry pointing to the same inode. Multiple absolute paths refer to the same node.
* **Symbolic (Soft) Link:** A special leaf node that stores the *path name* of the target entity as data.
![[linking-unix-ex.png\|400]]
**(MOUNTING)** A key mechanism for **integrating different namespaces** transparently. It associates a node in the current namespace (the **mount point**) with the root (**mounting point**) of a **foreign namespace** (e.g., a remote filesystem).
![[mounting-sys-ex.png\|400]]
*Example:* Mounting a remote NFS export `/remote/vu` to the local directory `/home/steen/remote` allows accessing files via `/home/steen/remote/mbox`.

#### Implementation of a namespace
For large-scale, distributed namespaces (like DNS), the naming graph is distributed across many **name servers**. The namespace is partitioned into logical layers:
1.  **Global Layer:** Stable, high-level directory nodes (e.g., DNS root and top-level domains `.com`, `.uk`). Managed by different administrations.
2.  **Administrational Layer:** Mid-level directories for orgs (e.g., `example.com`). Each zone is managed independently.
3.  **Managerial Layer:** Low-level nodes within a single administration (e.g., `www.example.com`, `mail.example.com`). Often replicated for robustness and performance.

| DNS Partitioning Example          | Namespace Implementation Table            |
| --------------------------------- | ----------------------------------------- |
| ![[dns-partitioning-ex.png\|300]] | ![[namespace-implementation-ex.png\|300]] |
**(DOMAIN NAME System - DNS)** the canonical distributed, hierarchical naming sys for the internet.
* **Zone Data:** Authoritative data for a domain portion, including records like **A** (address), **NS** (name server), **MX** (mail exchange), and **CNAME** (canonical name alias). Data is cached with a **Time-To-Live (TTL)**.
![[dns-node-info.png\|300]]

| Iterative Resolution                                                                  | Recursive Resolution                                                                                 |
| ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| ![[dns-iterative.png\|350]]                                                           | ![[recursive-ex.png\|350]]                                                                           |
| The contacted server returns the **next server** to ask. The client does the legwork. | The contacted server **forwards the query** itself, eventually returning the final answer to client. |
* **Trade-offs:** Recursive resolution places more load on name servers but enables more effective caching at intermediate servers. Iterative resolution is less demanding on individual servers.
![[scalability-issues.png\|400]]

### Attribute-based naming
Also known as **directory services**. Instead of looking up an entity by its precise name, you search for it using a set of **descriptive attributes**. This is like using a "yellow pages" versus a "white pages" directory.

**(LDAP - LIGHTWEIGHT DIRECTORY ACCESS PROTOCOL)** A standard protocol for directory services. Data is organised in a **Directory Information Tree (DIT)**, where each entry is a collection of (`attribute`, `value`) pairs and is uniquely named by a **Relative Distinguished Name (RDN)**.

| LDAP Example Arch     | DIT Example                    |
| --------------------- | ------------------------------ |
| ![[ldap-ex.png\|300]] | ![[dir-info-tree-ex.png\|300]] |
* *Example Search:* A query like `(C=UK)(O=Leeds University)(OU=Computing)(CN=Main Server)` would return all directory entries matching those attributes, potentially revealing multiple host servers.
* *Example Table:* Two entries distinguished by their `HostName` RDN.

**(JAVA NAMING AND DIRECTORY INTERFACE - JNDI)** A Java API that provides a uniform interface to multiple different naming and directory services (LDAP, DNS, CORBA, RMI registry) through pluggable **service providers**.
![[jndi-arch-ex.png\|300]]
It allows apps to bind objects to names and perform lookups and searches independent of the underlying service implementation.

**Check of Understanding**
> **Question:** Your distributed app needs to find an available, colour, A3-capable printer located on the same floor as the user. The sys knows the user's current floor and the printers are registered with various attributes (type, capabilities, location). Would you use DNS, the local filesystem namespace, or an LDAP-based directory service to implement this discovery? Justify your choice.
> **Answer:** You would use an **LDAP-based directory service**.
> * **DNS** is designed for **structured name resolution** (hostname to IP address). It is poorly suited for searching by arbitrary, multi-valued attributes like "colour-capable" and "A3".
> * The **filesystem namespace** is also for structured, hierarchical name lookup, not attribute-based querying.
> * An **LDAP directory** is specifically built for **attribute-based naming**. Each printer could be an entry with attributes like `cn=Printer42`, `type=laser`, `capabilities=colour,A3,duplex`, `locationFloor=5`. The app can then perform a search query like `(&(type=laser)(capabilities=colour)(capabilities=A3)(locationFloor=5))` to find all matching printers. This is the exact functionality directory services provide. Directory Services
## 11. Timing and Synchronisation - *Chapter 6*
### Synchronisation in a DS
In DSs, there is **no global agreement on time**. Each machine's internal clock (typically a **quartz crystal oscillator**) drifts at a unique rate, measured in **parts per million (ppm)**. This **clock skew** means an event that occurred later can be assigned an earlier timestamp, causing issues for logs, schedules, failure detection, and event ordering.
![[timing-clock-ex.png\|400]]
### Internal and external physical clocks
**(INTERNAL CLOCKS)** A computer's internal clock counts oscillations of a quartz crystal, triggering interrupts ("ticks") to maintain a software clock. The drift rate is bounded by a **maximum drift rate (ρ)**, meaning $1-ρ ≤ dC/dt ≤ 1+ρ$.

**(EXTERNAL STANDARDS & UTC)** The worldwide standard is **UTC (Coordinated Universal Time)**, based on atomic clocks (TAI) with leap seconds added to align with solar time. Machines need to synchronise their internal clocks with an external source like UTC.
![[utc-time-standard.png\|400]]
**(TIME REPRESENTATION)** Common formats are **Unix time** (seconds since epoch) and **ISO 8601**. A critical bug related to handling a **leap second** on 30 June 2012 caused widespread crashes, highlighting the importance of robust timekeeping.
### Clock synchronisation algorithms
Algorithms aim to minimise skew between clocks. If one machine has a UTC receiver, it acts as a **time server**.
#### Drift as a Function of UTC
• If two clocks are drifting from UTC in opposite directions, at a time dt after synchronisation they may be as much as 2$\rho$dt apart.
![[drift-as-func-utc.png\|200]]
25/02/1991. Patriot Missile Failure: Software error in the sys clock. Accumulated clock drift.
**(CRISTIAN'S ALGORITHM)** A client requests the time from a server. It estimates the network delay and adjusts its clock to `server_time + (round_trip_delay / 2)`. It must apply the correction gradually to avoid time running backward.
![[christian-algo.png\|200]]
![[est-time-over-network.png\|300]]
**Round-trip network delay** $\delta = (t_4 - t_1) - (t_3 - t_2)$
**Estimated server time** when client receives response: $t_3 + \delta/2$
**Estimated clock skew**: $\delta = t_3 + \delta/2 – t_4 = (t_2 - t_1 + t_3 - t_4)/2$

**(BERKELEY ALGORITHM)** Used when no UTC source is available. A **time daemon** (coordinator) polls all machines for their time, calculates the **average** (ignoring outliers), and instructs each machine on how much to adjust its clock (forward or backward).

| Berkeley Algorithm Ex           | Average Calculation                                                                                                                                                                                                                                                                                |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![[berkley-algo-time.png\|100]] | $= \text{diff}(D, D) + \text{diff}(D, C_1) + \text{diff}(D, C_2)\ \ / \ \text{no. clocks}$  <br><br>$= \text{diff}(\text{3:00}, \text{ 3:00}) + \text{diff}(\text{3:00}, \text{ 3:50}) + \text{diff}(\text{3:00}, \text{ 3:25})\ / 3$<br><br>$= ( 0 -10 + 25 ) / 3$ <br><br>$= +5 \text{ minutes}$ |
1. Time daemon clock $D$ shows **3:00**, requests network clock values.
2. Network clocks $C_n$, return their values to the daemon $C_1 =$ 2:50, $C_2 =$ 3:25.
3. Daemon calculates average. 
4. Daemon adjusts all clocks to match its own (3:00) + 5 more minutes= **3:05.**
### Election Algorithm
When a coordinator (e.g., the time daemon in Berkeley) fails, an **election algorithm** selects a new leader.
**(BULLY ALGORITHM)** The process with the highest ID wins.
1.  Any process noticing coordinator failure sends **ELECTION** messages to processes with higher IDs.
2.  If it gets no **OK** response, it wins and announces **I WON** to lower-ID processes.
3.  If it gets an **OK**, it waits for the **I WON** message.
4.  A process receiving an **ELECTION** message replies **OK** and starts its own election.
![[bully-algo-ex.png\|400]]
![[bully-algo-ex-pt-2.png\|300]]
It uses `O(n²)` messages.
**a)** Process 7 has crashed and Process 4 holds an election 
**b)** Process 5 and 6 respond, telling 4 to stop
**c)** Now 5 and 6 hold elections **d)** Process 6 tells 5 to stop **e)** Process 6 wins and tells everyone.
If P7 is restarted, it will send all the others a COORDINATOR msg and bully them into submission.

**(RING ALGORITHM)** Processes are arranged in a logical ring. A process starting an election sends an **ELECTION** message with its ID to its successor. Each successor adds its own ID and forwards the message. When the message returns to the initiator, it circulates a **COORDINATOR** message with the highest ID as the new leader.
![[ring-algo.png\|400]]
Election algorithm using a ring. The solid line shows the election messages initiated by P6; the dashed one those by P3. We can see what happens if two processes, P3 and P6, discover simultaneously that the previous coordinator, process P7, has crashed. Each of these builds an ELECTION msg and each of them starts circulating its message, independent of the other one. 
	Eventually, both messages will go all the way around, and both P3 and P6 will convert them 
into COORDINATOR messages, with exactly the same members and in the same order. When both have gone around again, both will be removed. It does no harm to have extra messages circulating; at worst it consumes a little bandwidth, but this is not considered wasteful.
### Network time protocol (NTP)
**NTP** is the hierarchical, robust protocol used to synchronise clocks across the Internet to UTC.
**(STRUCTURE)** Servers are organised in **strata**.
* **Stratum 1:** Primary servers directly connected to UTC sources (e.g., atomic clocks, GPS).
* **Stratum 2:** Synchronise with stratum 1 servers.
* **Stratum 3+ and clients:** Synchronise with higher-stratum servers. Accuracy decreases with stratum number.

**(CLOCK CORRECTION)** NTP calculates the offset (θ) between client and server.
Systems that rely on clock sync need to monitor clock skew!
* **|θ| < 125 ms:** **Slew** the clock (adjust speed gradually).
* **125 ms ≤ |θ| < 1000 ms:** **Step** the clock (jump to the correct time).
* **|θ| ≥ 1000 ms:** **Panic** (do nothing, require manual intervention).

**Check of Understanding**
> **Question:** A distributed sensor network for a scientific experiment logs each reading with a local timestamp. During analysis, you notice events from Sensor A always appear 50ms before equivalent events from Sensor B, but you suspect they are simultaneous. You know both sensors synchronise via NTP to the same stratum-2 server. List three distinct potential causes for this consistent 50ms offset in the timestamps. **Answer:**
> 1. **Asymmetric Network Delay:** The most likely cause. NTP assumes symmetric network paths. If the path from the NTP server to Sensor A has a different latency than the path to Sensor B (e.g., due to different network hops or congestion), the calculated clock offset will be inaccurate. A consistent 50ms difference suggests a fixed routing asymmetry.
> 2. **Different NTP Strata or Servers:** If Sensor B is synchronising indirectly (e.g., getting time from Sensor A as a stratum-3 source) while Sensor A syncs directly with the stratum-2 server, Sensor B's time would have an extra hop of latency and potential error added.
> 3. **Local Processing Delay on Sensor B:** The timestamp might be applied *after* some internal processing or queueing on Sensor B, adding a constant delay. This is a software/architectural issue, not a clock synchronisation error per se, but results in the same observable offset.
## 12. Consistency and Replication Pt.1 - *Chapter 7*
### Introduction to the problem
A fundamental issue in DSs is **replication**: maintaining multiple copies of the same data (e.g., a file, db record) across different machines.
![[replica-db-ex.png\|300]]
**(REASONS FOR REPLICATION)**
* **Reliability:** If one replica crashes, the sys can continue using another, increasing fault tolerance.
* **Performance & Scalability:**
  * **Scaling for Size:** Distributes client load across multiple servers.
  * **Scaling Geographically:** Places data closer to users, reducing access latency (e.g., a content delivery network or CDN).

**(THE CONSISTENCY PROBLEM)** The core challenge is **keeping replicas consistent**. When one copy is updated, all other copies must eventually reflect that change. If updates aren't propagated correctly, replicas diverge, leading to stale data and incorrect results.

**(THE PERFORMANCE-SCALE DILEMMA)** Strict consistency (synchronous replication) requires that all replicas agree on the order of **conflicting operations** (Read-Write or Write-Write conflicts). Achieving this global agreement requires **synchronisation**, which is slow, communication-intensive, and hurts scalability.
*Solution:* To build scalable systems, we often **relax the consistency requirements**, accepting that replicas may be temporarily inconsistent. The specific rules for what is acceptable are defined by **consistency models**.
### Data-centric consistency models
A **consistency model** is a formal contract between a distributed data store and its client processes. It defines the possible results of **read** and **write** operations when they are executed concurrently by multiple processes, specifying what values a read operation is allowed to return.
![[data-store-ex.png\|200]]
#### Continuous Consistency
This model quantifies inconsistency, allowing apps to specify *how much* inconsistency they can tolerate, rather than requiring absolute consistency. A replica can deviate from others in three dimensions:
1.  **Numerical Deviation:** The absolute difference in value (e.g., a bank account balance on replica A is £1000, on replica B it's £950, a deviation of £50).
2.  **Staleness Deviation:** The maximum time a replica's value is older than another's.
3.  **Ordering Deviation:** The number of pending (uncommitted/not-yet-propagated) update operations.

**(CONSISTENCY UNIT - CONIT)** The data unit over which these deviations are measured is called a **Conit**. An app defines a Conit, and the sys ensures deviations stay within specified bounds.

**(CONIT EXAMPLE)** A fleet manager tracks average fuel cost. The Conit consists of three variables: gallons tanked (`g`), price paid (`p`), and distance driven (`d`). These are replicated.
![[conit-replicas.png\|400]]
* **Vector Clocks** track known operations from each replica.
* **Order Deviation (3 for A):** A has 3 local, tentative operations pending permanent commitment.
* **Numerical Deviation (2,482 for A):** The sum of the values from operations A has *missed* from B (e.g., two missed updates totalling £70 + £412).
*   An app could specify: "Never let the numerical deviation for the 'fuel cost' Conit exceed £1,000." The sys would then trigger synchronisation before this bound is breached.
#### Sequential Consistency
This is a stricter model. The result of any execution must be equivalent to the results of some **sequential execution** of all processes' operations, where the operations *of each individual process* appear in this sequence in the **order specified by its program** (program order).

**(DEFINITION & DIAGRAM)** It does *not* require operations from different processes to happen in real-time order, only that all processes agree on a single, global sequential order of *all* operations.
![[seq-consistency.png\|250]]
In Diagram (a): P2 reads `x` as `NIL`, then later reads `a`. This is sequentially consistent because we can construct a valid sequential order: `W1(x)a` -> `R2(x)NIL` -> `R2(x)a`.

**(THE ROLE OF TIME)** Sequential consistency is about logical order, not physical time.
![[seq-consistency-no-time.png\|501]]
* **Diagram (a) is Valid:** P3 and P4 *both* see `W2(x)b` happening before `W1(x)a`. They agree on the interleaving.
* **Diagram (b) is a Violation:** P3 and P4 see *different* interleavings of the writes. There is no single sequential order that satisfies both views.
**(EXAMPLE WITH PROGRAMS)**
![[seq-print-ex.png\|400]]
For three processes with the above code, sequential consistency does *not* guarantee that all prints will show `1,1,1`. It only guarantees that the execution is equivalent to *some* sequential interleaving of the statements that respects each process's program order. Many interleavings are possible, leading to various `(x,y,z)` print outputs (like `1,0,0`), all of which are valid under this model.

**(THE HAPPENS-BEFORE RELATION)** This is a foundational concept for defining order in DSs without a global clock.
* **Definition:** Event `a` **happens-before** event `b` (`a → b`) if:
    1.  `a` and `b` are on the same process and `a` occurs before `b`.
    2.  `a` is the sending of a message and `b` is the receipt of that *same* message.
    3.  There exists an event `c` such that `a → c` and `c → b` (transitivity).
*   If neither `a → b` nor `b → a`, the events are **concurrent** (`a || b`).
![[happens-before.png\|200]]
- a → b, c → d, and e → f due to node execution order
- b → c and d → f due to messages m1 and m2
- a → c, a → d, a → f, b → d, b → f, and c → f due to transitivity
- a ‖ e, b ‖ e, c ‖ e, and d ‖ e
#### Causal Consistency
A relaxation of sequential consistency. It only requires that **causally related** writes be seen by all processes in the *same* order. **Concurrent writes** (writes that are not causally related) may be seen in different orders by different processes.

**(DEFINITION & DIAGRAM)** A write `W2` is causally related to a previous write `W1` if `W2` could have been influenced by knowing the result of `W1` (e.g., a process read the value written by `W1` before performing `W2`).
![[causal-consistency.png\|300]]
* **Valid:** `W1(x)c` and `W2(x)b` are concurrent (no causal link). Therefore, P2 and P3 are allowed to see them in different orders.

**(VIOLATION EXAMPLE)**
![[violated-causal-consistency.png\|400]]
* **Diagram (a) is a Violation:** `R2(x)a` (the read of `a` by P2) creates a causal link from `W1(x)a` to `W2(x)b`. Therefore, all processes must see `W1(x)a` before `W2(x)b`. P3 sees them in reverse order, violating causal consistency.
* **Diagram (b) is Acceptable** for causal (but *not* sequential) consistency because `W1(x)a` and `W2(x)b` are concurrent.

**Check of Understanding**
> **Question:** A social media post and its comments are stored across three geographically distributed replicas (US, EU, Asia). A user in the US posts an update (Write A). A user in Asia reads that post and immediately posts a comment (Write B). A user in Europe then reads the original post. Under a **causally consistent** model, is it permissible for the European user to see the comment (Write B) but *not* see the original post (Write A) it replied to? Explain using the happens-before relation.
> **Answer:** **No, this is not permissible under causal consistency.**
> **Explanation using happens-before:** The Asian user's read of the post establishes a causal link: `Write A → Read A (in Asia) → Write B`. Therefore, `Write A` happens-before `Write B` (`Write A → Write B`). The causal consistency model requires that all processes see causally related writes in the same order. If the European user sees `Write B` (the comment), they must also have seen `Write A` (the original post) that causally preceded it. Seeing the effect (comment) without its cause (original post) violates the contract.
## 13. Consistency and Replication Pt.1 - *Chapter 7*
### Client-centric consistency models
Client-centric consistency models guarantee consistency for the **operations of a single client** across a distributed data store. They do not provide guarantees about concurrent operations from different clients. These models are crucial for mobile or intermittently-connected users who access the data store from different locations, ensuring their view remains self-consistent even as they move.

**(BASIC ARCHITECTURE)** The sys involves multiple local stores (e.g., `L1`, `L2`). A client process (`P`) performs read (`R`) and write (`W`) operations on a data item `x`. Different versions of `x` (e.g., `x1`, `x2`) exist across these stores.
![[client-centric-arch.png\|400]]
Notation: `W1(x1;x2)` means process P1 writes version `x2` based on the previous version `x1`. `W1(x1|x2)` indicates a concurrent write that does not follow from `x1`.
#### Monotonic Reads (MRs)
If a process reads a particular version of a data item, any future read by that same process must return that version or a **never-older** version. It prevents a user from seeing data "go back in time" when they switch servers.
**(EXAMPLE)** A user reads their unread email count (10 emails) on a server in London (`L1`). Later, they connect from a server in New York (`L2`). Monotonic Reads guarantees the count in New York is **at least 10**. It could be 12 (if new mail arrived), but never 9.
![[monotonic-read-ex-1.png\|300]]
*Diagram (b) shows a violation:* P1 reads `x1` at L1, but a later read at L2 returns `x2`, which is a version that was created *concurrently* to and does *not* include the updates seen in `x1`. This is not allowed.
#### Monotonic Writes (MWs)
Write operations performed by a single process must be **propagated to all replicas in the same order** they were issued. This ensures a user's sequence of updates is applied correctly everywhere.

**(EXAMPLE)** A user first updates a document's title (Write A), then updates its body text (Write B). Monotonic Writes ensures that no server will ever see Write B applied *before* Write A. This is critical for operations that depend on previous ones (e.g., installing software libraries in the correct order).
![[monotonic-write-ex1.png\|300]]
*Diagram (b) shows a violation:* The write that produced `x2` at L1 is propagated to L2 *after* a later write that produced `x3`. The order of writes seen at L2 is wrong.
#### Read Your Writes (RYWs)
The effects of a write operation by a process are **always visible to that process's subsequent read operations**, no matter from which location it reads. It prevents a user from seeing stale data after they have just updated it.

**(EXAMPLE)** You update your profile picture on a social media site (write). When you immediately refresh the page (read), you are guaranteed to see the new picture, not the old cached version.
![[read-your-write-consistency-1.png\|300]]
*Diagram (b) shows a violation:* P1 writes `x1` at L1. Later, when reading at L2, it sees `x2`, which is a version created *without* incorporating its own previous write (`x1`). This violates the "read your own writes" guarantee.
### Replica management
This involves deciding **where** to place replica servers, **what content** to put on them, and **how** to keep them consistent.
#### Content Replication
Replicas can be categorised by their initiation method:
* **Permanent Replicas:** The initial, fixed set of replicas that form the core of the distributed data store.
* **Server-Initiated Replicas:** Created dynamically by the sys (owner of the data) to improve performance, typically placed near clusters of demanding clients.
* **Client-Initiated Replicas (Caches):** Created at the request of a client to locally store data for fast access (e.g., a web browser cache).
![[replica-rings.png\|400]]
#### Server-Initiated Replica Example
A sys dynamically creates temporary replicas of popular content to reduce load on the origin server and bring data closer to users. The decision is based on tracking access patterns.
**(ALGORITHM)** Servers monitor file access counts. Requests are aggregated by the server closest to the client.
![[server-initiated-replica.png\|400]]
*   If accesses for file `F` from a region exceed a **Replication threshold (R)**, a new replica of `F` is created in that region.
*   If accesses fall below a **Deletion threshold (D)**, the replica is removed.
*   If accesses are between D and R, the file might be **migrated** (moved) instead of replicated.

**(EXAMPLE)** A viral video hosted in London (`Q`) suddenly gets millions of requests from Asia. The London server sees the high access count `cntQ(P, F)` from an Asian proxy server (`P`). It then decides to create a server-initiated replica of the video on a server in Singapore to serve Asian users directly.
#### Consistency Protocols: Primary-backup Protocol with Remote Writes
This is a primary-based protocol for implementing sequential consistency. One server is designated the **primary** (or master) for a data item.

**(MECHANISM - BLOCKING)**
1.  A client sends a write request to the primary server.
2.  The primary performs the write on its local copy.
3.  The primary forwards the update to all **backup** (secondary) servers.
4.  The primary waits for acknowledgements from all backups.
5.  Only after all backups confirm, does the primary send an acknowledgement back to the client.
![[primary-bakcup-remote-write-prot.png\|400]]
**(CHARACTERISTICS & PERFORMANCE)**
* **Strong Consistency:** Provides sequential consistency because the primary serialises all writes.
* **Fault Tolerance:** High, as the write is confirmed on multiple servers before completion.
* **Performance:** Write latency is high (blocking) because the client must wait for multiple network round-trips. Read performance is good (can be served locally from any replica).
#### Consistency Protocols: Primary-backup Protocol with Local Writes
A variant where the **primary role can move** to the client that wants to perform a write. This is beneficial for mobile or disconnected operation.
**(MECHANISM)**
1.  A client needing to write locates and "moves" the primary copy of the data to its local machine.
2.  The client can now perform multiple writes **locally** and very quickly.
3.  Updates are propagated to the backup servers **asynchronously** (non-blocking) after the local writes are done.
![[primary-bakcup-local-write-prot.png\|400]]
**(USE CASES & TRADE-OFFS)**
* **Use Case 1 - Mobile Disconnected Operation:** A user takes a file primary with them on a laptop, works offline, and synchronises changes back to the network later.
* **Use Case 2 - Performance Optimisation:** A client performing a burst of updates (e.g., a batch job) can temporarily become the primary to avoid network latency for each write.
* **Trade-off:** Provides lower write latency for the primary holder but weakens immediate consistency guarantees for other readers until propagation is complete.

**Check of Understanding**
> **Question:** A globally distributed note-taking app uses a primary-backup protocol. A user in Tokyo starts editing a document. To minimise latency, the app makes her local device the primary for that document. She makes ten rapid edits (local writes). At the same time, a collaborator in Berlin refreshes the document.
> 1.  Under a **remote-write** protocol, what would the Berlin user see during the Tokyo edits, and what is the performance impact for the Tokyo user?
> 2.  Under the described **local-write** protocol, what is the Berlin user likely to see, and what consistency model (client-centric) is potentially violated for the Berlin user?
> **Answer:**
> 3. **Remote-Write:** The Berlin user would see each edit **as soon as it is fully replicated** to the backups. The Tokyo user would experience **high write latency** for each edit, as each one would need to be sent to and acknowledged by the central primary and all backups before she could continue.
> 4. **Local-Write:** The Berlin user is **likely to see stale data** (the old version) until the Tokyo user's device propagates its batch of updates. This scenario violates the **Monotonic Reads** guarantee for the Berlin user if he had previously seen a more recent version from another server. More generally, it highlights the trade-off for **Read Your Writes** consistency *across different users*.
## 14. Fault Tolerance - *Chapter 8*
The key technique for handling failures is redundancy.
### Dependability, reliability and availability in a DS
Being fault tolerant is strongly related to what are called dependable systems.
Requirements for a **dependable** sys: *Availability*, *Reliability*, *Safety* and *Maintainability*.

**(Availability)** The probability the sys is operational and ready for use at a _given instant in time_. A highly available sys has minimal downtime.
**(Reliability)** The probability the sys operates _continuously without failure_ over a specified _time interval_ instead of an instance in time. A highly reliable sys has long periods of uninterrupted service.
**(Safety)** When a sys fails temporarily, it fails in a way that does not cause catastrophic damage (e.g., a nuclear reactor control sys failing to a safe state).
**(Maintainability)** The ease and speed with which a failed sys can be repaired. High maintainability supports high availability.

**(QUANTIFYING AVAILABILITY & RELIABILITY)** Key metrics include:
The **availability** $A(t)$ of **a component** in the time interval $[0, t]$ is the *average fraction of time* that the component has been *functioning correctly* during that interval. 
The **long-term availability** $A$ of a component is defined as $A(∞)$.
Likewise, the **reliability** $R(t)$ of a component in the time interval $[0, t]$ is the *conditional probability* that it has been *functioning correctly* during that interval *given* that it was *functioning correctly* at time $T = 0$.
- **Mean Time To Failure** ($MTTF$): The average time until a component fails
- **Mean Time To Repair** ($MTTR$): The average time needed to repair a component
- **Mean Time Between Failures** ($MTBF$) = $MTTF + MTTR$
- General Availability: $A ={ MTTF \over MTBF} = {MTTF \over{MTTF + MTTR}}$. For example, a sys with an MTTF of 1000 hours and an MTTR of 10 hours has an availability of `1000 / 1010 ≈ 0.99` (99%).

**(NOTE)** Reliability and availability make sense only if we have an accurate notion of what a *failure* actually is. Availability and reliability measure different things. A sys that is down for 1ms every hour has 99.9999% availability but is unreliable (fails frequently). A sys shut down for 2 weeks every year is reliable during operation but only 96% available.
### Terminology
A sys is said to *fail* when it cannot meet its promises. In particular, if a DS is designed to provide its users with a number of services, the sys has failed when one or more of those services cannot be (completely) provided.

| Term                                    | Description                                                              | Example                                              |
| --------------------------------------- | ------------------------------------------------------------------------ | ---------------------------------------------------- |
| Failure                                 | A component is not living up to its<br>specifications.                   | Program crashes.                                     |
| Error                                   | Part of a component (system's state) that can lead to a failure.         | Programming Bug.                                     |
| Fault                                   | The cause of an error.                                                   | The developer.                                       |
| Fault Prevention                        | Prevent the occurrence of a fault.                                       | Thorough software verification and validation.       |
| Fault Tolerance (FT)<br>(**Important**) | Build a component such that it can mask the occurrence of a fault.       | Build each component by two independent programmers. |
| Fault Removal                           | Reduce the presence, number, or seriousness of a fault.                  | Thorough software verification<br>and validation.    |
| Fault Forecasting                       | Estimate current presence, future incidence, and consequences of faults. | Check code quality /<br>programming experience.      |
The developer is the *fault* for the *error*: bug which caused the program to crash in *failure*.
Aim: improve *fault tolerance* -> sys can provide its services *even* in the case of faults.
### Failure models

| Type of Failure                           | Description of Server’s Behaviour                                                                                              |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| Crash                                     | Halts, but is working correctly until it halts                                                                                 |
| Omission:<br> Receive<br> Send            | Fails to respond to incoming requests<br>Fails to receive incoming messages<br>Fails to send messages (response)               |
| Timing                                    | Response lies outside a specified time interval (timeout)                                                                      |
| Performance                               | Server responds too late.                                                                                                      |
| Response<br> Value<br> State-transition   | Response is incorrect e.g. 2+2=5<br>The value of the response is wrong<br>Deviates from correct flow of control (reacts wrong) |
| Arbitrary (or Byzantine)<br>**(Serious)** | May produce arbitrary responses at arbitrary<br>times that is not detected as incorrect. False info.                           |
#### Dependability vs Security (Omission versus Commission)
Arbitrary failures are sometimes qualified as malicious. 
- *Omission* failures: a component *fails to take an action* that it *should have* taken
- *Commission* failure: a component *takes an action* that it *should not have* taken. Malicious (Byzantine) failures are often commission failures.
These deliberate failures are often *security* problems. Distinguishing between *deliberate* failures and *unintentional* ones is difficult.
#### Failure Masking by Redundancy
If a sys is to be fault tolerant, the best it can do is to try to hide the occurrence of failures from other processes. The key technique for masking faults is to use redundancy:
- **Information** redundancy: Add extra bits to data units so that errors can be recovered when bits are garbled e.g., Hamming code
- **Time** redundancy: Design a sys where actions can be performed again if anything went wrong e.g., retransmission request to a server when lacking an expected response
- **Physical** redundancy: add equipment or processes in order to allow one or more components to fail. Often used in DS e.g., extra processes are added to a sys so that the sys can still function correctly if processes crash.
### Process resilience by Groups
FT can actually be achieved in DS by *protecting* against *process failures*, which is achieved by **replicating processes** into groups. 

Organise several identical processes into a group. For all groups: when a message is sent to the group itself, all members of the group receive it. In this way, if one process in a group fails, hopefully some other process can take over for it.

Process groups may be dynamic. New groups can be created and old groups can be destroyed. A process can join a group or leave one during sys operation. A process can be a member of several groups at the same time. Consequently, mechanisms are needed for managing groups and group membership.
#### Groups org
![[process-groups.png\|300]]
**(FLAT GROUP)** All processes are equal (symmetric). More robust (no single point of failure) but decision-making (e.g., voting) is slower and more complex.
**(HIERARCHICAL GROUP)** One process acts as the coordinator (leader). Efficient for decision-making but the coordinator is a single point of failure; if it crashes, a new leader must be elected.
#### Groups and Failure Masking
A sys is **k-fault tolerant** if it can survive `k` component failures and still meet its specification. The required group size depends on the failure type:

- For **Halting or Crash/Omission Failures (fail-stop):** You need **k + 1** processes. If `k` fail, the one remaining correct process provides the answer.
- For **Arbitrary (Byzantine) Failures:** You need at least **2k + 1** processes. In the worst case, `k` malicious processes could send the same bad answer. You need a majority (`k+1`) of correct processes to out-vote them and reach a consensus.
### Consensus with crash failures
For a replicated service to work correctly, all non-faulty replicas must **agree (reach consensus)** on the order and content of client commands. With only crash failures, this is simpler.

In a **fault-tolerant process group**, each nonfaulty process executes the same commands, in the same order, as every other nonfaulty process
#### Crash Failures: Flooding-based Consensus
A simple algorithm where processes exchange lists of proposed commands in rounds. Each process merges received lists and uses a **deterministic function** (same for all) to select the next command to execute.  
![[crash-ex-1.png\|400]]This approach works as long as processes do not fail. Problems start when a process $P_i$ detects, during round $r$, that, say process $P_k$ has crashed. 
- **Challenge:** If a process crashes mid-round, some members may have received its proposal and others not. Processes must wait an extra round to ensure they all have the same information before deciding, ensuring consistency.

**(EXAMPLE)** assume we have a process group of four processes {P1, . . . , P4} and that $P_1$ crashes during round $r$. Also, assume that $P_2$ receives the list of proposed commands from $P_1$ before it crashes, but that $P_3$ and $P_4$ do not (in other words, $P_1$ crashes before it got
a chance to send its list to $P_3$ and $P_4$).
![[crash-ex-1.png\|400]]
- P3 may have detected that P1 crashed, but does not know if P2 received anything, i.e., P3 cannot know if it has the same info as P2 -> cannot make decision (same holds for P4)
- P3 and P4 postpone decision to next round
- P2 makes a decision and broadcast that decision to the others
- P3 and P4 are able to make a decision at round (r+1)
- P3 and P4 decide to execute the same command selected by P2
#### Raft Consensus Algorithm
**A popular, understandable algorithm for managing a replicated log.
- Uses a fairly straightforward **leader-election** algorithm. The current leader operates during the **current term**.
- Every server (typically, five) keeps a **log** of operations, some of which have been committed. *A backup will not vote for a new leader if its own log is more up to date*.
- All committed operations have the same position in the log of each respective server.
- The leader decides which pending op to commit next ⇒ a **primary-backup approach**.
- 
**When submitting an operation**
- A client submits a request for operation $o$
- The leader appends the request $⟨o, t.k⟩$ to its own log (registering the current term $t$ and length of ), $k$ is the index of o in the leader’s log
- The log is (conceptually) broadcast to the other servers
- The others (conceptually) copy the log and acknowledge the receipt
- When a majority of ACKs arrives, the leader commits $o$.

**(EXAMPLE)**
1. A **leader** is elected for a **term**. All client commands go to the leader.
2. The leader appends the command to its log and replicates it to **followers**.
3. Once a **majority** of followers acknowledge, leader **commits** the entry and notifies them.
4. This ensures all servers have the same log of committed commands.  
![[leader-crashes-raft.png\|300]]
- **Leader Failure:** If the leader crashes, a new election is held. The new leader must have the most up-to-date log to ensure consistency. Any missing commits will eventually be sent to the other backups.

(**Note**) In practice, only updates are *broadcast*
- At the end, every server has the *same view* and knows about the $c$ committed operations
- Effectively, any information at the backups is *overwritten*
#### The Two Generals Problem
This thought experiment illustrates the fundamental difficulty of achieving **perfect reliability** over an unreliable communication channel.  
![[two-generals-problem.png\|200]]

- **Scenario:** Two armies must coordinate an attack by sending messengers through enemy lines. Messengers can be captured.
- **Dilemma:** No matter how many acknowledgements are sent, the sender can never be _100% certain_ the final acknowledgement was received, creating an infinite regress of uncertainty.
- **Implication:** In practice, DSs use timeouts and probabilistic guarantees (e.g., TCP retransmission) rather than seeking perfect certainty.
**How Should the Generals Decide?**
a) General 1 always attacks, even if no response is received?
- Send lots of messengers to increase probability that one will get through
- If all are captured, general 2 does not know about the attack, so general 1 loses
b) General 1 only attacks if positive response from general 2 is received?
- Now general 1 is safe
- But general 2 knows that general 1 will only attack if general 2's response gets through
- Now general 2 is in the same situation as general 1 in option 1
No common knowledge: the only way of knowing something is to communicate it.
### The Byzantine Generals Problem
This is the consensus problem in the presence of **arbitrary (Byzantine) failures**, where components may behave maliciously and send conflicting information.  
![[byzantine=generals-problem.png\|200]]

- **Goal:** All loyal (non-faulty) generals must agree on a common plan (e.g., Attack or Retreat) despite the presence of `f` traitors who may lie and send contradictory messages.
- **Famous Result (Lamport et al.):** To tolerate `f` Byzantine failures, the sys must have **at least `3f + 1`** total components. This means **less than one-third** can be faulty. Cryptography (digital signatures) helps but doesn't change this fundamental bound.
### Consensus with arbitrary failures
For replicas only subject to *crash failures*, a process group needs to consist of $2k + 1$ servers to survive $k$ crashed members. Here, we assume that a process does not collude with another process and that it is consistent in its messages to others. 

Now, we look at reaching consensus in a fault-tolerant process group in which k members can fail assuming arbitrary failures. We need at least $3k + 1$ members to reach consensus under these failure assumptions.

We consider process groups in which communication between processes is **inconsistent**.
![[arb-consensus-1.png\|200]]
**System model**
• Process group consisting of n members of which one is designated to be the *primary* $P$ and $n - 1$ *backups* $B1, …Bn-1$.
• A client sends a value $v \in {T\text{rue}, F\text{alse}}$ to $P$
• Messages may be lost, but this can be detected
• Messages cannot be corrupted beyond detection
• A receiver of a message can reliably detect its sender.
#### Byzantine Agreement: Requirements
BA1: Every nonfaulty backup process stores the same value
BA2: If the primary is nonfaulty then every nonfaulty backup process stores exactly what the primary had sent.
1. Primary is faulty -> BA1 says that backups may store the same, but different (and thus wrong) value than originally sent by the client.
2. Primary is not faulty -> satisfying BA2 implies that BA1 is satisfied 
#### Why having 3k processes is not enough
**(EXAMPLE)** consider the situation of tolerating the failure of a single process, that is, $k = 1$.
![[arb-consensus-2.png.png\|400]]
*Solution*: $3k+1$
- **Example (Why 4 is needed for f=1):** With 3 generals (1 faulty), the faulty one can tell different lies to the two loyal ones, leaving each loyal general with a different view and no way to determine the truth, as they cannot form a majority (2 out of 3) for a single value.  
![[arb-consensus-3.png.png.png\|400]]
- **Solution (With 4 for f=1):** With 4 generals (1 faulty), the 3 loyal generals can exchange messages. Even if the traitor tells different stories, the 3 loyal ones can compare notes and agree on the majority value, achieving consensus.
#### System Models
In reality, failures are complex, both nodes and networks may be faulty!. sys designers make explicit **assumptions (models)** about:
- **Network Behaviour:** Can messages be lost, duplicated, reordered? (e.g., Two Generals).
- **Node Behaviour:** Do nodes only crash, or can they be Byzantine?
- **Timing Behaviour:** Are there bounds on message delay (latency) and processing speed (synchronous) or not (asynchronous)?  
The chosen models for these three aspects define what is possible (e.g., consensus is solvable in a synchronous sys with crash failures, but much harder in an asynchronous sys with Byzantine failures).

**Check of Understanding**
> **Question:** You are designing a new blockchain that uses a Proof-of-Stake consensus mechanism. The network must tolerate up to `f` validators acting maliciously (Byzantine failures) and also handle validators going offline (crash failures). Using the principles from Byzantine fault tolerance, what is the _minimum_ fraction of the total stake that must be controlled by honest (non-Byzantine) validators for the network to guarantee safety (i.e., prevent conflicting blocks from being finalised)? Explain the reasoning using the concepts of `k-fault tolerance` and the Byzantine Generals bound.
> **Answer:** The honest validators must control **more than two-thirds ( > 2/3 )** of the total stake.
> **Reasoning:** This is a direct app of the Byzantine Generals solution. To tolerate `f` Byzantine failures, the sys needs **`3f + 1`** total participants (or stake-weight equivalents). Therefore, the number of honest participants needed is **`3f + 1 - f = 2f + 1`**.
> 	The _fraction_ of honest stake is therefore `(2f + 1) / (3f + 1)`. As `f` increases, this 
> fraction approaches (but always remains greater than) `2/3`. For any `f`, `(2f+1)/(3f+1) > 2/3`. This ensures that the honest nodes (controlling $>2/3$ of the stake) always have an overwhelming majority (`2f+1`) over the colluding malicious nodes (`f`), allowing them to out-vote any conflicting proposals and maintain a single, agreed-upon chain. 
> 	If honest stake fell to $2/3$ or below, malicious validators could potentially form a large 
> enough coalition (`f` could be >= `n/3`) to violate the `3f+1` bound and break consensus.
## 15. Cloud Computing
### Technology Landscape
Cloud computing is envisioned as delivering computing resources as a utility, similar to the power grid. This evolution is driven by exponential growth in processing power (exascale supercomputers), network bandwidth (terabit speeds), and storage density.

**(EVOLUTION OF DISTRIBUTED COMPUTING)** This marks the fourth major phase:
1. **Linking Machines:** The Internet (TCP/IP).
2. **Linking Documents:** The World Wide Web (HTTP, HTML).
3. **Linking Applications:** Web Services & SOA (REST).
4. **Linking Everything as a Utility:** **Cloud Computing**, enabled by **virtualisation**.
### Towards a Definition of Cloud Computing
A cloud is a massive pool of **virtualised** resources (compute, storage, network) that are:
* **On-Demand:** Provisioned automatically by users.
* **Elastic:** Can be scaled up or down dynamically to match workload.
* **Pay-Per-Use:** Billed based on consumption.
* **Broadly Accessible:** Accessed over the network via standard mechanisms.

**(CLOUD DATA CENTERS)** These are the physical foundation: warehouses containing tens to hundreds of thousands of tightly coupled servers, managed to run multiple massive-scale applications (e.g., Amazon, Google, Microsoft). Key challenges include load balancing, data management, and fault tolerance at an immense scale.
### Virtualised infrastructures
Virtualisation is the core technology enabling cloud computing. It abstracts physical hardware (CPUs, memory, storage) to create multiple, isolated virtual machines (VMs) or containers on a single physical server.
**(BENEFITS)**
* **Server Consolidation:** Run multiple workloads on fewer physical machines, reducing cost and complexity.
* **Hardware Independence:** Applications are decoupled from specific hardware, avoiding vendor lock-in.
* **Simplified Management:** Enables automation of provisioning, scaling, and recovery.
### Conceptual Cloud Architecture
Cloud architecture is commonly viewed through two complementary models: a layered service model and a conceptual stack.
![[concept-cloud-vision.png\|300]]*The cloud provides a simplified, service-oriented interface to complex, pooled resources.*
![[layered-cloud-arch.png\|300]]*The stack shows how virtualisation and management software abstract the physical infrastructure.*
### Taxonomy of cloud Models
Cloud services are offered in three main layers, forming a stack where each higher layer builds upon the capabilities of the layer below.
#### **Infrastructure as a Service (IaaS)**
* **What it is:** Provides fundamental computing resources as virtualised services. Consumers get raw VMs, storage, and networks.
* **Consumer Control:** Manages OS, storage, deployed applications, and some networking components.
* **Provider Responsibility:** Physical hardware, hypervisor, and core network.
* **Example:** **Amazon EC2**, where you rent virtual servers by the hour.
#### **Platform as a Service (PaaS)**
* **What it is:** Provides a complete software development and deployment environment in the cloud.
* **Consumer Control:** Deploys and manages *applications* using provider-supported programming languages, libraries, and tools.
* **Provider Responsibility:** Underlying OS, runtime, middleware, and infrastructure.
* **Example:** **Microsoft Azure App Services**, **Google App Engine**.
#### **Software as a Service (SaaS)**
* **What it is:** Delivers complete, ready-to-use application software over the internet.
* **Consumer Control:** Only application configuration and user data.
* **Provider Responsibility:** Everything: application, data, runtime, infrastructure.
* **Example:** **Google Workspace (Gmail, Docs)**, **Salesforce**.

**(TYPICAL CLOUD ARCHITECTURE)** The physical realisation involves massive clusters of commodity servers, interconnected via high-speed switches and housed in warehouse-scale data centers.
![[typical-cloud-arch.png\|400]]
### Virtual Infrastructure Managers (VIMs)
Managing thousands of VMs across a data center requires sophisticated orchestration software—a **Virtual Infrastructure Manager (VIM)**.

**(ROLE OF A VIM)** A VIM provides a uniform view of the resource pool and automates the **lifecycle management** of VMs: scheduling (where to place a VM), provisioning (cloning from templates), networking (assigning IPs), and monitoring.

**(OPENSTACK EXAMPLE)** A prominent open-source VIM that controls large pools of compute, storage, and networking resources.
![[openstack-ex.png\|200]]
* **Function:** Provides APIs and dashboards for administrators and users to provision and manage cloud resources. It integrates components for compute (Nova), networking (Neutron), storage (Cinder, Swift), and identity (Keystone).
### Cloud services
Beyond traditional Virtual Machines, **container-based virtualisation** has become central to cloud-native application development.
**(VIRTUAL MACHINES VS. CONTAINERS)**

| Virtual Machine          | Container                       |
| ------------------------ | ------------------------------- |
| ![[vm-arch-ex.png\|200]] | ![[container-arch-ex.png\|200]] |
- **Virtual Machine:** Virtualises the entire hardware stack, requiring a full guest OS. Higher isolation but more overhead.
* **Container:** Virtualises the OS, sharing the host kernel. Multiple isolated user-space instances run on a single OS. More lightweight and efficient than VMs.

**(THE CASE OF CONTAINERS & MICROSERVICES)** Containers are the ideal packaging for **microservices**—small, independent services that make up an application. An application is a collection of containers, each running a single microservice.
* **Orchestration Need:** Managing hundreds of interconnected containers requires an orchestrator like **Kubernetes**.
![[container-kubernetes-ex.png\|200]]
* **Kubernetes Role:** Automates deployment, scaling (load balancing), and management of containerised applications. It handles scheduling containers onto nodes, health monitoring, and recovery from failures.
### Private, Public, and Hybrid Clouds
Deployment models define where the cloud infrastructure is located and who manages it.
![[public-private-hybrid-clouds.png\|400]]
* **Public Cloud:** Owned and operated by a third-party cloud provider (e.g., AWS, Azure, GCP). Resources are shared among multiple organisations (multi-tenant). Offers the greatest elasticity and shifts capital expenditure to operational expenditure (pay-as-you-go).
* **Private Cloud:** Cloud infrastructure operated solely for a single organisation. It may be managed by the organisation or a third party and can be located on-premises or off-premises. Offers greater control and security but requires higher capital investment and management overhead.
* **Hybrid Cloud:** A composition of two or more distinct cloud infrastructures (private, public) that remain unique entities but are bound together by standardised technology. This allows data and applications to be shared, enabling **cloud bursting** (using public cloud to handle spikes in demand) and flexible workload placement.

**Check of Understanding**
> **Question:** A start-up is building a new mobile app. They need to store user profiles and posts, run backend application logic, and want to use a machine learning API for content recommendations. They have a small team and want to minimise operational overhead. Using the cloud service models (IaaS/PaaS/SaaS), recommend a specific approach for each of their three needs and justify why it fits the start-up's constraints.
> **Answer:**
> 1.  **User Data Storage:** Use a **SaaS** database service like **Amazon DynamoDB** or **Google Firestore**. The start-up avoids managing database servers, scaling, backups, and patching. They just configure the data model and use an API, perfectly matching their need to minimise overhead.
> 2.  **Backend Application Logic:** Use a **PaaS** offering like **AWS Elastic Beanstalk** or **Google App Engine**. They can simply upload their application code (e.g., in Python/Node.js). The PaaS provider automatically handles the deployment, scaling, load balancing, and runtime management. This is ideal for a small team wanting to focus on code, not infrastructure.
> 3.  **ML Recommendations:** Use a **SaaS** AI service like **Google Cloud Vision API** or **Amazon Rekognition**. They don't need to build, train, or host ML models. They call a pre-built API, which is the fastest, lowest-overhead way to add advanced functionality.
## 16. Distributed Systems Topics and Trends Pt.1
Clouds are the foundational platform for modern and future internet services. As the internet expands beyond people and computers to include everyday objects, a new paradigm emerges—the **Internet of Things (IoT)**. IoT applications generate vast data streams, which must be processed and stored dynamically, creating a symbiotic relationship with cloud computing.
### IoT: the Internet of Things
The **Internet of Things (IoT)** is a sys where physical objects ("things") embedded with sensors, software, and network connectivity collect and exchange data over the internet. Kevin Ashton coined the term in 1999. Predictions indicate tens of billions of connected devices, generating data on the scale of **zettabytes**.

**(INTEGRATING A DEVICE INTO IoT - THE FRIDGE EXAMPLE)** Transforming an appliance like a fridge into an IoT device requires two core components:
1.  **Computational Intelligence:** Local processing capability to interpret sensor data (e.g., cameras for inventory, weight sensors, timers for door alarms).
2.  **Network Connectivity:** The ability to send and receive data over a network, enabling remote features.

* **Local Features (Computation):** Suggesting recipes based on inventory, sounding an alarm if the door is left open.
* **Network-Enabled Features (Connectivity):** Sending an SMS when stock is low, autonomously ordering food from an online supermarket by interacting with web services, and managing payment.

**(EXAMPLE ARCHITECTURE)** A typical IoT architecture involves sensors/devices collecting data, communicating via a gateway (which may perform initial processing), and ultimately sending data to the cloud for storage and advanced analytics.
![[iot-arch.png\|300]]
### Edge Computing
The traditional "cloud-only" model faces significant challenges with IoT:
* **Data Volume & Velocity:** Sending all raw sensor data to the cloud creates massive **data bottlenecks** and network **congestion**.
* **Latency:** Many applications (e.g., autonomous vehicles, industrial automation) require **real-time responses** (sub-second to minute-level), which round-trip cloud latency cannot support.
* **Bandwidth Cost:** Transmitting constant streams of raw data is expensive.

**(THE CLOUD TO EDGE SHIFT)** To solve this, **Edge Computing** moves computation and data storage **closer to the location where it is needed**, i.e., near the IoT devices at the "edge" of the network. This creates a multi-layered architecture:
* **Cloud Computing:** Centralised, data-center based, for heavy batch processing, long-term storage, and complex analytics.
* **Edge Computing:** Geographically distributed servers (e.g., base stations, micro-data centers) that handle time-sensitive processing, data filtering, and local decision-making.
![[network-edge-computing.png\|300]]
**(EXAMPLE - DRIVERLESS CARS)** An autonomous vehicle exemplifies this hybrid model. It requires:
1.  **Local (Edge) Processing:** Immediate sensor fusion (camera, LiDAR) for real-time obstacle avoidance and navigation.
2.  **Cloud Processing:** Downloading updated high-definition maps, running complex simulations for route optimization, and performing deep learning model training.
### Revisiting the Cloud Computing Stack
The classic three-layer cloud stack (IaaS, PaaS, SaaS) is being extended "downwards" to incorporate the edge and IoT layers, forming a **continuum of computing**.
![[cloud-arch-revisited.png\|400]]
This stack now includes:
* **Sensors/Actuators Layer (Things):** The physical IoT devices.
* **Edge Computing Layer:** Provides local compute, storage, and networking. It acts as a first line of data processing, performing monitoring, analysis, reduction (filtering), and caching before sending relevant data upstream.
### Vision for a (near) future
The future points toward an **edge-driven**, **intelligent** distributed systems paradigm.

**(EDGE-DRIVEN EXASCALE)** Next-generation **exascale supercomputing** (capable of a quintillion calculations per second) will be tightly integrated with edge computing. The edge acts as a **filter and pre-processor**, transforming I/O-bound data problems into compute-bound problems for the supercomputer by sending only critical, refined data.
* **Efficiency:** It's more efficient to analyse data at the edge and transmit only anomalies or valuable summaries for deep analysis.

**(INTELLIGENT DISTRIBUTED SYSTEMS)** The progression is clear: the demand from new **Disruptive Applications** (IoT, AI, real-time analytics) drives the evolution of the **Supporting Infrastructure**. This infrastructure is moving towards massive **Exascale/Zetascale** centralised compute, deeply integrated with a pervasive, intelligent edge, ultimately forming **Intelligent Distributed Systems** that are adaptive, responsive, and efficient.

**Check of Understanding**
> **Question:** A city deploys a smart traffic management sys with hundreds of cameras and sensors at intersections to optimise traffic flow and detect accidents. Using the concepts of IoT, Edge, and Cloud computing, describe a feasible three-tier architecture for this system. Specify what type of processing or action should logically happen at each tier (Sensor/Edge/Cloud) and justify your design based on the core challenges each tier solves.
>
> **Answer:** A feasible three-tier architecture would be:
> 1.  **Sensor/Traffic Intersection Tier (IoT Layer):**
>  * **Action:** Cameras and sensors **collect raw data** (vehicle count, speed, images).
>  * **Justification:** This is the source of the IoT data. The devices have minimal processing power and are geographically distributed at each intersection.
> 1.  **District Edge Server Tier (Edge Computing Layer):**
>  * **Action:** A server located in each city district performs **real-time, low-latency processing**. It analyses video feeds to **detect accidents or congestion immediately**, changes local traffic light patterns in response, and **filters/aggregates** data (e.g., converting 24/7 video into "congestion level: high" summaries).
>  * **Justification:** This solves the **latency** problem (lights must change in seconds), reduces **data volume** sent to the cloud (sends summaries, not raw video), and handles **bandwidth congestion**. It enables immediate, autonomous action.
> 1.  **City Cloud Data Center Tier (Cloud Layer):**
>  * **Action:** Receives aggregated data from all edge servers. It runs **long-term analytics and machine learning** to identify city-wide traffic patterns, optimise signal timing plans for daily routines, and predict future congestion hotspots. It also provides the **central management dashboard** for city planners.
>  * **Justification:** The cloud provides the **massive storage** and **heavy computational power** needed for batch analytics and model training that is not time-critical. It offers a **global, consolidated view** impossible to achieve at the edge alone.
## 17. Distributed Systems Topics and Trends Pt.2
### Module themes
This module covered the foundational principles of Distributed Systems (DS), which can be organised into four key parts:

**Part 1 - Foundations**
* **Concepts:** Definition of DS, the role of **Middleware**.
* **Structure:** Architectural Styles (layered, object-based, microservices), sys Architectures (centralised, decentralised, hybrid).
* **Communication & Principles:** Models (RPC, Message-Oriented), **Transparency** goals, and **Openness** through standard interfaces.

**Part 2 – Service Orientation**
* **Evolution:** From monolithic systems to **Service-Oriented Architectures (SOA)**.
* **Implementation:** **Web Services** and the **REST** architectural style.
* **Modern Paradigms:** Decomposition into **Microservices**, and the event-driven, resource-efficient model of **Serverless Computing** and **Function as a Service (FaaS)**.

**Part 3 – Distributed Systems Support**
* **Naming & Discovery:** **Flat** and **structured naming**, **Directory Services** (e.g., LDAP).
* **Coordination:** **Clock synchronisation** algorithms (Cristian's, Berkeley, NTP), and **election algorithms** (Bully, Ring).
* **Consistency & Resilience:** **Consistency models** (sequential, causal, client-centric), **replication** strategies, and **Fault Tolerance** techniques (process groups, consensus with Byzantine failures).

**Part 4 – Use Cases**
* **Modern Platforms:** **Cloud Computing** service models (IaaS, PaaS, SaaS), **Edge Computing**.
* **Applications:** The **Internet of Things (IoT)** and emerging **Disruptive Applications** driving sys evolution.

### Evolution of distributed computing
Distributed systems are scaling to unprecedented levels in both **computational power** and **user capacity**.

**(SCALABILITY IN COMPUTE - EXASCALE)** Systems like the **El Capitan** supercomputer (1.742 exaflops) represent the pinnacle of tightly-coupled, high-performance distributed computing, built from millions of coordinated CPU/GPU cores.

**(SCALABILITY IN USERS & DATA - INTERNET SCALE)** Modern platforms serve billions of users and process exabytes of data daily:
* **YouTube:** 1 billion hours of video watched/day.
* **Google:** 8.5 billion searches/day.
* **Data Generation:** ~400 exabytes of new data generated globally *each day*.

These scales necessitate the foundational principles covered in the module: replication for performance, consistency trade-offs, and fault-tolerant designs.

### Most Active Topics in Distributed Systems
Current research and development are shaped by major trends in cloud/edge computing, AI, security, and sustainability.
**(1-Edge-Cloud Continuum)**
The rigid separation between cloud and edge is blurring into a **continuum of compute resources**. This is critical for IoT and latency-sensitive applications (autonomous vehicles, AR/VR).
![[edge-cloud-continuum.png\|400]]
* **Key Requirement:** **Distributed orchestration** to seamlessly deploy and manage applications across this heterogeneous landscape, from central cloud to far edge devices.

**(2-Intelligent Distributed Systems)** A two-way relationship is defining the future:
* **AI for Systems:** Using **Large Language Models (LLMs)** and **agentic AI** to automate and optimise the DS itself—e.g., for intelligent scheduling, anomaly detection, automatic scaling, and self-repair.
* **Systems for AI:** Designing new distributed architectures (specialised hardware like TPUs, high-bandwidth networks) to support the massive computational demands of **distributed AI model training and inference**.
![[intelligent-ds.png\|200]]
**(3-Serverless and Event-Driven Architectures)**
**Serverless/FaaS** is evolving beyond its initial stateless constraints.
* **Active Issues:** Managing **stateful serverless functions**, eliminating **cold-start latency**, and achieving portability across different cloud providers.
* **Emerging Solution - WebAssembly (WASM):** A portable, lightweight binary format that executes safely at near-native speed. Its **fast startup** and **small footprint** make it ideal for secure, efficient workload deployment across the edge-cloud continuum.

**(4-Reliability, Consistency, and Correctness at Scale)**
Ensuring correctness in massively scalable, highly available systems remains a core challenge.
* **Beyond Traditional Consensus:** New data types like **Conflict-free Replicated Data Types (CRDTs)** allow replicas to be updated independently and concurrently without coordination, guaranteeing automatic convergence—ideal for collaborative applications.
* **Verification:** Developing techniques to formally verify the correctness of complex, interacting microservices and multi-agent systems.

**(5-Secure Large Scale Distributed Systems)**
Decentralisation and complexity introduce profound security challenges.
* **Distributed Identity:** Managing access for autonomous **agentic AI** workloads across domains.
* **Confidential Computing:** Using hardware-based **Trusted Execution Environments (TEEs)** to process encrypted data in memory, protecting it even from the cloud provider, which is crucial for sensitive workloads in shared environments.

**(6-Sustainability?)** Energy footprint of distributed computing is massive and cannot be ignored.
* **The Scale of the Problem:** Streaming a single popular music video since 2016 may have consumed over **51 TWh**—more than the annual electricity consumption of a country like Morocco.
* **Energy-Efficient Solutions:** Research focuses on optimisation at every layer:
  * **Hardware:** Dynamic Voltage and Frequency Scaling (DVFS).
  * **Data Center & Virtualisation:** Consolidating VMs, turning off idle servers.
  * **Software & Protocols:** Energy-aware network protocols and application design.

**Check of Understanding**
> **Question:** A company is building a new global collaborative document editor (like Google Docs). They require high availability, real-time co-editing with minimal lag, and must work even with intermittent user connectivity. Based on the trends discussed, recommend **two specific technical approaches** from different active topic areas (e.g., 1-Edge-Cloud, 4-Consistency, 6-Sustainability) that would be particularly suitable for this application. For each, briefly explain *how* it addresses a core requirement and *why* it's a good fit compared to a more traditional alternative.
> **Answer:**
> 1.  **Technical Approach from Topic 4 (Consistency): Use Conflict-free Replicated Data Types (CRDTs).**
> * **How it helps:** CRDTs allow each user's client to edit its local replica of the document independently, even offline. When connectivity is restored, the CRDT's mathematical properties guarantee that all changes will merge **automatically and consistently** without complex conflict resolution protocols.
> * **Why it's a good fit vs. Traditional:** A traditional approach using a primary server with locking (e.g., remote-write protocol) would create high latency (waiting for server acknowledgement) and fail completely during network partitions. CRDTs provide the **high availability** and **seamless offline operation** that a collaborative editor needs.
> 2.  **Technical Approach from Topic 1 (Edge-Cloud): Deploy using an Edge-Cloud continuum with WebAssembly (WASM).**
> * **How it helps:** The core editing logic (including the CRDT merge operations) could be packaged as a lightweight **WASM module**. This module can run consistently and securely on a user's local device (far edge for zero-latency), on a nearby edge server (for session persistence), or in the cloud (for heavy indexing). This enables **low-latency** editing and efficient use of resources.
> * **Why it's a good fit vs. Traditional:** A traditional thick client app is heavy and platform-dependent, while a pure cloud JS app suffers from latency and offline limitations. WASM on the edge-cloud continuum offers a **portable, fast-starting, and efficient** execution environment that can be dynamically placed to optimise for both performance and resource usage, contributing to **sustainability (Topic 6)** by reducing unnecessary data center load.