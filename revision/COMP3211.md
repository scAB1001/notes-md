# Structure
### Course Outline
#### Part 1
- Architectures
- Middleware
- Distributed object technology, communication
- Service Oriented Architectures and Web services (REST)
- Microservices and nanoservices
- Serverless architectures
#### Part 2
- Supporting services: naming, directory and discovery services, synchronisation, consistency, replication, fault- tolerance
#### Part 3
- Cloud computing, Virtualisation, Edge computing
## Part 1
### 1. Introduction to Distributed Systems Pt.1 - *Chapter 1*
- Definition of a DS
- Goals and challenges
- Sharing of resources
- Transparency
- Openness
- Scalability
### 2. Introduction to Distributed Systems Pt.2 - *Chapter 1* 
- Types of distributed systems
1. High performance distributed computing systems
2. Distributed information systems
3. Distributed systems for pervasive computing
- *Homework*: What is Google spanner? 
	- What consistency properties does it have?
	- How does it handle ACID transactions?
### 3.  Architectures - *Chapter 2*
- Understand the different ways on how to view the organisation of a distributed system
- Architectural styles
	- Layered
	- Object-based
	- Resource-centred
	- Event-based
- System architecture
	- Centralised
	- Decentralised
	- Hybrid
### 4. Communication - *Chapter 4*
- Foundations
	- Latency and Bandwidth
	- Layered Protocols
	- Types of communication
		- Synchronous vs asynchronous
		- Transience vs persistence
- Remote Procedure Call
- Message Oriented Middleware
	- Message oriented communication
	- Message Passing Interface
	- Message Queuing Model
### 5. Service Oriented Architectures
- Conceptual Design of Software Systems
- Architectures
	- 1-Tier
	- 2-Tier
	- 3-Tier
	- N-Tier
- Emergence of SOAs
- Vision
- Characteristics
### 6. Web Services and REST
- Why do we need Web services?
- What are web services?
- What is REST?
- What does it consist of?
- Claimed benefits
- HTTP
### 7. Programming RESTful Web Services
- REST: Quick Recap
- REST APIs: Examples
- Reference implementations:
	- Python (Flask Restful)
	- Java (Jersey)
- Data encoding and RPC
### 8. Microservices, Nanoservices and Serverless
- Recap: SOAs
- Microservices
- Nanoservices
- Serverless Computing
- Function as a Service
- Architectural Support
- Solutions
## Part 2
### 9. Naming Pt.1 - *Chapter 5*
- Fundamental concepts
- Classes of naming systems
- Namespaces
- Naming graphs
### 10. Naming Pt.2 - *Chapter 5*
- Structured naming
	- Namespaces
	- Name resolution
	- Implementation of a namespace
- Attribute-based naming
- Directory Services
### 11. Timing and Synchronisation - *Chapter 6*
- Synchronisation in a DS
- Internal and external physical clocks
- Clock synchronisation algorithms
- Election Algorithm
- Network time protocol (NTP)
### 12. Consistency and Replication Pt.1 - *Chapter 7*
- Data-centric consistency models
### 13. Consistency and Replication Pt.1 - *Chapter 7*
- Client-centric consistency models
- Replica management
- Consistency protocols
### 14. Fault Tolerance - *Chapter 8*
- Dependability, reliability and availability in a DS
- Terminology
- Failure models
- Process resilience
- Consensus with crash failures
- Consensus with arbitrary failures
- The Byzantine Generals Problem
- *Homework*: The Paxos consensus algorithm is a protocol used in distributed systems to allow a group of computers to agree on a single value despite failures. How does Paxos compare to Raft?
## Part 3
### 15. Cloud Computing
- Technology Landscape
- Towards a Definition of Cloud Computing
- Virtualised infrastructures
- Conceptual Cloud Architecture
- Taxonomy of cloud Models
- Virtual Infrastructure Managers
- Cloud services
- Types of Clouds
### 16. Distributed Systems Topics and Trends Pt.1
- IoT: the Internet of Things
- Edge Computing
- Revisiting the Cloud Computing Stack
- Vision for a (near) future
### 17. Distributed Systems Topics and Trends Pt.2
- Module themes
- Evolution of distributed computing
- Most Active Topics in Distributed Systems-
# Content 
## 1. Introduction to Distributed Systems Pt.1 - *Chapter 1*
### Definition of a Distributed System
> (1) A collection of *autonomous computing elements* (nodes) that appears to its *users* as a *single coherent system*.

NODES: Hardware devices and software processes (e.g. computer, car, robot) that need to collaborate. An AUTONOMOUS node has its own *notion of time* as every node has its own clock. 
There is no GLOBAL clock which is needed for synchronisation.
An autonomous node also needs to communicate to other nodes, providing network support.

> (2) A system in which components located at *networked* computers *communicate* and coordinate their actions only by *passing messages*.

The collection of nodes as a whole operates the same –no matter *where, when or how* the interaction takes places between the user and that system.
For example: 
- An end-user cannot tell where the computation is taking place
- Where data is stored exactly should be irrelevant to an application
- Whether or not data has been replicated is completely unknown/hidden. (Distribution transparency)
#### Examples of Distributed Systems
- The internet, The World Wide Web, A cellular mobile phone network
- The cloud
Applications (apps) built on top of Distributed Systems (DSs) are called Distributed apps (DA): Netflix, Spotify, Instagram.
“You know you have a DS when the crash of a computer you’ve never heard of stops you from getting any work done” – Leslie Lamport
#### Distributed versus Decentralised Systems
There are two views of DS:
1. INTEGRATIVE view: connecting existing networked computer systems into a larger a system.
2. EXPANSIVE view: an existing networked computer system is extended with additional computers
![[centralised-decentralised-distributed-system.png]]
> A DISTRIBUTED system is a networked computer system in which processes and resources are **sufficiently** spread across multiple computers. (expansive view).

> A DECENTRALISED system is a networked computer system in which processes and resources are **necessarily** spread across multiple computers. (integrative view)

Here, data is normally brought to the high-performance computers that literally train models before they can be used. But when data needs to stay within the perimeter of an organisation (e.g. security reasons), training is brought to the data. The result is known as **federated learning**.
#### Examples of Decentralised and Distributed Systems
1. **Blockchain (distributed ledger)  decentralised system**
A distributed ledger, blockchain: we need to deal with the situation that participating parties do not trust each other enough to set up simple schemes for collaboration. 
	Instead, what they do is essentially make the transactions among each other fully public (and
 verifiable) by an extend-only ledger that keeps records of those transactions. The ledger itself is fully spread across the participants, and the participants are the ones who validate transactions (of others) before admitting them to the ledger. 
	The result is a decentralised system in which processes and resources are, indeed,
necessarily spread across multiple computers, in this case due to lack of trust.
2. **Geographically dispersed  decentralised system**
Consider systems that are naturally geographically dispersed. This occurs typically with systems in which an actual location needs to be monitored, for example, in the case of a power plant, a building, a specific natural environment, and so on. 
	The system, controlling the monitors and where decisions are made, may easily be placed
somewhere else than the location being monitored. 
	One obvious example is monitoring and controlling of satellites, but also more mundane 
situations as monitoring and controlling traffic, trains, etc. Here, the necessity for spreading processes and resources comes from a spatial argument.
3. **Content Delivery Networks (CDNs) DS**
The content of an actual Website, is copied and spread across various servers of the CDN. 
	When visiting a Website, the user is transparently redirected to a nearby server that holds all
or part of the content of that Website. A server is selected for which good performance in terms of latency and bandwidth can be guaranteed. 
	The CDN dynamically ensures that the selected server will have the required content readily 
available, as well as update that content when needed, or remove it from the server when there are no or very few users to service there. 
	Meanwhile, the user knows nothing about what is going on behind the scenes (which, again, 
is a form of distribution transparency). We also see that content is not copied to all servers, yet only to where it makes sense, that is, *sufficiently*, and for reasons of performance. CDNs also copy content to multiple servers to provide high levels of dependability.
4. **Network-Attached Storage (NAS) DS**
Consider a domestic-use setup based on a NAS, a typical NAS consists of 2–4 slots for internal hard disks. 
	The NAS operates as a file server: it is accessible through a (generally wireless) network for
any authorised device, and can offer services like shared storage, automated backups, streaming media, etc. 
	The NAS itself can best be seen as a single computer optimised for storing files, and offering 
the ability to easily share those files. The latter is important, and together with multiple users, we essentially have a setup of a distributed system. 
	The users will be working with a set of files that are locally (i.e., from their laptop) easily 
accessible (in fact, seemingly integrated into the local file system), while also directly accessible by and for other users. 
	Again, where and how the shared files are stored is hidden (i.e., the distribution is 
transparent). If file sharing is the goal, then we see that a NAS can provide *sufficient* spreading of processes and resources.
### Goals and challenges
A DS aims for: 1. Sharing of resources, 2. Distribution Transparency, 3. Openness, 4. Scalability,
A DS has many challenges:
• Architecture: common organisations, common styles
• Process: what kind of processes, and their relationships
• Communication: facilities for exchanging data
• Coordination: application-independent algorithms
• Naming: how do you identify resources?
• Consistency and replication: performance requires of data, which need to be the same
• Fault tolerance: keep running in the presence of partial failures
• Security: ensure authorised access to resources
### Sharing of resources
**Examples**: File sharing on P2P, shared web hosting, shared cloud-based storage
There are many reasons for wanting to share resources; its economically cheaper to have a single high-end reliable storage facility than having to buy and maintain storage for each user separately.
	Connecting users and resources also makes it easier to collaborate and exchange 
information, as is illustrated by the Internet with its simple protocols for exchanging files, mail, documents, audio, and video. 
	The connectivity of the Internet has allowed geographically widely dispersed groups of 
people to work together by all kinds of groupware, that is, software for collaborative editing, teleconferencing, and so on, as is illustrated by multinational software-development companies that have outsourced much of their code production to Asia.
### Transparency
> The phenomenon by which a DS attempts to *hide* the fact that its processes and resources are *physically distributed* across *multiple computers*, possibly separated by large distances.
	
This is handled through many different techniques in the MIDDLEWARE layer that sits between apps and OSs. E.g. Limited transparency: network services like sockets are directly visible to app dev.

Aiming for distribution transparency may be a nice goal when designing and implementing a DS, but that it should be considered together with other issues such as performance and comprehensibility. The price for achieving full transparency may be surprisingly high.
#### Middleware
![[middleware-layer-ex.png]]
Its the glue *between* apps and OSs, extending over multiple machines; contains commonly used components and functions that need not be implemented by apps separately.
The aim of the middleware is to hide heterogeneity of the underlying platforms from apps.
#### Types of Transparency
| Transparency  | Description                                                           |
| ------------- | --------------------------------------------------------------------- |
| _Access_      | Hide differences in data representation and how an object is accessed |
| _Location_    | Hide where an object is physically located in the system              |
| _Migration_   | Hide that an object may move to another location                      |
| _Relocation_  | Hide that an object may be moved to another location while in use     |
| _Replication_ | Hide that an object is replicated                                     |
| _Concurrency_ | Hide that an object may be shared by several competitive users        |
| _Failure_     | Hide the failure and recovery of an object                            |
### Openness
> An OPEN DS offers components that can easily be used by, or integrated into other systems. An open DS itself will often consist of components that originate from elsewhere.

![[middleware-openness-ex.png]]
They *share* the same *interface* and *communicate* with the same *common protocol* to be able to *interact with services from other open systems*, irrespective of the underlying environment.

Systems should conform to well-defined *interfaces*, easily *interoperate*, support *portability* of apps and be easily *extensible*.
### Scalability Issues
A DS can be scaled in size, geographically or administratively if it remains effective after scaling.
#### Scale in Size (number of users and/or processes)
Scalability for the internet was effortless because information is organised hierarchically rather than linearly: $O(\log(n))$. Poor scalability is when the cost of supporting $n$ users is worse than $O(n)$.
#### Scale Geographically (maximum distance between nodes)
1. DSs designed for LANs are hard to scale as they are based on *synchronous communication*.
Here, a party requesting service (*client*), *blocks* until a *reply* is sent back from the *server* implementing the service e.g., a database transaction. 
	Communication between two machines in a LAN: ~few hundred microseconds ($\mu s$)
However, the *interprocess communication* in a WAN may be hundreds of milliseconds ($m s$), three orders of magnitude ($10^3$) slower; this is a LATENCY problem.
2. Communication in WANs has *limited bandwidth* and is inherently much *less reliable* than LANs. 
In a home network, ensuring a stable, fast stream of high-quality video frames from a media server to a display is simple. If you place that same server far away with a standard TCP connection to the display, it will fail due to bandwidth limitations but also maintaining due to unreliable connection. Solutions developed for LANs are not easily *ported* to a WAN.
	Furthermore, WANs have very limited facilities for multipoint communication. Separate 
services, such as naming and directory services need to be developed for queries to be sent. These services must also be scalable, for which, no obvious solutions exist. TODO.
	In contrast, LANs often support *efficient broadcasting mechanisms* which are useful for 
discovering components and services, and are desired from a management point of view. 
#### Scale Administratively (no. administrative domains, e.g. Google data centres worldwide)
To scale a DS across multiple, independent administrative domains, solve the major problem of *conflicting policies* with respect to *resource usage* (and payment), *management*, and *security*. 
**(EXAMPLE 1)** scientists want to share usage of their expensive equipment in computational grid. 
- In these grids, a global DS is constructed as a federation of local DSs, allowing a program 
running on a computer at org A to directly access resources at org B. 
- Many components of a DS that reside within a single domain can be trusted by users that operate within that same domain. 
- In such cases, system administration may have tested and certified apps, taking special measures to ensure that such components cannot be tampered with.
- So, users trust their system administrators but this trust does not expand naturally across domain boundaries.
If a DS *expands to another domain*, two types of *security measures* need to be taken. 
**(1)** The DS has to *protect* itself against malicious *attacks* from the *new domain*. 
 - Users from the new domain may have only read access to the file system in its original domain. 
 - Facilities such as expensive image setters or high-performance computers may not be made available to unauthorised users. 
**(2)** The new domain has to protect itself against malicious attacks from the DS. 
 - E.g. downloading programs such as applets in Web browsers. Basically, the new domain does not know what to expect from such foreign code. These limitations are hard to enforce.

**(EXAMPLE 2)** consider developing a radio telescope where the final system is a federated DS.
- The telescope itself may be a wireless DS developed as a grid of a thousand sensor nodes, each collecting radio signals and collaborating with neighbouring nodes to filter out relevant events. 
	- The nodes dynamically maintain a *sink tree* by which selected events are *routed to a central point* for further analysis. 
	- The central point needs to be a reasonably powerful system, capable of storing and processing the events sent to it by the sensor nodes. 
- This system is *necessarily* placed in *proximity* of the sensor nodes, but is otherwise to be considered to operate independently. 
	- it may operate as a small local DS, storing all recorded events and offering access to remote systems owned by partners in the consortium. 
- Most partners have local DS (often a cluster of computers) that they use to further process the data collected by the telescope. 
	- In this case, the local systems directly access the central point at the telescope using a standard communication protocol. 
	- Naturally, many results produced within the consortium are made available to each partner.
	- So, the complete system will cross boundaries of several administrative domains, and that special measures are needed to ensure that data that only accessible to a specific consortium of partners. 

**(EXAMPLE 3)** DSs spanning multiple administrative domains that do not suffer from administrative scalability problems like file-sharing P2P networks. 
 - In these cases, end users simply install a program implementing distributed search and download functions and within minutes can start downloading files. 
	 - Other examples include P2P apps for telephony over the Internet such as Skype, and peer-assisted audio-streaming apps such as Spotify.
 - In these DSs, the end users -not administrative entities, collaborate to keep the system running. 
	 - At best, underlying administrative orgs such as Internet Service Providers (ISPs) can police the network traffic that these P2P systems cause, but such efforts are not very effective.
### Scaling Techniques
Scalability problems in DSs usually appear as performance problems caused by limited capacity of servers and/or networks. 
> SCALING UP is a solution of capacity improvement through memory increases, CPU upgrades, or network module replacement

> SCALING OUT is a solution of increasing machine deployment via hiding communication latencies, work distribution and replication.
#### (SOLUTION 1) Hiding Communication Latencies
This approach is about making better use of response times -applicable for geographic scalability.
The difference between letting:
	a) a server or
	b) a client check forms as they are being filled

**(EXAMPLE 1)** When a service has been requested at a remote machine, useful *work can be done at the requester’s side* while waiting for a reply. 
- To do this, the requesting application must uses only ASYNCHRONOUS communication. 
- When a reply arrives, the application is interrupted and a special handler is called to complete the previously issued request. 
- Asynchronous communication can be used in batch-processing systems and parallel apps where independent tasks can be scheduled for execution while another task is waiting for the communication to complete. 
	- Alternatively, a new thread of control can be started to perform the request. 
	- Although, it blocks waiting for the reply, other threads in the process can continue.

**(EXAMPLE 2)** Many apps that cannot make effective use of asynchronous communication. 
When a user sends a request in an interactive app, they have nothing better to do than to wait. 
- Here, a better solution is to *reduce the overall communication*, for example, by *moving part of the computation* that is normally done at the *server* to the *client process requesting the service*. 
A typical case where this approach works is accessing databases using forms. 
- Filling in forms can be done by sending a separate message for each field and awaiting confirmation from the server; the server may check for syntax errors before accepting an entry.
- A better solution is to ship the code for filling in the form, and possibly checking the entries, to the client, and have the client return a completed form. 
 - This approach is widely supported by the Web by means of Java applets and JS.

#### (SOLUTION 2) Partitioning and Distribution (Domain Name Lookup & Addressing)
#### (SOLUTION 3) Replication
