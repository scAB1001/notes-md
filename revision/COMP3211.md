# Content 
## 1. Introduction to Distributed Systems Pt.1 - *Chapter 1*
### Definition of a Distributed System (DS)
> (1) A collection of *autonomous computing elements* (nodes) that appears to its *users* as a *single coherent sys*.

NODES: Hardware devices and software processes (e.g. computer, car, robot) that need to collaborate. An AUTONOMOUS node has its own *notion of time* as every node has its own clock. 
There is no GLOBAL clock which is needed for synchronisation.
An autonomous node also needs to communicate to other nodes, providing network support.

> (2) A sys in which components located at *networked* computers *communicate* and coordinate their actions only by *passing messages*.

The collection of nodes as a whole operates the same –no matter *where, when or how* the interaction takes places between the user and that sys.
For example: 
- An end-user cannot tell where the computation is taking place
- Where data is stored exactly should be irrelevant to an app
- Whether or not data has been replicated is completely unknown/hidden. (Distribution transparency)
#### Examples of Distributed Systems
- The internet, The World Wide Web (WWW), A cellular mobile phone network
- The cloud
Applications (apps) built on top of DSs are called Distributed apps (DA): Netflix, Spotify, Instagram.
“You know you have a DS when the crash of a computer you’ve never heard of stops you from getting any work done” – Leslie Lamport
#### Distributed versus Decentralised Systems
There are two views of DS:
1. INTEGRATIVE view: connecting existing networked computer systems into a larger a sys.
2. EXPANSIVE view: an existing networked computer sys is extended with additional computers
![[centralised-decentralised-distributed-system.png\|400]]
> A DISTRIBUTED system is a networked computer sys in which processes and resources are **sufficiently** spread across multiple computers. (expansive view).

> A DECENTRALISED sys is a networked computer sys in which processes and resources are **necessarily** spread across multiple computers. (integrative view)

Here, data is normally brought to the high-performance computers that literally train models before they can be used. But when data needs to stay within the perimeter of an org (e.g. security reasons), training is brought to the data. The result is known as **federated learning**.
#### Examples of Decentralised and Distributed Systems
1. **Blockchain (distributed ledger)  decentralised system**
A distributed ledger, blockchain: we need to deal with the situation that participating parties do not trust each other enough to set up simple schemes for collaboration. 
	Instead, what they do is essentially make the transactions among each other fully public (and
 verifiable) by an extend-only ledger that keeps records of those transactions. The ledger itself is fully spread across the participants, and the participants are the ones who validate transactions (of others) before admitting them to the ledger. 
	The result is a decentralised sys in which processes and resources are, indeed,
necessarily spread across multiple computers, in this case due to lack of trust.
2. **Geographically dispersed  decentralised system**
Consider systems that are naturally geographically dispersed. This occurs typically with systems in which an actual location needs to be monitored, for example, in the case of a power plant, a building, a specific natural env, and so on. 
	The sys, controlling the monitors and where decisions are made, may easily be placed
somewhere else than the location being monitored. 
	One obvious example is monitoring and controlling of satellites, but also more mundane 
situations as monitoring and controlling traffic, trains, etc. Here, the necessity for spreading processes and resources comes from a spatial argument.
3. **Content Delivery Networks (CDNs) DS**
The content of an actual Website, is copied and spread across various servers of the CDN. 
	When visiting a Website, the user is transparently redirected to a nearby server that holds all
or part of the content of that Website. A server is selected for which good performance in terms of latency and bandwidth can be guaranteed. 
	The CDN dynamically ensures that the selected server will have the required content readily 
available, as well as update that content when needed, or remove it from the server when there are no or very few users to service there. 
	Meanwhile, the user knows nothing about what is going on behind the scenes (which, again, 
is a form of distribution transparency). We also see that content is not copied to all servers, yet only to where it makes sense, that is, *sufficiently*, and for reasons of performance. CDNs also copy content to multiple servers to provide high levels of dependability.
4. **Network-Attached Storage (NAS) DS**
Consider a domestic-use setup based on a NAS, a typical NAS consists of 2–4 slots for internal hard disks. 
	The NAS operates as a file server: it is accessible through a (generally wireless) network for
any authorised device, and can offer services like shared storage, automated backups, streaming media, etc. 
	The NAS itself can best be seen as a single computer optimised for storing files, and offering 
the ability to easily share those files. The latter is important, and together with multiple users, we essentially have a setup of a DS. 
	The users will be working with a set of files that are locally (i.e., from their laptop) easily 
accessible (in fact, seemingly integrated into the local file sys), while also directly accessible by and for other users. 
	Again, where and how the shared files are stored is hidden (i.e., the distribution is 
transparent). If file sharing is the goal, then we see that a NAS can provide *sufficient* spreading of processes and resources.
### Goals and challenges
A DS aims for: 1. Sharing of resources, 2. Distribution Transparency, 3. Openness, 4. Scalability,
A DS has many challenges:
• Architecture: common orgs, common styles
• Process: what kind of processes, and their relationships
• Communication: facilities for exchanging data
• Coordination: app-independent algorithms
• Naming: how do you identify resources?
• Consistency and replication: performance requires of data, which need to be the same
• Fault tolerance: keep running in the presence of partial failures
• Security: ensure authorised access to resources
### Sharing of resources
**Examples**: File sharing on P2P, shared web hosting, shared cloud-based storage
There are many reasons for wanting to share resources; its economically cheaper to have a single high-end reliable storage facility than having to buy and maintain storage for each user separately.
	Connecting users and resources also makes it easier to collaborate and exchange 
information, as is illustrated by the Internet with its simple protocols for exchanging files, mail, documents, audio, and video. 
	The connectivity of the Internet has allowed geographically widely dispersed groups of 
people to work together by all kinds of groupware, that is, software for collaborative editing, teleconferencing, and so on, as is illustrated by multinational software-development companies that have outsourced much of their code production to Asia.
### Transparency
> The phenomenon by which a DS attempts to *hide* the fact that its processes and resources are *physically distributed* across *multiple computers*, possibly separated by large distances.
	
This is handled through many different techniques in the MIDDLEWARE layer that sits between apps and OSs. E.g. Limited transparency: network services like sockets are directly visible to app dev.

Aiming for distribution transparency may be a nice goal when designing and implementing a DS, but that it should be considered together with other issues such as performance and comprehensibility. The price for achieving full transparency may be surprisingly high.
#### Middleware
![[middleware-layer-ex.png\|400]]
Its the glue *between* apps and OSs, extending over multiple machines; contains commonly used components and functions that need not be implemented by apps separately.
The aim of the middleware is to hide heterogeneity of the underlying platforms from apps.
#### Types of Transparency
| Transparency  | Description                                                           |
| ------------- | --------------------------------------------------------------------- |
| _Access_      | Hide differences in data representation and how an object is accessed |
| _Location_    | Hide where an object is physically located in the sys                 |
| _Migration_   | Hide that an object may move to another location                      |
| _Relocation_  | Hide that an object may be moved to another location while in use     |
| _Replication_ | Hide that an object is replicated                                     |
| _Concurrency_ | Hide that an object may be shared by several competitive users        |
| _Failure_     | Hide the failure and recovery of an object                            |
### Openness
> An OPEN DS offers components that can easily be used by, or integrated into other systems. An open DS itself will often consist of components that originate from elsewhere.

![[middleware-openness-ex.png\|200]]
They *share* the same *interface* and *communicate* with the same *common protocol* to be able to *interact with services from other open systems*, irrespective of the underlying env.

Systems should conform to well-defined *interfaces*, easily *interoperate*, support *portability* of apps and be easily *extensible*.
### Scalability Issues
A DS can be scaled in size, geographically or adminly if it remains effective after scaling.
#### Scale in Size (number of users and/or processes)
Scalability for the internet was effortless because information is organised hierarchically rather than linearly: $O(\log(n))$. Poor scalability is when the cost of supporting $n$ users is worse than $O(n)$.
#### Scale Geographically (maximum distance between nodes)
1. DSs designed for LANs are hard to scale as they are based on *synchronous communication*.
Here, a party requesting service (*client*), *blocks* until a *reply* is sent back from the *server* implementing the service e.g., a db transaction. 
	Communication between two machines in a LAN: ~few hundred microseconds ($\mu s$)
However, the *interprocess communication* in a WAN may be hundreds of milliseconds ($m s$), three orders of magnitude ($10^3$) slower; this is a LATENCY problem.
2. Communication in WANs has *limited bandwidth* and is inherently much *less reliable* than LANs. 
In a home network, ensuring a stable, fast stream of high-quality video frames from a media server to a display is simple. If you place that same server far away with a standard TCP connection to the display, it will fail due to bandwidth limitations but also maintaining due to unreliable connection. Solutions developed for LANs are not easily *ported* to a WAN.
	Furthermore, WANs have very limited facilities for multipoint communication. Separate 
services, such as naming and directory services need to be developed for queries to be sent. These services must also be scalable, for which, no obvious solutions exist. TODO.
	In contrast, LANs often support *efficient broadcasting mechanisms* which are useful for 
discovering components and services, and are desired from a management point of view. 
#### Scale adminly (no. admin domains, e.g. Google data centres worldwide)
To scale a DS across multiple, independent admin domains, solve the major problem of *conflicting policies* with respect to *resource usage* (and payment), *management*, and *security*. 
**(EXAMPLE 1)** scientists want to share usage of their expensive equipment in computational grid. 
- In these grids, a global DS is constructed as a federation of local DSs, allowing a program 
running on a computer at org A to directly access resources at org B. 
- Many components of a DS that reside within a single domain can be trusted by users that operate within that same domain. 
- In such cases, sys administration may have tested and certified apps, taking special measures to ensure that such components cannot be tampered with.
- So, users trust their sys admins but this trust does not expand naturally across domain boundaries.
If a DS *expands to another domain*, two types of *security measures* need to be taken. 
**(1)** The DS has to *protect* itself against malicious *attacks* from the *new domain*. 
 - Users from the new domain may have only read access to the file sys in its original domain. 
 - Facilities such as expensive image setters or high-performance computers may not be made available to unauthorised users. 
**(2)** The new domain has to protect itself against malicious attacks from the DS. 
 - E.g. downloading programs such as applets in Web browsers. Basically, the new domain does not know what to expect from such foreign code. These limitations are hard to enforce.

**(EXAMPLE 2)** consider developing a radio telescope where the final sys is a federated DS.
- The telescope itself may be a wireless DS developed as a grid of a thousand sensor nodes, each collecting radio signals and collaborating with neighbouring nodes to filter out relevant events. 
	- The nodes dynamically maintain a *sink tree* by which selected events are *routed to a central point* for further analysis. 
	- The central point needs to be a reasonably powerful sys, capable of storing and processing the events sent to it by the sensor nodes. 
- This sys is *necessarily* placed in *proximity* of the sensor nodes, but is otherwise to be considered to operate independently. 
	- it may operate as a small local DS, storing all recorded events and offering access to remote systems owned by partners in the consortium. 
- Most partners have local DS (often a cluster of computers) that they use to further process the data collected by the telescope. 
	- In this case, the local systems directly access the central point at the telescope using a standard communication protocol. 
	- Naturally, many results produced within the consortium are made available to each partner.
	- So, the complete sys will cross boundaries of several admin domains, and that special measures are needed to ensure that data that only accessible to a specific consortium of partners. 

**(EXAMPLE 3)** DSs spanning multiple admin domains that do not suffer from admin scalability problems like file-sharing P2P networks. 
 - In these cases, end users simply install a program implementing distributed search and download functions and within minutes can start downloading files. 
	 - Other examples include P2P apps for telephony over the Internet such as Skype, and peer-assisted audio-streaming apps such as Spotify.
 - In these DSs, the end users -not admin entities, collaborate to keep the sys running. 
	 - At best, underlying admin orgs such as Internet Service Providers (ISPs) can police the network traffic that these P2P systems cause, but such efforts are not very effective.
### Scaling Techniques
Scalability problems in DSs usually appear as performance problems caused by limited capacity of servers and/or networks. 
> SCALING UP is a solution of capacity improvement through memory increases, CPU upgrades, or network module replacement

> SCALING OUT is a solution of increasing machine deployment via hiding communication latencies, work distribution and replication.
#### (SOLUTION 1) Hiding Communication Latencies
This approach is about making better use of response times -applicable for geographic scalability.
It between letting: **a)** a server or **b)** a client check forms as they are being filled.
![[hiding-comm-latencies-ex.png\|501]]
**(EXAMPLE 1)** When a service has been requested at a remote machine, useful *work can be done at the requester’s side* while waiting for a reply. 
- To do this, the requesting app must uses only ASYNCHRONOUS communication. 
- When a reply arrives, the app is interrupted and a special handler is called to complete the previously issued request. 
- Asynchronous communication can be used in batch-processing systems and parallel apps where independent tasks can be scheduled for execution while another task is waiting for the communication to complete. 
	- Alternatively, a new thread of control can be started to perform the request. 
	- Although, it blocks waiting for the reply, other threads in the process can continue.

**(EXAMPLE 2)** Many apps that cannot make effective use of asynchronous communication. 
When a user sends a request in an interactive app, they have nothing better to do than to wait. 
- Here, a better solution is to *reduce the overall communication*, for example, by *moving part of the computation* that is normally done at the *server* to the *client process requesting the service*. 
A typical case where this approach works is accessing dbs using forms. 
- Filling in forms can be done by sending a separate message for each field and awaiting confirmation from the server; the server may check for syntax errors before accepting an entry.
- A better solution is to ship the code for filling in the form, and possibly checking the entries, to the client, and have the client return a completed form. 
 - This approach is widely supported by the Web by means of Java applets and JS.
#### (SOLUTION 2) Partitioning and Distribution (Domain Name Lookup & Addressing)
Involves taking a *component*, splitting it into *smaller parts*, and subsequently *spreading* those parts *across the sys*.
**(EXAMPLE 1)** The Internet Domain Name sys (DNS). The DNS name space is hierarchically organised into a tree of domains, which are divided into non-overlapping ZONES. 
	These zones allow the DNS to scale through distribution and decentralised administration, 
allowing different orgs to manage their own portions of the name space independently while maintaining a coherent global sys.
- The names in each zone are handled by a single name server: Each path name being the name of a host in the Internet, and is thus associated with a network address of that host. 
![[zones-dns.png\|300]]
RESOLVING a name means returning the network address of the associated host.
E.g. `flits.cs.vu.nl` $R(\text{name}, Z_i) = \text{address}$ be the resolving of a name to a zone $i$ to an address. 
$$\begin{align} 
R(\text{flits.cs.vu.nl}, Z_1) = Z_2) \\ 
R(\text{flits.cs.vu}, Z_2) = Z_3) \\
R(\text{flits.cs}, Z_3) = \\
\text{host address} \\
\end{align}$$
- Eventually a zone will return the address of the associated host, show how the naming service, given by DNS, is distributed across several machines, thus avoiding that a single server has to deal with all requests for name resolution. 
- Hierarchical nature of DNS means that name lookup does not take twice as long when the number of machines on the Internet doubles

**(EXAMPLE 2)** the WWW is physically partitioned and distributed across a few hundred million servers, each handling a number of Web documents. 
- The name of the server handling a document is *encoded* into that document’s URL. 
- The Web only scaled to its current size due to this method of distributing of documents.
#### (SOLUTION 3) Replication
Scalability problems often appear in the form of *performance degradation*; the solution is to REPLICATE components across a DS. It *increases availability* and *balances the load* between components leading to better performance. Also, in geographically widely dispersed systems, having a copy nearby can *hide communication latency* problems.

**(DRAWBACKS)** Having multiple copies (cached or replicated), leads to *inconsistencies*: modifying one copy makes that copy different from the rest.
- Always keeping copies consistent requires *global synchronisation* on each modification.
- Global synchronisation precludes large-scale solutions.
### Pitfalls (9)
Many DSs are needlessly complex caused by mistaken assumptions:
The network is: **(1)** reliable, **(2)** secure, **(3)** homogeneous.
**(4)** Topology doesn't change. **(5)** Latency is zero. **(6)** Bandwidth is infinite.
**(7)** Transport cost is zero. **(8)** There is one admin. **(9)** All clocks are synchronised.
	These *properties* are *unique* to DSs: reliability, security, heterogeneity, and network topology; 
latency and bandwidth; transport costs; admin domains and clock synchronisation. 
When developing non-DAs, most of these issues will most likely not show up.
## 2. Types of Distributed Systems Pt.2 - *Chapter 1*
There are three types of DS that serve distinct purposes.
### 1. High performance distributed computing systems (HPDCS)
#### Distributed Shared Memory systems
High-performance distributed computing started with parallel computing
- Multiprocessor and multi-core versus multi-computer 
**(CONTEXT)** Distributed Shared Memory systems vs Multiprocessors
- Multiprocessors are easy to program in comparison to multi-computers, yet have problems when increasing the number of processors (or cores). 
- Distributed shared memory could never compete with that of multiprocessors, and failed to meet the expectations of programmers; they have been widely abandoned now.
**(RESULT)**  A shared-memory model on top of a multi-computer via virtual-memory techniques. 
- Map all main-memory pages (from different processors) into one single virtual address space. 
- If a process at processor A addresses a page P located at processor B, the OS at A traps and fetches P from B, just as it would if P had been located on local disk. 
#### Cluster Computing
Cluster computing systems became popular when the price/performance ratio of personal computers and workstations improved. It made economical sense to build a supercomputer using off-the-shelf technology by simply hooking up a collection of relatively simple computers in a high-speed network. 
- Mostly used for parallel programming in which a single (compute intensive) program is run in parallel on multiple machines.
- Essentially a group of high-end systems connected through a Local Area Network (LAN)
- Homogeneous: same OS, near-identical hardware
- Single managing node running the OS
**(EXAMPLE)** Linux-based Beowulf clusters
![[cluster-computing-system-ex.png\|400]]
Each cluster is a collection of mostly-identical COMPUTE nodes that are controlled and accessed by means of a single MASTER node. 
- This node handles the allocation of nodes to a particular parallel program, maintains a batch queue of submitted jobs, and provides an interface for the users of the sys. 
- As such, the master node actually runs the *middleware* needed for the execution of programs and management of the cluster, while the compute nodes are equipped with a standard OS extended with typical middleware functions for communication, storage, fault tolerance, etc.
#### Grid Computing
The next step along in development is it have lots of nodes from any and everywhere.
These DSs are often constructed as a federation of computer systems, each may fall under a different admin domain: hardware, software and network technology could differ.
- Enables flexible, secure and coordinated resource sharing among dynamic collections of individuals, institutions, and resources.
- Enable communities "Virtual orgs" to share geographically distributed resources.

There is an emerging trend of a more hybrid architecture whereby nodes are configured specifically for certain tasks. This is even more prevalent in grid computing. 
- The processes belonging to the same virtual org have access rights to the resources that are provided to that org. 
- Typically, resources consist of compute servers (including supercomputers, possibly implemented as cluster computers), storage facilities, and dbs. 
- In addition, special networked devices such as telescopes, sensors, etc., can be provided.

Most grid-computing software focuses on managing access to different admin domains, and apps to users and virtual orgs. This outlines the architectural issues.
![[grid-computing-systems-proposed-arch-ex.png\|200]]
Typically the collective, connectivity, and resource layer form the heart of what could be called a GRID MIDDLEWARE LAYER, jointly providing access to and management of resources that are potentially dispersed across multiple sites; they act as a site (or admin) unit.
	This prevalence is emphasised by the gradual shift toward a service-oriented architecture in 
which sites offer access to the various layers through a collection of Web services.
This defines an alternative architecture: Open Grid Services Architecture (OGSA), which is based upon a standardised version of the original architecture that follows Web service standards. 
#### Cloud Computing
From the perspective of grid computing, a next logical step is to simply outsource the entire infrastructure that is needed for compute-intensive apps. 
> (1) Cloud computing is an information technology infrastructure in which computing resources are *virtualised* and accessed as a *service*.

Provides the facilities to dynamically construct an infrastructure and compose what is needed from available services. Different to grid computing, which is associated with HPC.
> (2) Cloud computing is characterised by an easily usable and accessible pool of _virtualised_ resources. 

Usage of resources can be configured dynamically, providing the basis for scalability:
- if more work needs to be done, a customer can simply acquire more resources. 
- This links to utility computing because cloud computing is generally based on a pay-per-use model in which guarantees are offered by means of customised service level agreements (SLAs).
**(CONTEXT)** Researchers wanted to organise computational grids that were easily accessible and orgs running data centers wanted to open-up their resources to customers. 
	This lead to UTILITY computing: a customer could upload tasks to a data centre and be
charged on a per-resource basis, forming the basis of cloud computing.

The cloud has four layers:
![[the-organisation-of-clouds.png\|400]]
- Generally implemented at data centres.
- Processors, routers, power and cooling systems- invisible to customers
**(2) Infrastructure**: Provides customer a mix of virtual storage and computing resources.
- Deploys virtualisation techniques. 
- Evolves around allocating and managing virtual storage devices and virtual servers
**(3) Platform**: Provides higher-level abstractions for storage and such. 
- An OS to app devs: easy to develop and deploy apps that need to run in a cloud. 
- In practice, an app developer is offered a vendor-specific API, which includes calls to uploading and executing a program in that vendor’s cloud. 
**(4) app**: apps themselves run here, e.g., text processors, presentation apps.
- It is important to realise that these apps are executed in the VENDOR’s cloud.
- Comparable to the suite of apps shipped with OSes.

**(PROVISION)** These layers are provided through various interfaces (including CLI tools, programming and Web interfaces), leading to three different types of services:
- **Infrastructure-as-a-Service** (IaaS) covering the hardware and infrastructure layer
- **Platform-as-a-Service** (PaaS) covering the platform layer
- **Software-as-a-Service** (SaaS) in which their apps are covered
Cloud computing outsources local computing infrastructures but still has its obstacles including *provider lock-in*, *security* and *privacy* issues, and *dependency on the availability* of services.
### 2. Distributed information systems
#### Context
Many of the existing middleware solutions are the result of working with an infrastructure in which it was easier to integrate apps into an enterprise-wide information sys. 
Organisations had *many* *networked apps* but struggled to achieve interoperability.

**(INTEGRATION)** A networked app simply of a server running that app (often including a db) and making it available to remote programs, called *clients*. 
- Clients send a request to the server for executing a specific operation and a response returned. 
- Integration at the lowest level allows clients to wrap a number of requests, possibly for different apps or servers, into a single larger request and have it executed as a distributed transaction.
- The key idea is that all, or none of the requests are executed. 
- As apps became more sophisticated and were separated into independent components (notably distinguishing db components from processing components), apps needed to communicate directly with each other, leading to *Enterprise app Integration (EAI)*
#### Transactions
Database apps where operations are carried out in the form of TRANSACTIONS.
- In a mail sys, there might be primitives to send, receive, and forward mail. 
- In an accounting sys, they might be quite different. READ and WRITE are typical examples, however. Ordinary statements, procedure calls, and so on, are also allowed inside a transaction. 
- In particular, **remote procedure calls (RPCs)**, that is, procedure calls to remote servers, are often also encapsulated in a transaction, leading to what is known as a **transactional RPC**.

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
The operations between them form the body of the transaction. 
- The characteristic feature of a transaction is _either all of these operations are executed or none are executed_. 
	- These may be sys calls, library procedures, or bracketing statements in a language, depending on the implementation. 
- This all-or-nothing property of transactions is one of the four characteristic properties that transactions have. 

More specifically, transactions adhere to the so-called ACID properties:
**Atomic**: happens indivisibly (seemingly)
**Consistent**: does not violate sys in-variants
**Isolated**: not mutual interference between concurrent transactions
**Durable**: commit means changes are permanent

**(SUB-TRANSACTION)** join to form a nested transaction. 
- The top-level transaction may fork off children that run in parallel with one another, on different machines, to gain performance or simplify programming. 
- Each child may also execute one or more sub-transactions, or fork off its own children. 
**(ISSUES)** Imagine a transaction starts several subs in parallel, and one commits, making its results visible to the parent transaction. 
- After further computation, the parent aborts, restoring the entire sys to the state it had before the top-level transaction started. 
- Consequently, the results of the sub that committed must be undone. Thus, the permanence referred to above applies only to top-level transactions.

Since transactions can be nested arbitrarily deep, considerable administration is needed to get everything right. The semantics are clear, however. 
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire sys for it to manipulate_ as it wishes. 
	- If it aborts, its private universe just vanishes, as if it had never existed. 
	- If it commits, its private universe replaces the parent’s universe. 
- Thus if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- Likewise, if an enclosing (higher level) transaction aborts, all its underlying subtransactions have to be aborted as well. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.
**(TRANSACTION PROCESSING MONITOR)**  The *data* involved in a transaction is *distributed across several servers*. 
A TP Monitor is responsible for coordinating the execution of a transaction and the commitment of subtransactions following a standard protocol known as distributed.

Nested transactions are important in DS, naturally providing a way of distributing a transaction across multiple machines, following a logical division of the work of the original transaction. 

For example, a transaction for planning a trip by which three different flights need to be reserved can be logically split up into three subtransactions. 
- Each of these subtransactions can be managed separately and independently of the other two.
![[tpm-in-a-ds.png\|400]]
Apps wanting to coordinate several subtransactions into a single transaction did not have to implement this coordination themselves because a TP monitor does this for them. 
This is exactly where MIDDLEWARE comes into play: 
- it implements services that are useful for many apps avoiding that such services have to be reimplemented over and over again by app developers.

**(Middleware and EAI)** Several types of communication middleware exist. 
With remote procedure calls (RPC), an app component can effectively send a request to another app component by doing a local procedure call, which results in the request being packaged as a message and sent to the callee. 

The result will be sent back and returned to the app as the result of the procedure call. 
Later, techniques were developed to allow calls to remote objects, leading to what is known as **remote method invocations (RMI)**. An RMI is _essentially the same as an RPC, except that it operates on objects instead of functions_.

Middleware offers communication facilities for app integration (EAIs)
![[middleware-in-eai.png\|300]]
**Remote Procedure Call (RPC):** Requests are sent through local procedure call, packaged as message, processed, responded through message, and result returned as return from call.

RPC and RMI have the _disadvantage_ that the caller and callee both need to be up and running at the time of communication. 

In addition, they need to know **exactly** how to refer to each other. This **tight coupling** is often experienced as a serious drawback, and has lead to what is known as **message-oriented middleware (MOM)**. 

Here, apps send messages to logical contact points, described by means of a subject. 
- Likewise, apps can indicate their interest for a specific type of message, after which the communication middleware will take care that those messages are delivered to those apps. 
- These so-called **publish/subscribe systems** form an important and expanding class of DS.
- Messages are sent to logical contact point (published), and forwarded to subscribed apps.
### 3. Distributed Systems for Pervasive computing (DPS)
Next for DS where nodes are small, mobile, and often embedded in a larger sys, characterised by the fact that the sys *naturally blends into the user's env*. Coined as the IoT.
- We often need to deal with the intricacies of wireless and mobile communication, will require special solutions to make a pervasive sys as transparent or unobtrusive as possible.

They're unique because the separation between users and sys components is more blurred. 
There is often no single dedicated interface, such as a screen/keyboard combination. 
Instead, a **pervasive** sys is often equipped with many **sensors** that pick up various aspects of a user’s behaviour. Likewise, it may have a myriad of **actuators** to provide information and feedback, often even purposefully aiming to _steer_ behaviour.
#### Ubiquitous Computing Systems (UCS)
Pervasive and continuously present, i.e., a continuous interaction between sys and user often with the user being unaware that the interaction is happening.
Core requirements for a UCS roughly as follows: 
1. (**Distribution**) Devices are networked, distributed, and accessible in a transparent manner 
2. (**Interaction**) Interaction between users and devices is highly unobtrusive, hiding interfaces.
Much of the interaction by humans will be **implicit**: _one that is not primarily aimed to interact with a computerised sys but which such a sys understands as input_.
**(Ex)** The settings of a car’s driver’s seat, steering wheel, and mirrors is fully personalised. 
If Bob takes a seat, the sys will recognise that it is dealing with Bob and subsequently makes the appropriate adjustments. 
3. (**Context awareness**) The sys is aware of a user’s context in order to optimise interaction.
_any information that can be used to characterise the situation of entities (i.e., whether a person, place or object) that are considered relevant to the interaction between a user and an app, including the user and the app themselves_. 

In practice, context is often characterised by location, identity, time, and activity: 
- the where, who, when, and what. 
- A sys will need to have the necessary (sensory) input to determine one or several of these context types. 
- Raw data as collected by various sensors is lifted to a level of abstraction that can be used by apps. 

A concrete example is detecting where a person is, for example in terms of GPS coordinates, and subsequently mapping that information to an actual location, such as the corner of a street, or a specific shop or other known facility. 

The question is where this processing of sensory input takes place: 
- is all data collected at a central server connected to a db with detailed information on a city, or is it the user’s smartphone where the mapping is done? 
- When it comes to combining flexibility and potential distribution, so-called shared data spaces in which processes are decoupled in time and space are nice, yet, suffer from scalability problems. 
4. (**Autonomy**) Devices operate autonomously without human interven- tion, and are thus highly self-managed 
In a UCS env, the sys admin cannot keep everything up and running. 
The sys should act autonomously, and automatically react to changes using many techniques:
- **Address allocation**
	- In order for networked devices to communicate, they need an IP address. Addresses can be allocated automatically using protocols like the Dynamic Host Configuration Protocol (DHCP) (which requires a server) or Zeroconf. 
- **Adding devices**
	- It should be easy to add devices to an existing sys. A step towards automatic configuration is realised by the Universal Plug and Play Protocol (UPnP).
	- Using UPnP, devices can discover each other and make sure that they can set up communication channels between them. 
- **Automatic updates
	- Many devices in a UCS should be able to regularly check the Internet software updates
	- Manual intervention is to be kept to a minimum
4. (**Intelligence**) The sys as a whole can handle a wide range of dynamic actions and interactions.
UCS often use methods and techniques from the field of artificial intelligence. 
In many cases, a wide range of advanced algorithms and models need to be deployed to handle incomplete input, quickly react to a changing environment, handle unexpected events, and so on. 
#### Mobile computing systems 
Pervasive, but emphasises that devices are inherently mobile.
A DS where devices can change physical location and network attachment points while maintaining connectivity, often forming a key part of **pervasive computing**.

* **Device Heterogeneity:** Encompasses a wide range of **IP-enabled devices** beyond smartphones/tablets (e.g., IoT sensors, wearables, vehicular systems).
* **Wireless Communication:** The primary enabler of mobility (*Mobile typically implies wireless*).
* **Dynamic Location:** A device's **network location** (IP address/point of attachment) and **physical location** change over time. This is the central challenge.
![[mobile-comp-ex.png\|200]]
**A. Service Discovery & Availability**
* **Problem:** As devices move, the set of locally available services changes.
* **Required Mechanism:** **Dynamic service discovery** (services announce presence, clients discover them). This is crucial for context-aware apps.
*   *Example:* Your smartphone automatically discovering and connecting to a nearby wireless printer when you enter an office, but not seeing it at home.
**B. Location Management**
*   **Geographical Location:** Needed for **tracking/tracing** (e.g., GPS in a delivery vehicle).
*   **Network Location:** Needed for routing. Solved by protocols like **Mobile IP**, which separates a device's permanent **home address** from its temporary **care-of address**, allowing seamless movement between networks.
**C. Communication in Dynamic Networks: MANETs & DTNs**
*   **MANET (Mobile Ad-hoc Network):** A self-configuring network of mobile devices connected wirelessly without fixed infrastructure.
    *   **Core Routing Problem:** **Static routes fail** as nodes move. Requires **proactive** (e.g., DSDV) or **reactive** (e.g., AODV) routing protocols.
*   **DTN (Disruption/Disconnection-Tolerant Network):** Assumes **intermittent connectivity** and no guaranteed end-to-end path.

DTN Routing Principles (Store-Carry-Forward).
When a continuous path cannot be established, DTNs use a **store-carry-forward** paradigm:
1.  **Store:** A node receives and **stores** a message.
2.  **Carry:** The node physically **carries** the message as it moves.
3.  **Forward:** When it finds another node, it may **forward** the message based on a chosen strategy.
_Passing messages in a (mobile) disruption-tolerant network_
![[passing-messages-disruption-tolerant-network.png\|300]]
**Routing Strategies:**
*   **Flooding-Based (e.g., Epidemic Routing):** Messages spread redundantly. High delivery chance but high resource cost.
*   **Selective Forwarding:** More efficient. Decisions based on:
    *   **Utility/Probability:** Forward to nodes with higher historical probability of contacting the destination.
    *   **Social Patterns:** Forward to nodes belonging to the same "community" (**social-based routing**).
    *   **Connectivity:** Forward to **well-connected nodes** (central hubs in the mobility pattern).
*   ***Technical Example:** A wildlife tracker on a zebra (Node A) needs to send data to the ranger station (Node D). There's no direct path. The zebra moves and encounters a vulture's tracker (Node B). Using a **utility-based** protocol, Node A forwards the data to Node B. The vulture flies and later lands near a fixed sensor node (Node C) connected to the station, finally delivering the data. The network tolerated long delays and disconnections.*
Mobile computing shifts the design focus from stable connectivity and fixed locations to **mobility management, discovery, and tolerant routing** in the face of constant change and potential disconnection. Mobile devices set up connections to stationary servers, bringing mobile computing in the position of clients of cloud-based services.
#### Sensor (and actuator) networks: 
Pervasive, with emphasis on the actual (collaborative) sensing and actuation of the env.
Here is the content reformatted as requested, with headings prepended to the paragraphs and the "Features" section integrated.

**(SENSOR NETWORKS)** They are more than just a collection of input devices; instead, sensor nodes often collaborate to efficiently process the sensed data in an app-specific manner, making them very different from, for example, traditional computer networks. A sensor network generally consists of tens to hundreds or thousands of relatively small nodes, each equipped with one or more sensing devices. In addition, nodes can often act as actuators, a typical example being the automatic activation of sprinklers when a fire has been detected.

**(KEY CHARACTERISTICS)** Many sensor networks use wireless communication, and the nodes are often battery powered. Their limited resources, restricted communication capabilities, and constrained power consumption demand that efficiency is high on the list of design criteria.

**(NODE ARCHITECTURE)** When zooming into an individual node, we see that, conceptually, they do not differ a lot from “normal” computers: above the hardware there is a software layer akin to what traditional operating systems offer, including low-level network access, access to sensors and actuators, and memory management. Normally, support for specific services is included, such as localisation, local storage (think of additional flash devices), and convenient communication facilities such as messaging and routing. However, similar to other networked computer systems, additional support is needed to effectively deploy sensor network apps. In DSs, this takes the form of middleware.

**(PROGRAMMING MODELS & COMMUNICATION SCOPE)** For sensor networks, instead of looking at middleware, it is better to see what kind of programming support is provided. One typical aspect in programming support is the scope provided by communication primitives. This scope can vary between addressing the physical neighbourhood of a node, and providing primitives for sys-wide communication. In addition, it may also be possible to address a specific group of nodes. Likewise, computations may be restricted to an individual node, a group of nodes, or affect all nodes.
*   *Example (Abstract Regions):* A node can define a region of its eight nearest neighbours. It can write a sensor reading to that region, and then perform an operation like finding which node in the region has the maximum reading, enabling local, collaborative processing.

**(DATABASE VIEW & DATA ACCESS)** As another related example, consider a sensor network as implementing a distributed db. This view is common as many networks are deployed for measurement and surveillance, where an operator extracts information by issuing queries.

**(DATA PROCESSING ARCHITECTURES)** To organise a sensor network as a distributed db, there are essentially two extremes:
1.  **Centralised Processing:** Sensors send all raw data to a central operator's site. This wastes network resources and energy.
2.  **Purely Edge Processing:** Queries are sent to sensors, each computes an answer, and the operator aggregates all responses. This wastes the opportunity for sensors to aggregate data locally, reducing the volume of traffic.

**(IN-NETWORK PROCESSING)** What is needed are facilities for **in-network data processing**. One common method is to use a **routing tree**. A query is propagated down the tree to all nodes, and results are **aggregated** (e.g., summed, averaged, filtered) at intermediate nodes as they are passed back up to the root (the operator). This conserves energy and bandwidth.
*   *Example (TinyDB):* Implements a declarative SQL-like interface. It uses a tree-based routing algorithm where parent nodes schedule data collection from children and aggregate results before forwarding them upward.

**(PUBLISH/SUBSCRIBE MODEL)** An alternative to a single, query-specific tree is a **publish/subscribe** model. Nodes publish specific types of data (e.g., temperature readings) which are forwarded to designated **broker nodes**. Queries for that data type are then sent to the corresponding broker. This decouples data producers from consumers.

**(FEATURES)** Sensor networks are characterised by:
*   **Collaborative Processing:** Nodes work together to process data.
*   **Severe Resource Constraints:** Limited battery, CPU, memory, and communication bandwidth.
*   **Application-Specific Design:** The network architecture is tightly coupled to its sensing task.
*   **Wireless Communication & Ad-hoc Deployment.**
*   **In-Network Processing:** Computation occurs within the network to reduce data volume and save energy.
*   **Diverse Programming Models:** Including the db abstraction (e.g., TinyDB) and region-based programming.
**(BOTTOM LINE)** While mobile devices often act as clients to cloud services (**mobile cloud/edge computing**), sensor networks represent a different paradigm: a massively distributed, collaborative, and deeply embedded sys where processing is pushed into the network itself to overcome severe resource limitations and achieve scalability.
![[cloud-edge-continuum.png\|400]]
**(WHAT IS GOOGLE SPANNER?)** Google Spanner is a globally distributed, strongly consistent, relational db service. Its unique achievement is combining the horizontal scalability of NoSQL systems with **Strong ACID transactions** and an SQL interface across data centers worldwide. It was first described in 2012 and is used internally for services like Google Ads, Gmail, and Google Photos before being offered as Cloud Spanner

**(CORE INNOVATIONS)** It works by automatically sharding data across many Paxos-based replication groups for availability. Its most significant innovation is **TrueTime**, a globally synchronised clock API that uses GPS and atomic clocks to bound clock uncertainty across servers. TrueTime is the key that enables its powerful consistency properties without sacrificing performance globally.

**(CONSISTENCY PROPERTIES)** By default, Spanner provides **External Consistency**, which is the strictest isolation level for transaction-processing systems.
It has two formal guarantees:
- **Strict Serializability**: The sys behaves as if all transactions executed one at a time in some order (**serializability**), and that order respects real-time: if transaction T2 starts _after_ T1 commits, T2 must appear to execute _after_ T1 in the serial order.
- **External Consistency**: This adds a real-time ordering guarantee for transactions _observed_ to commit in sequence. If T2's commit process begins after T1's commit _returns successfully_, T2 is guaranteed to have a later commit timestamp than T1. This prevents anomalies like seeing a withdrawal before a deposit that visibly completed earlier. External consistency is a stronger property than both linearizability (for single operations) and basic strong consistency

**(HANDLING ACID TRANSACTIONS)** Spanner provides full ACID guarantees globally
This is achieved through a specific architecture:
- **Atomicity & Durability**: Uses a **Paxos-based synchronous replication** across every data shard. A write must be accepted by a majority of replicas before it is committed, ensuring durability. The two-phase commit protocol coordinates commits across different shards involved in a transaction
- **Isolation & Consistency**: Primarily uses **Multi-Version Concurrency Control (MVCC)**. Writes create new, timestamped versions of data
- TrueTime assigns globally meaningful, monotonically increasing timestamps to commits, establishing the externally consistent serial order. **Read-write transactions** (for reads and writes) use locking and get external consistency. **Read-only transactions** are executed at a chosen timestamp (system-chosen for strong reads, past for stale reads) without acquiring locks, reading a consistent snapshot.
- **Alternative Isolation Level**: Spanner also offers **Repeatable Read (Snapshot) Isolation**, which can improve performance for read-heavy, low-conflict workloads. Unlike the default, it is susceptible to **write skew**, so apps must use locking reads (`FOR UPDATE`) for correctness in critical sections.
## 3.  Architectures - *Chapter 2*
### Understand the different ways on how to view the org of a DS
An architecture defines the sys through **components** (replaceable units with interfaces), their **connectors**, the **data** they exchange, and how they are **configured** into a whole.
![[machine-interfaces.png\|300]]
### Architectural styles
#### Layered
Components are organised in hierarchical layers. A component in layer `Lj` can make a **downcall** to a lower layer `Li` (`i<j`) and expects a response. Upcalls to higher layers are rare.
![[layered-architectures.png\|400]]
**(LAYERED COMMUNICATION PROTOCOL)** A classic example is a protocol stack (e.g., TCP/IP). Each layer offers a **service** through a specific **interface** and implements that service using a **protocol** (rules for data exchange). For instance, **TCP** offers a reliable, connection-oriented service via a `socket` interface, governed by a protocol managing connections and data ordering.
![[layered-communication-protocol-ex.png\|300]]
**(APPLICATION LAYERING: THE PAD MODEL)** Many client-server apps use three logical layers:
1.  **Presentation:** User interface.
2.  **Application:** Core processing logic.
3.  **Data:** Persistent data management.
![[pad-search-engine-ex.png\|400]]
*Example:* In a cloud search engine, a user's query (Presentation) is routed by a load balancer to an app server (Application), which processes it by querying an index (Data).
![[cloud-search-ex.png\|400]]
#### Object-based
The sys is structured as a collection of loosely coupled **objects** (or **components**), where each object encapsulates its state (data) and behaviour (methods). Interaction occurs via **Remote Method Invocation (RMI)**.
Objects are accessed through interfaces, hiding their implementation. In a distributed object, the interface can be placed on a client machine via a **proxy** (client stub), while the object itself resides on a server. The proxy handles communication (marshalling/unmarshalling), forwarding requests to the server-side **skeleton**.
![[object-based-arch-remote-obj-client-side-proxy.png\|400]]
*Example:* A Java RMI app where a client-side proxy object forwards method calls to the actual object on the server.
This style, promoting clear encapsulation, forms the foundation for **Service-Oriented Architectures (SOA)**, where systems are built by composing independent, often remotely hosted, services (e.g., an online shop using a third-party payment service).
#### Resource-centred
This style, exemplified by **REST (Representational State Transfer)**, models a sys as a collection of uniquely named **resources**. All services offer the same, minimal interface and interactions are **stateless**.
*   **Key Principles:**
    1.  Resources have unique identifiers (e.g., URIs).
    2.  Uniform interface (typically **CRUD** operations).
    3.  Messages are self-descriptive (e.g., HTTP headers).
    4.  Stateless execution (server forgets client after request).
*   **CRUD Operations:** `POST` (Create), `GET` (Retrieve), `PUT` (Update), `DELETE`.
*   *Example:* **Amazon S3**. A file (object) in a bucket (directory) is accessed via a URI (`http://BucketName.s3.amazonaws.com/ObjectName`). Creating the object uses `PUT` on that URI; listing bucket contents uses `GET`.

#### Event-based (Publish-Subscribe)
Components interact by generating and reacting to events, leading to **loose coupling** between producers (publishers) and consumers (subscribers).
Coordination models vary based on temporal and referential coupling:
![[event-based-vs-shared-data-space-arch-ex.png\|400]]
*   **Event-Based Coordination:** Subscribers express interest in event types (**topics** or **content**). When a publisher generates an event, the middleware (**event bus**) delivers it to all matching subscribers. Processes are **referentially decoupled** (don't know each other) but often **temporally coupled** (subscriber must be running).
*   **Shared Data Space:** Processes communicate by writing and reading **tuples** (structured data records) to/from a shared, associative store. This is both **referentially and temporally decoupled**. A process can subscribe to tuples matching a pattern and be notified when one is published.
*   *Example:* A sensor network where a motion sensor (publisher) emits an "occupancy" event. A security service (subscriber), which registered for this event, receives the notification and checks if the door lock status (another event) is "unlocked" to trigger an alert.
### System architecture
#### Centralised
Based on the **client-server model**, where **servers** (providers) and **clients** (users) of services are distinct roles, communicating via a request/reply model.
![[centralised-org-arch-ex.png\|150]]
*   **2-Tiered:** Direct client-to-server communication. The client may handle presentation and some app logic (fat client) or be very thin.
![[2-tiered-arch-ex.png\|400]]
*   **3-Tiered:** A more scalable architecture with distinct presentation, app logic, and data tiers. Servers in one tier can act as clients to the next (e.g., an app server requesting data from a db server).
![[3-tier-arch-ex.png\|250]]
#### Decentralised (Peer-to-Peer - P2P)
All processes (**peers**) are equal, acting as both client and server (**servant**). Functions are distributed across all peers.
*   **Unstructured P2P:** Peers connect in an ad-hoc manner, forming a random overlay network. Data location is non-deterministic.
    *   **Search by Flooding:** A query is sent to all neighbours, who propagate it further (limited by a TTL). High network cost but fast.
    *   **Search by Random Walk:** A query is forwarded to one randomly chosen neighbour at a time. Lower cost but slower; multiple parallel walks speed it up.
*   **Structured P2P:** Peers organise into a specific, deterministic overlay (e.g., a ring, tree, or hypercube). Data items are assigned unique **keys** via hashing and stored in a **Distributed Hash Table (DHT)**. Lookups are routed efficiently through the overlay to the peer responsible for that key.
![[p2p-arch-as-4d-hyper-cube-ex.png\|300]]
*   *Example (Structured P2P):* In the 4D hypercube above, to find data with key `14` (`1110` in binary), node `0111` would route the request through connected neighbours, following the overlay protocol, until it reaches the node responsible for key `1110`.
#### Hybrid
Combines elements of centralised and decentralised architectures.
A prime example is the **edge-server sys**. Servers are placed at the network's **edge** (e.g., at an Internet Service Provider). An origin server in a central data centre replicates content to these edge servers. End-users connect to a nearby edge server for low-latency access, while the sys as a whole is managed centrally.
![[internet-as-edge-servers-ex.png\|400]]
This model extends to **fog/edge computing**, where computation and storage are distributed between the central cloud, edge servers, and even end-user devices for ultra-responsive services.
## 4. Communication - *Chapter 4*
At its core, communication in a DS involves **processes on different machines exchanging information** via **low-level message passing** over a network. Higher-level models (like RPC) are abstractions built on this fundamental mechanism.
### Latency and Bandwidth
Two critical performance metrics for any communication channel.
**(LATENCY)** The **delay** or time it takes for a single message (or the first bit of data) to travel from source to destination. Limited by the speed of light and processing delays in network hardware.
*   *Example:* Datacenter (1ms) vs. Intercontinental (100ms) vs. "Sneakernet" in a van (1 day).

**(BANDWIDTH)** The **capacity** or volume of data that can be transmitted per unit of time.
Example:* 5G (~5-20Gbps) vs. Broadband (~300Mbps) vs. Van of Hard Drives (~1 Gbps effective).

**Key Relationship:** The total time to transfer a message is the sum of the **latency** and the time to push all the data through the pipe, which is the message size divided by the **bandwidth**.

**Check of Understanding**
>**Question:** How long does it take to send a 10 MB (80 Mbit) file over a link with 100 ms latency and 1 Gbps (1000 Mbit/s) bandwidth?
>**Answer:** Time = Latency + (Size/Bandwidth) = $0.1s + {80 M\text{bit} \over 1000 M\text{bit}/s} = 0.1s + 0.08s = 0.18 \text{seconds}.$ Latency is the dominant factor. The bandwidth term would dominate for a 10 GB file.
### Layered Protocols
Communication is organised into layers, each providing a service to the layer above and using the service of the layer below. This modularity hides complexity.

The classic **OSI model** has seven layers, but practical systems like **TCP/IP** use fewer.
![[layered-protocols.png\|300]]

**(MIDDLEWARE PROTOCOLS)** This layered model is extended in DSs by **Middleware**, which sits between the OS/transport layers and the app. It provides common services like secure communication, data marshalling, and naming, freeing the app developer from re-implementing them.
![[middleware-protocols.png\|300]]
![[middleware-adapted-layering-scheme.png\|300]]
### Types of Communication
Communication can be characterised along two key dimensions:
**(SYNCHRONOUS vs. ASYNCHRONOUS)** This concerns the **timing and blocking behaviour** of the sender and receiver.
*   **Synchronous:** The sender blocks and waits until the receiver is ready to accept the message (or a reply is received).
*   **Asynchronous:** The sender continues execution immediately after issuing the send; the message is stored (buffered) until the receiver can process it.
![[communication-sync-ex.png\|300]]
**(TRANSIENT vs. PERSISTENT)** This concerns the **lifetime and storage** of the message in the communication sys.
*   **Transient:** A message is only stored *en route* while travelling to the receiver. If it cannot be delivered immediately (e.g., the receiver is down), it is **discarded**.
*   **Persistent:** A message is stored in a **queue** (e.g., at a message server) until it can be successfully delivered to the receiver, even if the receiver is temporarily unavailable.

**Common Combinations:**
*   **Client-Server / RPC:** Typically uses **transient, synchronous** communication (request-reply). Both parties must be active.
*   **Message-Oriented Middleware:** Typically uses **persistent, asynchronous** communication. Provides decoupling and fault tolerance.

**Check of Understanding**
> **Question:** What is a major drawback of the transient, synchronous model used in classic client-server systems?
> **Answer:** It requires **both client and server to be running simultaneously**. If the server crashes after the client sends a request, the client may be left hanging (blocked) indefinitely, leading to poor reliability and scalability.
### Remote Procedure Call (RPC)
RPC is an abstraction that makes a remote function call appear like a **local procedure call** to the programmer, hiding all network complexity.

**(BASIC OPERATION)** A **client stub** on the caller's machine masquerades as the actual server procedure. It **marshals** parameters into a network message, sends it, waits for the reply, **unmarshals** the result, and returns it.
![[rpc-operation-ex.png\|400]]
*Example:*
```java
// This looks local, but `processPayment` executes on a remote server.
Result result = paymentsService.processPayment(cardDetails, amount);
```

**(PARAMETER PASSING & MARSHALLING)** A core challenge is transforming data (parameters, results) from a machine-specific in-memory format into a neutral network format. This involves:
*   **Byte Order (Endianness):** Converting between big-endian and little-endian integer representations.
*   **Data Representation:** Agreeing on formats for floats, strings (ASCII/EBCDIC), and complex structures.
*   **Serialisation:** Flattening an object's state into a byte stream.
![[parameter-passing.png\|200]]

**(ASYMPTOTOIC RPC)** To avoid clients blocking, RPC can be made asynchronous. The client sends a request and continues, later collecting the result via a separate call or a callback.
![[async-rpc-ex.png\|300]]

**(REMOTE METHOD INVOCATION - RMI)** RMI is the object-oriented extension of RPC, allowing invocation of methods on remote objects. It uses client-side **proxy objects** that represent the remote object.
![[rmi-ex.png\|400]]

**Check of Understanding**
> **Question:** What is the key technological difference between early RPC systems (e.g., SunRPC) and modern ones like gRPC?
> **Answer:** While the core concept is the same, modern RPC frameworks like **gRPC** typically use **HTTP/2** as the transport for multiplexing and efficiency, and **Protocol Buffers** (a binary, language-neutral interface definition language) for highly efficient, version-tolerant marshalling/serialisation, as opposed to older textual or more verbose formats like XML.
### Message Oriented Middleware
MOM provides **persistent, asynchronous communication** via messages, leading to loosely coupled, reliable systems.

**(MESSAGE-QUEUING MODEL)** The core abstraction is a **queue**. Senders place messages into a queue, and receivers take messages from it. Queues decouple the communicating processes in time (asynchronous) and space (they don't need to know each other's address).
![[mpi-message-queue-model.png\|200]]
![[message-queuing-system-ex.png\|300]]

**(THE MESSAGE-PASSING INTERFACE - MPI)** MPI is a standard API for **transient messaging** primarily used in High-Performance Computing (HPC). It assumes a known, running group of processes.
*   It provides a rich set of primitives for point-to-point (`MPI_Send`, `MPI_Recv`) and collective communication (broadcast, scatter/gather).
![[advanced-transient-messaging-ex.png\|150]]
*Example Primitives:*
    *   `MPI_Send(buf, count, type, dest, tag, comm)`: Sends a message.
    *   `MPI_Recv(buf, count, type, source, tag, comm, status)`: Receives a message (blocks).
    *   `MPI_Isend()` / `MPI_Irecv()`: Non-blocking versions for overlap of communication and computation.

**(ADVANCED MESSAGE QUEUING PROTOCOL - AMQP)** AMQP is an open standard app-layer protocol for MOM, defining how messages are formatted and transmitted between systems (e.g., between a publisher and a queue, or a queue and a consumer).
*   *Example systems:* **RabbitMQ** (a widely-used AMQP broker), **Apache ActiveMQ**. They are used for enterprise integration, task distribution, and in cloud platforms.
*   *Use Case - OpenStack:* Uses a message queue (like RabbitMQ) as the central nervous sys. All service requests (e.g., "launch a VM") are placed on queues, and worker processes listen to these queues and execute the tasks, enabling a scalable, decoupled control plane.

**Check of Understanding**
> **Question:** A financial trading sys needs to ensure that a "sell order" message is never lost, even if the trading engine process crashes momentarily. Which communication paradigm and characteristic (transient/persistent) is essential here?
> **Answer:** This requires **Message-Oriented Middleware** with **persistent communication**. The "sell order" message would be placed into a durable queue. The trading engine, upon restarting, would retrieve the message from the queue and process it, guaranteeing "at-least-once" delivery.
## 5. Service Oriented Architectures
### Conceptual Design of Software systems
At a conceptual level, software systems are broken into layers, leading to tiered architectures.
![[1-2-3-n-tier-architectures-ex.png\|700]]
### Architectures
**(1-Tier)** All components (presentation, logic, data) are tightly integrated into a single, monolithic app and deployment unit. Example: A standalone desktop app with its own embedded db.

**(2-Tier)** The classic client-server model. A **client** (handling presentation and some logic) communicates directly with a **server** (typically a db). Example: A desktop app that connects to a central db server.

**(3-Tier)** Introduces a dedicated middle tier, separating concerns:
*   **Presentation Tier:** User interface.
*   **Application/Logic Tier:** Core business rules and processes.
*   **Data Tier:** Persistent data storage.
This provides better scalability, security (the db is not directly exposed), and maintenance. Example: A web app (Presentation) using an app server (Logic) to query a db (Data).
#### N-Tier
A generalisation of the 3-tier model, where any tier can be subdivided or services can be integrated, creating a more complex, distributed architecture. This often arises from:
1.  Integrating full legacy systems (which are themselves 2 or 3-tier) as resources.
2.  Adding dedicated web or API gateway servers as an extra presentation tier.
**Challenge:** N-tier architectures face significant complexity in **integration** due to a lack of universal standards, requiring extensive custom middleware to connect disparate systems.
### Emergence of SOAs
SOA emerged as a direct response to the challenges of N-tier and enterprise app integration. The trend was toward integrating complex, decoupled systems, often from different orgs. SOA facilitates this by treating app **functionality as reusable, loosely-coupled services**.
### Vision
SOA is an architectural style for building apps from **independent services** that communicate via messages. It represents a paradigm shift from tightly-coupled object-oriented systems to **message-oriented**, platform-independent integration. ARCH MODEL:
Top-bottom: Services/APIs, App, App server, Managed Runtime Env, OS, Hypervisor, Storage, Network, Hardware.
The SOA vision is built on integrating **Resources** (data, compute), **Communities** (procedures), and **Technologies** through standardised **Services** and **Connectivity**.
#### What is a ‘Service’?
> A *reusable*, *self-contained* *software component* that implements a *discrete business function* (e.g., "get user data," "process payment"). It is accessed strictly through its defined, message-based interface. Examples include weather forecasts, credit checks, or translation engines.
#### Architecture Model
The SOA model defines three core roles and three core operations:
![[soa-architecture-ex.png\|400]]
*   **Roles:** Service Provider (hosts), Service Registry (finds), Service Consumer (uses).
*   **Operations:** Publish (provider lists service), Find (consumer locates service), Bind (consumer connects to and uses the provider).
#### Service Provision and Consumption
![[service-provision-and-consumption.png\|400]]
This illustrates the dynamic interaction: a service is implemented and published by a provider. A consumer discovers it via the registry and subsequently binds to the provider to execute it.
#### Differences between SOAs and traditional n-tier architectures
1.  **Decentralised Middleware:** In SOA, communication logic (middleware) is embedded within each service's environment, not centralised in a single app server tier.
2.  **Strong Emphasis on Loose Coupling:** Services have minimal dependencies on each other's internal implementation, communicating via standardised messages. This contrasts with the tighter integration often found in N-tier layers.
3.  **Adherence to Well-Supported Standards:** SOA relies heavily on global, platform-neutral standards (e.g., XML, SOAP, WSDL) to enable interoperability across orgal boundaries, unlike the often proprietary or bespoke interfaces in N-tier systems.

**Check of Understanding**
> **Question:** A company has a monolithic 1-tier app for customer orders. They want to expose the "check inventory" and "process payment" functions to new mobile and web apps, and also to partner companies. Explain why moving to an SOA style would be more effective than simply creating a 3-tier version of their existing sys.
> **Answer:** A 3-tier architecture would primarily separate logic internally for scalability but would likely still result in a single, integrated app with a single point of access. SOA, by contrast, would decompose "check inventory" and "process payment" into **independent, reusable services**. These services could then be:
> *  **Discovered and consumed separately** by the new mobile app, web app, and partner systems.
> *   **Scaled independently** based on demand.
> *   **Updated or replaced** without affecting the entire order sys, thanks to **loose coupling** via standardised interfaces. This provides the agility and openness needed for multi-channel and inter-organisational integration that a standard 3-tier refactor would not.
## 6. Web Services and REST
### Why do we need Web services?
The evolution of distributed computing has moved from linking machines (Internet/TCP/IP) to linking documents (WWW/HTTP/HTML) to linking **applications**. Web services fulfill this third stage, enabling programmatic, reliable interaction between disparate apps over networks, moving beyond unreliable "screen scraping" of websites. They support industry needs for standardised data exchange using formats like **XML** and **JSON**.
### What are web services?
A **Web Service** is a unit of business logic accessible over a network (Internet/intranet) using standard protocols (HTTP, SMTP) and data formats (XML, JSON). It provides a **coarse-grained** way to expose functionality from existing platforms (JEE, .NET) within a **Service-Oriented Architecture (SOA)**, promoting large-scale, **loosely-coupled** systems without mandating specific implementation technologies.

**(SOA ROLES & EXAMPLE)** In an SOA with web services, there are three core roles: the **Provider** (publishes the service), the **Broker** (registry/directory like a "Yellow Pages"), and the **Requestor** (finds and uses the service). This enables dynamic discovery and integration.
![[soa-web-services.png\|400]]
*Example:* An airline's booking sys (on Oracle Solaris) and a car rental website (on Linux) can be integrated via web services, allowing seamless combined bookings despite different underlying hardware and software.

**(KEY FEATURES)**
*   **Standardised Data Packaging:** Uses XML/JSON over standard transports (HTTP, SMTP).
*   **Interoperability Abstraction:** Provides a web-friendly layer over traditional RPC/RMI.
*   **Self-Describing:** Described using languages like WSDL.
*   **Discoverable:** Published to and discoverable via service registries.
### What is REST?
**REST (Representational State Transfer)** is an **architectural style** (not a standard) for building DSs, particularly web services. It views a sys as a collection of **resources** (each identified by a **URI**) and uses a **uniform interface** (HTTP verbs) to manipulate them. It is the predominant style for modern web APIs.
### What does it consist of?
**(RESOURCE-BASED ARCHITECTURE)** The core REST principles are:
1.  **Resources identified by URIs:** Every piece of data is a resource with a unique address (e.g., `/parts/00345`).
2.  **Uniform Interface:** A constrained set of well-defined operations using HTTP verbs: **POST** (Create), **GET** (Retrieve), **PUT** (Update), **DELETE** (Delete).
3.  **Self-Descriptive Messages:** Requests and responses contain all information needed to be understood (via HTTP headers and body).
4.  **Stateless Interactions:** The server does not retain client context between requests. Each request contains all necessary information.
![[rest-triangle.png\|200]]
**(REPRESENTATIONAL STATE TRANSFER)** The name derives from how clients interact with the sys: a client retrieves a **representation** (e.g., HTML, JSON, XML) of a resource, which places the client in an app state. By following hyperlinks (URIs) within that representation, the client initiates a new request, transferring to a new state.
![[rest-boeing-ex.png\|200]]
**(PRACTICAL EXAMPLE)** Consider a parts depot web service.
*   **GET list of parts:** `GET http://www.parts-depot.com/parts` returns an XML list with hyperlinks to each part.
*   **GET part details:** `GET http://www.parts-depot.com/parts/00345` returns detailed XML for part 00345, which may contain further links (e.g., to its specification).
*   **Submit a Purchase Order (PO):** `POST http://www.parts-depot.com/orders` with an XML PO document in the request body creates a new order.
![[rest-purchase-order-ex.png\|300]]
![[web-service-pruchase-order.png\|200]]
### Claimed benefits
Proponents of REST highlight several advantages:
*   **Improved Performance:** Native support for HTTP caching improves response times.
*   **Scalability:** Statelessness allows requests to be handled by any server, simplifying scaling.
*   **Simplicity & Interoperability:** Relies on well-understood, ubiquitous web standards (HTTP, URI), reducing vendor lock-in.
*   **Discoverability:** Hyperlinks embedded in representations guide clients through the API, acting as a built-in discovery mechanism.
### HTTP
HTTP is the primary protocol for RESTful interactions. Understanding its mechanics is crucial.

**(ANATOMY OF HTTP REQUEST)** A client sends a request with:
*   **Request Line:** `<VERB> <URI> HTTP/<version>` (e.g., `GET /parts/00345 HTTP/1.1`).
*   **Headers:** Metadata (e.g., `Content-Type: app/json`, `Authorisation: Bearer ...`).
*   **Body (optional):** The payload data (e.g., JSON or XML for POST/PUT).
![[anatomy-of-http-req.png\|200]]
![[get-post-req-ex.png\|400]]
**(ANATOMY OF HTTP RESPONSE)** The server replies with:
*   **Status Line:** `HTTP/<version> <Status Code> <Reason>` (e.g., `HTTP/1.1 200 OK`).
*   **Headers:** Metadata about the response.
*   **Body (optional):** The requested representation (e.g., HTML, JSON).
![[anatomy-of-http-resp.png\|200]]
**(HTTP STATUS CODES)** Key categories include:
*   **2xx Success:** `200 OK` (request succeeded), `201 Created` (resource created).
*   **3xx Redirection:** `301 Moved Permanently`.
*   **4xx Client Error:** `400 Bad Request`, `401 Unauthorised`, `404 Not Found`.
*   **5xx Server Error:** `500 Internal Server Error`.
**(ADDITIONAL HTTP VERBS)** Beyond CRUD, HTTP defines other verbs like `HEAD` (get headers only), `PATCH` (partial update), and `OPTIONS` (discover allowed operations).

**Check of Understanding**
> **Question:** You are designing a web API for a university's course registration sys. You need operations to: a) retrieve a list of all courses for a term, b) allow a student to enroll in a specific course, and c) allow an admin to update a course's maximum capacity. Design the RESTful endpoints (URIs and HTTP methods) for these operations, justifying your choices.
> **Answer:**
> a) **Retrieve course list:** `GET /terms/{termId}/courses`
>    *   Justification: `GET` is for safe retrieval. The URI is hierarchical, listing courses as sub-resources of a specific term.
> b) **Student enrollment:** `POST /courses/{courseId}/enrollments`
>    *   Justification: `POST` is for creating a new subordinate resource. The enrollment is created as a new sub-resource under the specific course. The student's ID would be in the request body.
> c) **Update course capacity:** `PUT /courses/{courseId}`
>    *   Justification: `PUT` is for complete replacement/update of a resource. The request body would contain the full or partial updated course representation, including the new `maxCapacity` field. Alternatively, `PATCH` could be used for a partial update.
## 7. Programming RESTful Web Services
### REST: Quick Recap
*   **Architectural Style:** Built on web standards (HTTP, URI).
*   **Core Concepts:** Everything is a **resource** identified by a **URI**. Resources are manipulated via a **uniform interface** of HTTP methods (GET, POST, PUT, DELETE).
*   **Client-Server:** REST client accesses/modifies resources on a REST server.
*   **Multiple Representations:** A single resource can have different representations (JSON, XML, text), requested via HTTP content negotiation.
*   **Stateless:** Each request contains all necessary context.
### REST APIs: Examples
Major platforms provide RESTful APIs for programmatic access:
*   **Google APIs:** Custom Search, Maps, YouTube (e.g., `GET https://www.googleapis.com/customsearch/v1?key=API_KEY&q=query`).
*   **X (Twitter) API:** For reading and writing tweet data.
*   **Amazon Web Services (AWS):** Many services like Amazon S3 expose RESTful endpoints.
*   **OpenAI API:** For accessing AI models like GPT via HTTP requests.
### Reference implementations:
#### Python (Flask Restful)
**Flask-RESTful** is a lightweight Python extension for Flask that simplifies building REST APIs.
**(BASIC EXAMPLE)** A minimal API that returns JSON.
```python
from flask import Flask
from flask_restful import Resource, Api
app = Flask(__name__)
api = Api(app)
class HelloWorld(Resource):
    def get(self):
        return {'hello': 'world'}
api.add_resource(HelloWorld, '/')
if __name__ == '__main__':
    app.run(debug=True)
```
Running this (`python api.py`) and calling `curl http://127.0.0.1:5000/` returns `{"hello": "world"}`.
**(CRUD RESOURCE EXAMPLE)** A simple in-memory "todo" API demonstrating HTTP methods.
```python
from flask_restful import Resource, reqparse
todos = {}
class TodoSimple(Resource):
    def get(self, todo_id):
        return {todo_id: todos[todo_id]} # GET to retrieve
    def put(self, todo_id):
        todos[todo_id] = request.form['data'] # PUT to create/update
        return {todo_id: todos[todo_id]}
api.add_resource(TodoSimple, '/<string:todo_id>')
```
*   **`PUT /todo1 -d "data=Remember the milk"`** creates/updates the item.
*   **`GET /todo1`** retrieves it.
#### Java (Jersey)
**Jersey** is the reference implementation for **JAX-RS (Java API for RESTful Web Services)**, a Java specification. It uses annotations to map Java classes to REST resources.
**(JAX-RS ANNOTATIONS)** Key annotations for defining REST endpoints:

| Annotation | Description |
| :--- | :--- |
| `@Path("/path")` | Defines the URI path for the resource class or method. |
| `@GET`, `@POST`, `@PUT`, `@DELETE` | Specifies the HTTP method the method responds to. |
| `@Produces(MediaType.TEXT_PLAIN)` | Declares the MIME type(s) the method returns (e.g., `"application/json"`). |
| `@Consumes(MediaType.APPLICATION_JSON)` | Declares the MIME type(s) the method accepts in the request body. |
| `@PathParam("id")` | Binds a URI path segment to a method parameter. |

**(EXAMPLE: SIMPLE CALCULATOR)** A REST service exposing `add` and `subtract` operations.
**1. Create the RESTful Web Service:**
```java
import javax.ws.rs.*;
@Path("/calc") // Base path for this resource
public class CalcREST {
    // Handles GET /calc/add/{a}/{b} and returns plain text
    @GET @Path("/add/{a}/{b}")
    @Produces(MediaType.TEXT_PLAIN)
    public String addPlainText(@PathParam("a") double a, 
                               @PathParam("b") double b) {
        return (a + b) + "";
    }
    // Handles same path but returns XML if client accepts it
    @GET @Path("/add/{a}/{b}")
    @Produces(MediaType.TEXT_XML)
    public String addXML(@PathParam("a") double a, 
                         @PathParam("b") double b) {
        return "<?xml version=\"1.0\"?><result>" + (a + b) + "</result>";
    }
    // Similar method for @Path("/sub/{a}/{b}")
}
```
**2. Publish the Service:** A main class to start an embedded HTTP server.
```java
public class CalcRESTStartUp {
    static final String BASE_URI = "http://localhost:9999/calcrest/";
    public static void main(String[] args) throws IOException {
        HttpServer server = HttpServerFactory.create(BASE_URI);
        server.start();
        sys.out.println("Server running. Press Enter to stop.");
        sys.in.read();
        server.stop(0);
    }
}
```
**3. Create a Client:** Java code to consume the service.
```java
Client client = Client.create();
WebResource addResource = client.resource("http://localhost:9999/calcrest")
                                 .path("calc/add/10/5");
// Request plain text
String textResult = addResource.accept(MediaType.TEXT_PLAIN).get(String.class);
// Request XML
String xmlResult = addResource.accept(MediaType.TEXT_XML).get(String.class);
```
**(OTHER FRAMEWORKS)** Other popular implementations include:
*   **Django REST Framework (Python):** A powerful, feature-rich toolkit for building Web APIs in Django.
*   **RESTEasy (Java):** A JBoss project, another full JAX-RS implementation.
### Data encoding and RPC
While REST over HTTP/JSON is dominant, other efficient data encoding formats are used, particularly in **RPC (Remote Procedure Call)** frameworks:
![[data-encoding-rpe-ex.png\|100]]
*   **Protocol Buffers (protobuf):** Google's language-neutral, platform-neutral mechanism for serializing structured data. It is smaller and faster than XML/JSON. Used extensively in **gRPC**, a modern high-performance RPC framework.
*   **Apache Thrift:** A scalable cross-language service development framework, combining a software stack with a code generation engine.
*   **Apache Avro:** A data serialisation sys providing rich data structures, a compact binary format, and direct code generation.
*   **Gson:** A Java library from Google to convert Java Objects into JSON and vice-versa.

**Check of Understanding**
> **Question:** You need to build a high-throughput internal microservice for real-time financial trade matching. The service requires very low latency, strict interface contracts, and must support streaming updates. Would you choose a REST/JSON approach using Flask or Jersey, or a gRPC/protobuf approach? Justify your choice based on the technical characteristics discussed.
> **Answer:** Choose **gRPC/protobuf**.
> *   **Performance & Latency:** gRPC uses **HTTP/2** (multiplexing, header compression) and **Protocol Buffers** (binary, efficient serialisation), leading to much lower latency and higher throughput than REST/JSON over HTTP/1.1, which is critical for real-time trading.
> *   **Interface Contracts:** Protobuf `.proto` files provide strict, forward/backward compatible interface definitions and enable automatic, type-safe client/server code generation. REST/JSON interfaces are looser and require manual validation.
> *   **Streaming:** gRPC has first-class support for **bidirectional streaming**, perfect for pushing continuous trade updates. REST would require inefficient polling or bespoke solutions like WebSockets.
> While Flask/Jersey REST is excellent for public, web-centric APIs, gRPC/protobuf is optimised for high-performance, contract-first internal service communication.
## 8. Microservices, Nanoservices and Serverless
### Recap: SOAs
SOAs established the foundational principles of **decentralised middleware**, a **strong emphasis on loose coupling** between components, and **adherence to well-supported standards** for cross-system communication.
### Microservices
Microservices are a specific, fine-grained implementation of the SOA pattern. An app is decomposed into a suite of **small, independent, and loosely coupled services**. Each service runs in its own process, communicates via lightweight mechanisms (often HTTP/REST or messaging), and is built around a specific business capability.
![[microservices-ex.png\|400]]
**(KEY FEATURES)**
*   **Specified Functionality:** Each service has a single, well-defined responsibility.
*   **Independent Deployment:** Services can be developed, tested, and deployed in isolation, enabling continuous delivery.
*   **Organised by Business Domain:** Services are structured around business processes (e.g., "User Service," "Order Service," "Payment Service").
**(ARCHITECTURAL SHIFT)** This represents a move from a monolithic architecture, where all functionality is bundled together, to a DS of collaborating services.
![[monolith-to-microservices.png\|200]]
**(CONTAINERS & MICROSERVICES)** Containers (e.g., Docker) are the ideal deployment vehicle for microservices. They package an app with all its dependencies into a single, lightweight, portable artefact. This provides the isolation and environment consistency that microservices require, without the overhead of a full virtual machine.

| Virtual Machine     | Container                  |
| ------------------- | -------------------------- |
| ![[vm-arch-ex.png\|400]] | ![[container-arch-ex.png\|400]] |

![[apps-and-dependencies-deployed-as-artifact.png.png\|300]]
### Nanoservices
Nanoservices take the microservices concept to an extreme level of granularity. A nanoservice is a **single-function service**—often just a few lines of code—that performs one very specific task. While microservices are "small," nanoservices are "tiny." This extreme decomposition can lead to massive operational complexity (managing thousands of services) and is generally considered an anti-pattern unless there is a specific, compelling need for such fine granularity.
### Serverless Computing
Serverless computing is a cloud execution model where the cloud provider dynamically manages the allocation and provisioning of servers. The developer writes **code (functions)** without any concern for the underlying infrastructure. The core principle is **"No Servers to Manage."**
### Function as a Service (FaaS)
FaaS is the specific implementation paradigm at the heart of serverless computing. It allows developers to deploy **individual functions** (blocks of code) that are executed in response to events. The platform automatically scales, runs, and bills for these functions down to the millisecond of execution time.
**(EXECUTION MODEL)** When an event triggers a function, the FaaS platform checks if an instance is already running ("warm"). If not, it performs a **cold start**: loading the function code, provisioning a runtime container, and then executing it. Subsequent invocations may reuse the warm instance for much lower latency.
![[serverless-execution-model-ex.png\|400]]
### Architectural Support
Serverless architectures are inherently **event-driven**. apps are composed of functions triggered by events from various sources (HTTP requests, db changes, message queues, timers).
![[serverless-apps-logic-flow.png\|400]]
**(KEY METRICS & CHALLENGES)**
1.  **Cold Start Latency:** The delay when instantiating a new function instance. This overhead is **not negligible** and is a critical performance consideration.
![[cold-vs-warm-serverless-app.png\|300]]
2.  **Statelessness:** Functions are inherently stateless. Any required state (user session, intermediate data) must be stored in external services (databases, caches), adding complexity and latency.
3.  **Function Composition:** Building complex apps requires orchestrating multiple functions, which can be done through sequential chaining, event-driven flows, or dedicated orchestration services.
**(EVOLUTION OF GRANULARITY)** The trend moves from large monoliths, to decomposed microservices, and further to ephemeral, event-triggered functions.
![[monolith-to-microservices-to-nanoservices.png\|200]]
### Solutions
**(COMMERCIAL & OPEN SOURCE FaaS PLATFORMS)**
*   **AWS Lambda:** The pioneer. Functions are triggered by events from almost any AWS service (S3, API Gateway, DynamoDB) or custom HTTP endpoints.
    ![[aws-lambda-ex.png\|400]]
![[aws-lambda-arch-ex.png\|400]]
*   **Microsoft Azure Functions:** A polyglot platform supporting multiple languages. It integrates deeply with Azure services and offers easy deployment from source control.
    ![[azure-functions-steps-ex.png\|400]]
*   **Google Cloud Functions, IBM Cloud Code Engine, Apache OpenWhisk, Knative:** Other major platforms providing similar FaaS capabilities, often with unique strengths in open-source integration or Kubernetes-native operation.

**(BENEFITS)**
*   **No Server Management:** Eliminates operational overhead.
*   **Continuous, Automatic Scaling:** Scales from zero to thousands of instances seamlessly.
*   **Pay-Per-Use Cost Model:** You only pay for the compute time your functions consume, never for idle resources.

**Check of Understanding**
> **Question:** Your team is designing a new photo-sharing app. A core feature requires generating multiple thumbnail sizes whenever a user uploads an image. Should this feature be implemented as part of a monolithic "Upload Service," as a standalone microservice, or as a serverless function? Justify your choice based on architectural concepts.
> **Answer:** The thumbnail generation is an ideal candidate for a **serverless function (FaaS)**. Justification:
> * **Event-Driven:** It is triggered by a specific, infrequent event (image upload).
> * **Variable, Bursty Workload:** Upload frequency is unpredictable. Serverless scales to zero when idle (cost-effective) and instantly handles sudden spikes (e.g., many users uploading at once).
> * **Short-Running, Stateless Task:** The job is compute-intensive but short-lived. It reads from object storage (the original image) and writes back to object storage (the thumbnails), requiring no persistent internal state.
> * **Decoupled & Focused:** Implementing it as a standalone function keeps it completely decoupled from the main upload logic, adhering to the microservices principle of single responsibility, but with the operational simplicity of serverless. A full microservice would require managing a constantly running container for a sporadic task, which is less efficient.
## 9. Naming Pt.1 - *Chapter 5*
### Fundamental concepts
**(DEFINITIONS)** In a DS, an **entity** (resource like a file, service, or host) must have a **name** to be accessed. A name is a string (e.g., `server42`, `/home/file.txt`). To locate and interact with an entity, its name must be **bound** to its attributes (e.g., IP address, port). The process of translating a name into these attributes is called **name resolution**.
![[name-res-ex.png\|300]]
**(ACCESS POINTS AND ADDRESSES)** An **access point** is a special entity used to operate on another entity. Its name is an **address** (e.g., `192.168.1.5:8080`). An entity can have multiple or changing access points. For flexibility, we prefer **location-independent names** (stable identifiers) rather than directly using addresses.
**(FUNCTION OF A NAMING System)** A naming sys manages **bindings** between names and entity attributes. Its primary function is **name resolution**. Secondary functions include creating/deleting bindings, listing names, and organising the **namespace** (the set of all valid names).
### Classes of naming systems
Three broad classes exist:
1.  **Flat Naming:** Uses unstructured identifiers (IDs). This lecture's focus.
2.  **Structured Naming:** Uses human-readable, hierarchical names (e.g., file paths, URLs). (Covered in Pt.2).
3.  **Attribute-Based Naming:** Entities are described/searchable by attributes (e.g., "printer, colour, floor 3"). (Covered in Pt.2).
### Namespaces
This section focuses on **flat naming systems**, where entities have non-hierarchical, often meaningless identifiers (IDs). The key challenge is **locating an entity when given only its flat ID**.

**(BROADCASTING)** One simple method is to **broadcast** a query asking "who has this ID?" across the local network.
*   *Example: Address Resolution Protocol (ARP)* resolves an IP address to a MAC address by broadcasting "Who has IP `192.168.1.10`?" on the local LAN.
![[arp-pro-ex.png\|200]]
*   **Drawback:** Does **not scale** beyond local networks; generates excessive traffic in large systems.

**(FORWARDING POINTERS)** When an entity moves, it leaves a **forwarding pointer** at its old location pointing to the new one. To locate it, a client follows the chain of pointers.
![[naming-pt1-ssp-chains-ex.png\|400]]
*   *Example in RMI:* A **client stub** can hold a forwarding pointer to the **server stub** (a **Stub-Scion Pair** or SSP). Shortcuts can be stored to improve future lookups.
![[ssp-chains-a-and-b.png\|400]]
*   **Drawbacks:** Long chains are **not fault-tolerant** (a broken pointer breaks resolution) and increase **latency**.

**(HOME-BASED APPROACH)** A central **home location** maintains the current address of a mobile entity.
*   *Example: Mobile IP.* A mobile host has a permanent **home address**. A **home agent** on its home network tracks its current **care-of-address**. Packets sent to the home address are **tunnelled** by the home agent to the current location.
![[mobile-ip-world-map-ex.png\|400]]
*   **Drawbacks:** The fixed home can become a **single point of failure** and a **performance bottleneck**, especially if the entity and client are far from the home (poor **geographical scalability**).
### Naming graphs
This metaphor describes structures for organising and resolving names on a larger scale.

**(DISTRIBUTED HASH TABLES - DHTs)** A DHT is a decentralised, structured overlay network that provides a hash-table-like interface: `put(key, value)` and `get(key)`. Entities (values) are assigned a unique **key** (via hashing). Responsibility for storing the `(key, address)` binding is distributed across all nodes in the network based on the key's value.
![[distributed-hash-table-principle.png\|400]]
![[dht-ex.png\|300]]
*   *Example:* In a Chord DHT, nodes are arranged in a logical ring. An entity with key `k` is stored on the node with the smallest ID ≥ `k` (its **successor**). Lookups are routed efficiently around the ring in `O(log N)` hops.
*   **Advantages:** Highly **scalable** and **decentralised**; no single point of failure.

**(HIERARCHICAL APPROACHES)** This method organises directory nodes into a **tree** that mirrors the network's hierarchical structure (e.g., countries, orgs, departments). Directories store location information for entities within their subtree.
![[hierarchical-tree-ex.png\|300]]
*   **(LOOKUP OPERATION)** To find an entity, start at the local leaf node. If it doesn't know the location, the request moves **up** the tree until a directory that knows about the entity is found (ultimately, the root knows all). The request then moves **down** via pointers to the leaf node holding the address.
![[lookup-operation.png\|300]]
*   **(INSERT OPERATION)** To insert a new entity address, the request moves up the tree until finding a node that already knows about the entity. A chain of **forwarding pointers** is then created back down to the leaf node storing the new address.
![[insert-operation-a-b.png\|400]]
*   **Trade-off:** More structured and scalable than broadcasting or pure home-based, but insert/lookup operations involve multiple network hops.

**Check of Understanding**
> **Question:** You are designing a peer-to-peer file-sharing sys where millions of nodes join and leave frequently, and files (entities) are referenced by a unique hash of their content (a flat name). Which flat naming resolution scheme discussed would be most suitable and why? What is a key operational challenge you would need to manage?
> **Answer:** A **Distributed Hash Table (DHT)** would be most suitable.
> **Why:** The unique file hash serves as a perfect **key**. A DHT efficiently and **decentralises** the responsibility of storing the binding between this key and the current node(s) storing the file. It scales to millions of nodes and handles churn (nodes joining/leaving) gracefully through its structured overlay and replication mechanisms.
> **Key Challenge:** **Maintaining the DHT's consistency and routing tables** in the face of high churn. As nodes frequently join and leave, the sys must efficiently update successor pointers and replicate data to ensure `get(key)` operations remain reliable and efficient.
## 10. Naming Pt.2 - *Chapter 5*
### Structured naming
Structured names are human-readable and often hierarchical (e.g., `/home/user/file.txt`, `www.example.com`). They are composed from simple names and organised within a **namespace**.
#### Namespaces
A namespace is the set of all valid names recognised by a sys. It can be represented as a **directed graph** with two node types:
*   **Leaf Nodes:** Represent named entities (files, hosts, services). They store the entity's attributes or state.
*   **Directory Nodes:** Act as catalogs. They store a **directory table** of (`edge-label`, `node-identifier`) pairs, defining the outgoing edges.
Each path through the graph, identified by a sequence of labels (e.g., `L1, L2, ... Ln`), is a structured name.
![[naming-file-sys-ex.png\|200]]
*Example:* A filesystem namespace. A single root node is common.
![[single-root-naming-ex.png\|400]]

#### Name resolution
Name resolution is the process of traversing the naming graph to translate a name into the entity it refers to. The starting point is defined by a **closure mechanism**.
**(CLOSURE MECHANISM)** This is the rule or context that determines the **initial node** (index node) for resolution.
*   *Example (Unix Filesystem):* To resolve `/home/steen/mbox`, the closure mechanism provides access to the root directory's inode. The OS finds this via the disk's superblock.
![[closure-mechanism-unix-ex.png\|400]]
*   *Other Examples:* `www.distributed-systems.net` starts resolution at a known DNS server; `0031 20 598 7784` starts at the local telephone exchange.
**(PATHNAME RESOLUTION)** The process of walking the graph. In a Unix filesystem, every file/directory is an **inode**. A directory is a file mapping names to inode numbers. Resolution walks this chain.
![[inode-ex.png\|400]]
![[path-name-resolution-ex.png\|501]]

**(LINKING)** Creates **aliases**, allowing multiple names for one entity.
*   **Hard Link:** A direct directory entry pointing to the same inode. Multiple absolute paths refer to the same node.
*   **Symbolic (Soft) Link:** A special leaf node that stores the *pathname* of the target entity as data.
![[linking-unix-ex.png\|400]]
**(MOUNTING)** A key mechanism for **integrating different namespaces** transparently. It associates a node in the current namespace (the **mount point**) with the root (**mounting point**) of a **foreign namespace** (e.g., a remote filesystem).
![[mounting-sys-ex.png\|400]]
*Example:* Mounting a remote NFS export `/remote/vu` to the local directory `/home/steen/remote` allows accessing files via `/home/steen/remote/mbox`.

#### Implementation of a namespace
For large-scale, distributed namespaces (like DNS), the naming graph is distributed across many **name servers**. The namespace is partitioned into logical layers:
1.  **Global Layer:** Stable, high-level directory nodes (e.g., DNS root and top-level domains `.com`, `.uk`). Managed by different administrations.
2.  **Administrational Layer:** Mid-level directories for orgs (e.g., `example.com`). Each zone is managed independently.
3.  **Managerial Layer:** Low-level nodes within a single administration (e.g., `www.example.com`, `mail.example.com`). Often replicated for robustness and performance.
![[dns-partitioning-ex.png\|400]]
![[namespace-implementation-ex.png\|400]]

**(DOMAIN NAME System - DNS)** DNS is the canonical distributed, hierarchical naming sys for the internet.
*   **Zone Data:** Authoritative data for a domain portion, including records like **A** (address), **NS** (name server), **MX** (mail exchange), and **CNAME** (canonical name alias). Data is cached with a **Time-To-Live (TTL)**.
![[dns-node-info.png\|300]]
*   **Resolution Methods:**
    *   **Iterative Resolution:** The contacted server returns the **next server** to ask. The client does the legwork.
![[dns-iterative.png\|400]]
    *   **Recursive Resolution:** The contacted server **forwards the query** itself, eventually returning the final answer to the client.
![[recursive-ex.png\|400]]
*   **Trade-offs:** Recursive resolution places more load on name servers but enables more effective caching at intermediate servers. Iterative resolution is less demanding on individual servers.
![[scalability-issues.png\|400]]

### Attribute-based naming
Also known as **directory services**. Instead of looking up an entity by its precise name, you search for it using a set of **descriptive attributes**. This is like using a "yellow pages" versus a "white pages" directory.

**(LDAP - LIGHTWEIGHT DIRECTORY ACCESS PROTOCOL)** A standard protocol for directory services. Data is organised in a **Directory Information Tree (DIT)**, where each entry is a collection of (`attribute`, `value`) pairs and is uniquely named by a **Relative Distinguished Name (RDN)**.
![[ldap-ex.png\|400]]
![[dir-info-tree-ex.png\|400]]
*   *Example Search:* A query like `(C=UK)(O=Leeds University)(OU=Computing)(CN=Main Server)` would return all directory entries matching those attributes, potentially revealing multiple host servers.
*   *Example Table:* Two entries distinguished by their `HostName` RDN.

**(JAVA NAMING AND DIRECTORY INTERFACE - JNDI)** A Java API that provides a uniform interface to multiple different naming and directory services (LDAP, DNS, CORBA, RMI registry) through pluggable **service providers**.
![[jndi-arch-ex.png\|300]]
It allows apps to bind objects to names and perform lookups and searches independent of the underlying service implementation.

**Check of Understanding**
> **Question:** Your distributed app needs to find an available, colour, A3-capable printer located on the same floor as the user. The sys knows the user's current floor and the printers are registered with various attributes (type, capabilities, location). Would you use DNS, the local filesystem namespace, or an LDAP-based directory service to implement this discovery? Justify your choice.
> **Answer:** You would use an **LDAP-based directory service**.
> *   **DNS** is designed for **structured name resolution** (hostname to IP address). It is poorly suited for searching by arbitrary, multi-valued attributes like "colour-capable" and "A3".
> * The **filesystem namespace** is also for structured, hierarchical name lookup, not attribute-based querying.
> * An **LDAP directory** is specifically built for **attribute-based naming**. Each printer could be an entry with attributes like `cn=Printer42`, `type=laser`, `capabilities=colour,A3,duplex`, `locationFloor=5`. The app can then perform a search query like `(&(type=laser)(capabilities=colour)(capabilities=A3)(locationFloor=5))` to find all matching printers. This is the exact functionality directory services provide. Directory Services
## 11. Timing and Synchronisation - *Chapter 6*
### Synchronisation in a DS
In DSs, there is **no global agreement on time**. Each machine's internal clock (typically a **quartz crystal oscillator**) drifts at a unique rate, measured in **parts per million (ppm)**. This **clock skew** means an event that occurred later can be assigned an earlier timestamp, causing issues for logs, schedules, failure detection, and event ordering.
![[timing-clock-ex.png\|400]]
### Internal and external physical clocks
**(INTERNAL CLOCKS)** A computer's internal clock counts oscillations of a quartz crystal, triggering interrupts ("ticks") to maintain a software clock. The drift rate is bounded by a **maximum drift rate (ρ)**, meaning $1-ρ ≤ dC/dt ≤ 1+ρ$.

**(EXTERNAL STANDARDS & UTC)** The worldwide standard is **UTC (Coordinated Universal Time)**, based on atomic clocks (TAI) with leap seconds added to align with solar time. Machines need to synchronise their internal clocks with an external source like UTC.
![[utc-time-standard.png\|400]]
**(TIME REPRESENTATION)** Common formats are **Unix time** (seconds since epoch) and **ISO 8601**. A critical bug related to handling a **leap second** on 30 June 2012 caused widespread crashes, highlighting the importance of robust timekeeping.
### Clock synchronisation algorithms
Algorithms aim to minimise skew between clocks. If one machine has a UTC receiver, it acts as a **time server**.
#### Drift as a Function of UTC
• If two clocks are drifting from UTC in opposite directions, at a time dt after synchronisation they may be as much as 2$\rho$dt apart.
![[drift-as-func-utc.png\|200]]
25/02/1991. Patriot Missile Failure: Software error in the sys clock. Accumulated clock drift.
**(CRISTIAN'S ALGORITHM)** A client requests the time from a server. It estimates the network delay and adjusts its clock to `server_time + (round_trip_delay / 2)`. It must apply the correction gradually to avoid time running backward.
![[christian-algo.png\|200]]
![[est-time-over-network.png\|300]]
**Round-trip network delay** $\delta = (t_4 - t_1) - (t_3 - t_2)$
**Estimated server time** when client receives response: $t_3 + \delta/2$
**Estimated clock skew**: $\delta = t_3 + \delta/2 – t_4 = (t_2 - t_1 + t_3 - t_4)/2$

**(BERKELEY ALGORITHM)** Used when no UTC source is available. A **time daemon** (coordinator) polls all machines for their time, calculates the **average** (ignoring outliers), and instructs each machine on how much to adjust its clock (forward or backward).
![[berkley-algo-time.png\|200]]
1. Time daemon clock $D$ shows **3:00**, requests network clock values.
2. Network clocks $C_n$, return their values to the daemon $C_1 =$ 2:50, $C_2 =$ 3:25.
3. Daemon calculates average: $$ \begin{align}
( \ \text{diff}(D, C_1) + \text{diff}(D, C_2)\ ) / \text{no. clocks} \\
( \ \text{diff}(\text{3:00}, \text{ 3:50}) + \text{diff}(\text{3:00}, \text{ 3:25})\ ) / 3 \\
( 0 -10 + 25 ) / 3 = +5 \text{ minutes}\\
\end{align}$$ 4. Daemon adjusts all clocks to match its own (3:00) + 5 more minutes= 3:05.
### Election Algorithm
When a coordinator (e.g., the time daemon in Berkeley) fails, an **election algorithm** selects a new leader.
**(BULLY ALGORITHM)** The process with the highest ID wins.
1.  Any process noticing coordinator failure sends **ELECTION** messages to processes with higher IDs.
2.  If it gets no **OK** response, it wins and announces **I WON** to lower-ID processes.
3.  If it gets an **OK**, it waits for the **I WON** message.
4.  A process receiving an **ELECTION** message replies **OK** and starts its own election.
![[bully-algo-ex.png\|400]]
![[bully-algo-ex-pt-2.png\|300]]
It uses `O(n²)` messages.
**a)** Process 7 has crashed and Process 4 holds an election 
**b)** Process 5 and 6 respond, telling 4 to stop
**c)** Now 5 and 6 hold elections **d)** Process 6 tells 5 to stop **e)** Process 6 wins and tells everyone.
If P7 is restarted, it will send all the others a COORDINATOR msg and bully them into submission.

**(RING ALGORITHM)** Processes are arranged in a logical ring. A process starting an election sends an **ELECTION** message with its ID to its successor. Each successor adds its own ID and forwards the message. When the message returns to the initiator, it circulates a **COORDINATOR** message with the highest ID as the new leader.
![[ring-algo.png\|400]]
Election algorithm using a ring. The solid line shows the election messages initiated by P6; the dashed one those by P3. We can see what happens if two processes, P3 and P6, discover simultaneously that the previous coordinator, process P7, has crashed. Each of these builds an ELECTION msg and each of them starts circulating its message, independent of the other one. 
	Eventually, both messages will go all the way around, and both P3 and P6 will convert them 
into COORDINATOR messages, with exactly the same members and in the same order. When both have gone around again, both will be removed. It does no harm to have extra messages circulating; at worst it consumes a little bandwidth, but this is not considered wasteful.
### Network time protocol (NTP)
**NTP** is the hierarchical, robust protocol used to synchronise clocks across the Internet to UTC.
**(STRUCTURE)** Servers are organised in **strata**.
*   **Stratum 1:** Primary servers directly connected to UTC sources (e.g., atomic clocks, GPS).
*   **Stratum 2:** Synchronise with stratum 1 servers.
*   **Stratum 3+ and clients:** Synchronise with higher-stratum servers. Accuracy decreases with stratum number.

**(CLOCK CORRECTION)** NTP calculates the offset (θ) between client and server.
Systems that rely on clock sync need to monitor clock skew!
*   **|θ| < 125 ms:** **Slew** the clock (adjust speed gradually).
*   **125 ms ≤ |θ| < 1000 ms:** **Step** the clock (jump to the correct time).
*   **|θ| ≥ 1000 ms:** **Panic** (do nothing, require manual intervention).

**Check of Understanding**
> **Question:** A distributed sensor network for a scientific experiment logs each reading with a local timestamp. During analysis, you notice events from Sensor A always appear 50ms before equivalent events from Sensor B, but you suspect they are simultaneous. You know both sensors synchronise via NTP to the same stratum-2 server. List three distinct potential causes for this consistent 50ms offset in the timestamps.
> **Answer:**
> 1.  **Asymmetric Network Delay:** The most likely cause. NTP assumes symmetric network paths. If the path from the NTP server to Sensor A has a different latency than the path to Sensor B (e.g., due to different network hops or congestion), the calculated clock offset will be inaccurate. A consistent 50ms difference suggests a fixed routing asymmetry.
> 2.  **Different NTP Strata or Servers:** If Sensor B is synchronising indirectly (e.g., getting time from Sensor A as a stratum-3 source) while Sensor A syncs directly with the stratum-2 server, Sensor B's time would have an extra hop of latency and potential error added.
> 3.  **Local Processing Delay on Sensor B:** The timestamp might be applied *after* some internal processing or queueing on Sensor B, adding a constant delay. This is a software/architectural issue, not a clock synchronisation error per se, but results in the same observable offset.
## 12. Consistency and Replication Pt.1 - *Chapter 7*
### Introduction to the problem
A fundamental issue in DSs is **replication**: maintaining multiple copies of the same data (e.g., a file, db record) across different machines.
![[replica-db-ex.png\|300]]
**(REASONS FOR REPLICATION)**
*   **Reliability:** If one replica crashes, the sys can continue using another, increasing fault tolerance.
*   **Performance & Scalability:**
    *   **Scaling for Size:** Distributes client load across multiple servers.
    *   **Scaling Geographically:** Places data closer to users, reducing access latency (e.g., a content delivery network or CDN).

**(THE CONSISTENCY PROBLEM)** The core challenge is **keeping replicas consistent**. When one copy is updated, all other copies must eventually reflect that change. If updates aren't propagated correctly, replicas diverge, leading to stale data and incorrect results.

**(THE PERFORMANCE-SCALE DILEMMA)** Strict consistency (synchronous replication) requires that all replicas agree on the order of **conflicting operations** (Read-Write or Write-Write conflicts). Achieving this global agreement requires **synchronisation**, which is slow, communication-intensive, and hurts scalability.
*Solution:* To build scalable systems, we often **relax the consistency requirements**, accepting that replicas may be temporarily inconsistent. The specific rules for what is acceptable are defined by **consistency models**.
### Data-centric consistency models
A **consistency model** is a formal contract between a distributed data store and its client processes. It defines the possible results of **read** and **write** operations when they are executed concurrently by multiple processes, specifying what values a read operation is allowed to return.
![[data-store-ex.png\|200]]

#### Continuous Consistency
This model quantifies inconsistency, allowing apps to specify *how much* inconsistency they can tolerate, rather than requiring absolute consistency. A replica can deviate from others in three dimensions:
1.  **Numerical Deviation:** The absolute difference in value (e.g., a bank account balance on replica A is £1000, on replica B it's £950, a deviation of £50).
2.  **Staleness Deviation:** The maximum time a replica's value is older than another's.
3.  **Ordering Deviation:** The number of pending (uncommitted/not-yet-propagated) update operations.

**(CONSISTENCY UNIT - CONIT)** The data unit over which these deviations are measured is called a **Conit**. An app defines a Conit, and the sys ensures deviations stay within specified bounds.

**(CONIT EXAMPLE)** A fleet manager tracks average fuel cost. The Conit consists of three variables: gallons tanked (`g`), price paid (`p`), and distance driven (`d`). These are replicated.
![[conit-replicas.png\|400]]
*   **Vector Clocks** track known operations from each replica.
*   **Order Deviation (3 for A):** A has 3 local, tentative operations pending permanent commitment.
*   **Numerical Deviation (2,482 for A):** The sum of the values from operations A has *missed* from B (e.g., two missed updates totalling £70 + £412).
*   An app could specify: "Never let the numerical deviation for the 'fuel cost' Conit exceed £1,000." The sys would then trigger synchronisation before this bound is breached.
#### Sequential Consistency
This is a stricter model. The result of any execution must be equivalent to the results of some **sequential execution** of all processes' operations, where the operations *of each individual process* appear in this sequence in the **order specified by its program** (program order).

**(DEFINITION & DIAGRAM)** It does *not* require operations from different processes to happen in real-time order, only that all processes agree on a single, global sequential order of *all* operations.
![[seq-consistency.png\|250]]
In Diagram (a): P2 reads `x` as `NIL`, then later reads `a`. This is sequentially consistent because we can construct a valid sequential order: `W1(x)a` -> `R2(x)NIL` -> `R2(x)a`.

**(THE ROLE OF TIME)** Sequential consistency is about logical order, not physical time.
![[seq-consistency-no-time.png\|501]]
*   **Diagram (a) is Valid:** P3 and P4 *both* see `W2(x)b` happening before `W1(x)a`. They agree on the interleaving.
*   **Diagram (b) is a Violation:** P3 and P4 see *different* interleavings of the writes. There is no single sequential order that satisfies both views.
**(EXAMPLE WITH PROGRAMS)**
![[seq-print-ex.png\|400]]
For three processes with the above code, sequential consistency does *not* guarantee that all prints will show `1,1,1`. It only guarantees that the execution is equivalent to *some* sequential interleaving of the statements that respects each process's program order. Many interleavings are possible, leading to various `(x,y,z)` print outputs (like `1,0,0`), all of which are valid under this model.

**(THE HAPPENS-BEFORE RELATION)** This is a foundational concept for defining order in DSs without a global clock.
*   **Definition:** Event `a` **happens-before** event `b` (`a → b`) if:
    1.  `a` and `b` are on the same process and `a` occurs before `b`.
    2.  `a` is the sending of a message and `b` is the receipt of that *same* message.
    3.  There exists an event `c` such that `a → c` and `c → b` (transitivity).
*   If neither `a → b` nor `b → a`, the events are **concurrent** (`a || b`).
![[happens-before.png\|200]]
- a → b, c → d, and e → f due to node execution order
- b → c and d → f due to messages m1 and m2
- a → c, a → d, a → f, b → d, b → f, and c → f due to transitivity
- a ‖ e, b ‖ e, c ‖ e, and d ‖ e
#### Causal Consistency
A relaxation of sequential consistency. It only requires that **causally related** writes be seen by all processes in the *same* order. **Concurrent writes** (writes that are not causally related) may be seen in different orders by different processes.

**(DEFINITION & DIAGRAM)** A write `W2` is causally related to a previous write `W1` if `W2` could have been influenced by knowing the result of `W1` (e.g., a process read the value written by `W1` before performing `W2`).
![[causal-consistency.png\|300]]
*   **Valid:** `W1(x)c` and `W2(x)b` are concurrent (no causal link). Therefore, P2 and P3 are allowed to see them in different orders.

**(VIOLATION EXAMPLE)**
![[violated-causal-consistency.png\|400]]
*   **Diagram (a) is a Violation:** `R2(x)a` (the read of `a` by P2) creates a causal link from `W1(x)a` to `W2(x)b`. Therefore, all processes must see `W1(x)a` before `W2(x)b`. P3 sees them in reverse order, violating causal consistency.
*   **Diagram (b) is Acceptable** for causal (but *not* sequential) consistency because `W1(x)a` and `W2(x)b` are concurrent.

**Check of Understanding**
> **Question:** A social media post and its comments are stored across three geographically distributed replicas (US, EU, Asia). A user in the US posts an update (Write A). A user in Asia reads that post and immediately posts a comment (Write B). A user in Europe then reads the original post. Under a **causally consistent** model, is it permissible for the European user to see the comment (Write B) but *not* see the original post (Write A) it replied to? Explain using the happens-before relation.
> **Answer:** **No, this is not permissible under causal consistency.**
> **Explanation using happens-before:** The Asian user's read of the post establishes a causal link: `Write A → Read A (in Asia) → Write B`. Therefore, `Write A` happens-before `Write B` (`Write A → Write B`). The causal consistency model requires that all processes see causally related writes in the same order. If the European user sees `Write B` (the comment), they must also have seen `Write A` (the original post) that causally preceded it. Seeing the effect (comment) without its cause (original post) violates the contract.
## 13. Consistency and Replication Pt.1 - Chapter 7
### Client-centric consistency models
Client-centric consistency models guarantee consistency for the **operations of a single client** across a distributed data store. They do not provide guarantees about concurrent operations from different clients. These models are crucial for mobile or intermittently-connected users who access the data store from different locations, ensuring their view remains self-consistent even as they move.

**(BASIC ARCHITECTURE)** The sys involves multiple local stores (e.g., `L1`, `L2`). A client process (`P`) performs read (`R`) and write (`W`) operations on a data item `x`. Different versions of `x` (e.g., `x1`, `x2`) exist across these stores.
![[client-centric-arch.png\|400]]
Notation: `W1(x1;x2)` means process P1 writes version `x2` based on the previous version `x1`. `W1(x1|x2)` indicates a concurrent write that does not follow from `x1`.
#### Monotonic Reads (MRs)
If a process reads a particular version of a data item, any future read by that same process must return that version or a **never-older** version. It prevents a user from seeing data "go back in time" when they switch servers.
**(EXAMPLE)** A user reads their unread email count (10 emails) on a server in London (`L1`). Later, they connect from a server in New York (`L2`). Monotonic Reads guarantees the count in New York is **at least 10**. It could be 12 (if new mail arrived), but never 9.
![[monotonic-read-ex-1.png\|300]]
*Diagram (b) shows a violation:* P1 reads `x1` at L1, but a later read at L2 returns `x2`, which is a version that was created *concurrently* to and does *not* include the updates seen in `x1`. This is not allowed.
#### Monotonic Writes (MWs)
Write operations performed by a single process must be **propagated to all replicas in the same order** they were issued. This ensures a user's sequence of updates is applied correctly everywhere.

**(EXAMPLE)** A user first updates a document's title (Write A), then updates its body text (Write B). Monotonic Writes ensures that no server will ever see Write B applied *before* Write A. This is critical for operations that depend on previous ones (e.g., installing software libraries in the correct order).
![[monotonic-write-ex1.png\|300]]
*Diagram (b) shows a violation:* The write that produced `x2` at L1 is propagated to L2 *after* a later write that produced `x3`. The order of writes seen at L2 is wrong.
#### Read Your Writes (RYWs)
The effects of a write operation by a process are **always visible to that process's subsequent read operations**, no matter from which location it reads. It prevents a user from seeing stale data after they have just updated it.

**(EXAMPLE)** You update your profile picture on a social media site (write). When you immediately refresh the page (read), you are guaranteed to see the new picture, not the old cached version.
![[read-your-write-consistency-1.png\|300]]
*Diagram (b) shows a violation:* P1 writes `x1` at L1. Later, when reading at L2, it sees `x2`, which is a version created *without* incorporating its own previous write (`x1`). This violates the "read your own writes" guarantee.
### Replica management
This involves deciding **where** to place replica servers, **what content** to put on them, and **how** to keep them consistent.
#### Content Replication
Replicas can be categorised by their initiation method:
*   **Permanent Replicas:** The initial, fixed set of replicas that form the core of the distributed data store.
*   **Server-Initiated Replicas:** Created dynamically by the sys (owner of the data) to improve performance, typically placed near clusters of demanding clients.
*   **Client-Initiated Replicas (Caches):** Created at the request of a client to locally store data for fast access (e.g., a web browser cache).
![[replica-rings.png\|400]]
#### Server-Initiated Replica Example
A sys dynamically creates temporary replicas of popular content to reduce load on the origin server and bring data closer to users. The decision is based on tracking access patterns.
**(ALGORITHM)** Servers monitor file access counts. Requests are aggregated by the server closest to the client.
![[server-initiated-replica.png\|400]]
*   If accesses for file `F` from a region exceed a **Replication threshold (R)**, a new replica of `F` is created in that region.
*   If accesses fall below a **Deletion threshold (D)**, the replica is removed.
*   If accesses are between D and R, the file might be **migrated** (moved) instead of replicated.

**(EXAMPLE)** A viral video hosted in London (`Q`) suddenly gets millions of requests from Asia. The London server sees the high access count `cntQ(P, F)` from an Asian proxy server (`P`). It then decides to create a server-initiated replica of the video on a server in Singapore to serve Asian users directly.
#### Consistency Protocols: Primary-backup Protocol with Remote Writes
This is a primary-based protocol for implementing sequential consistency. One server is designated the **primary** (or master) for a data item.

**(MECHANISM - BLOCKING)**
1.  A client sends a write request to the primary server.
2.  The primary performs the write on its local copy.
3.  The primary forwards the update to all **backup** (secondary) servers.
4.  The primary waits for acknowledgements from all backups.
5.  Only after all backups confirm, does the primary send an acknowledgement back to the client.
![[primary-bakcup-remote-write-prot.png\|400]]
**(CHARACTERISTICS & PERFORMANCE)**
*   **Strong Consistency:** Provides sequential consistency because the primary serialises all writes.
*   **Fault Tolerance:** High, as the write is confirmed on multiple servers before completion.
*   **Performance:** Write latency is high (blocking) because the client must wait for multiple network round-trips. Read performance is good (can be served locally from any replica).
#### Consistency Protocols: Primary-backup Protocol with Local Writes
A variant where the **primary role can move** to the client that wants to perform a write. This is beneficial for mobile or disconnected operation.
**(MECHANISM)**
1.  A client needing to write locates and "moves" the primary copy of the data to its local machine.
2.  The client can now perform multiple writes **locally** and very quickly.
3.  Updates are propagated to the backup servers **asynchronously** (non-blocking) after the local writes are done.
![[primary-bakcup-local-write-prot.png\|400]]
**(USE CASES & TRADE-OFFS)**
*   **Use Case 1 - Mobile Disconnected Operation:** A user takes a file primary with them on a laptop, works offline, and synchronises changes back to the network later.
*   **Use Case 2 - Performance Optimisation:** A client performing a burst of updates (e.g., a batch job) can temporarily become the primary to avoid network latency for each write.
*   **Trade-off:** Provides lower write latency for the primary holder but weakens immediate consistency guarantees for other readers until propagation is complete.

**Check of Understanding**
> **Question:** A globally distributed note-taking app uses a primary-backup protocol. A user in Tokyo starts editing a document. To minimise latency, the app makes her local device the primary for that document. She makes ten rapid edits (local writes). At the same time, a collaborator in Berlin refreshes the document.
> 1.  Under a **remote-write** protocol, what would the Berlin user see during the Tokyo edits, and what is the performance impact for the Tokyo user?
> 2.  Under the described **local-write** protocol, what is the Berlin user likely to see, and what consistency model (client-centric) is potentially violated for the Berlin user?
>
> **Answer:**
> 1. **Remote-Write:** The Berlin user would see each edit **as soon as it is fully replicated** to the backups. The Tokyo user would experience **high write latency** for each edit, as each one would need to be sent to and acknowledged by the central primary and all backups before she could continue.
> 2. **Local-Write:** The Berlin user is **likely to see stale data** (the old version) until the Tokyo user's device propagates its batch of updates. This scenario violates the **Monotonic Reads** guarantee for the Berlin user if he had previously seen a more recent version from another server. More generally, it highlights the trade-off for **Read Your Writes** consistency *across different users*.
## 14. Fault Tolerance - *Chapter 8*
The key technique for handling failures is redundancy.
### Dependability, reliability and availability in a DS
Being fault tolerant is strongly related to what are called dependable systems.
Requirements for a **dependable** sys: *Availability*, *Reliability*, *Safety* and *Maintainability*.

**(Availability)** The probability the sys is operational and ready for use at a _given instant in time_. A highly available sys has minimal downtime.
**(Reliability)** The probability the sys operates _continuously without failure_ over a specified _time interval_ instead of an instance in time. A highly reliable sys has long periods of uninterrupted service.
**(Safety)** When a sys fails temporarily, it fails in a way that does not cause catastrophic damage (e.g., a nuclear reactor control sys failing to a safe state).
**(Maintainability)** The ease and speed with which a failed sys can be repaired. High maintainability supports high availability.

**(QUANTIFYING AVAILABILITY & RELIABILITY)** Key metrics include:
The **availability** $A(t)$ of **a component** in the time interval $[0, t]$ is the *average fraction of time* that the component has been *functioning correctly* during that interval. 
The **long-term availability** $A$ of a component is defined as $A(∞)$.
Likewise, the **reliability** $R(t)$ of a component in the time interval $[0, t]$ is the *conditional probability* that it has been *functioning correctly* during that interval *given* that it was *functioning correctly* at time $T = 0$.
- **Mean Time To Failure** ($MTTF$): The average time until a component fails
- **Mean Time To Repair** ($MTTR$): The average time needed to repair a component
- **Mean Time Between Failures** ($MTBF$) = $MTTF + MTTR$
- General Availability: $A ={ MTTF \over MTBF} = {MTTF \over{MTTF + MTTR}}$. For example, a sys with an MTTF of 1000 hours and an MTTR of 10 hours has an availability of `1000 / 1010 ≈ 0.99` (99%).

**(NOTE)** Reliability and availability make sense only if we have an accurate notion of what a *failure* actually is. Availability and reliability measure different things. A sys that is down for 1ms every hour has 99.9999% availability but is unreliable (fails frequently). A sys shut down for 2 weeks every year is reliable during operation but only 96% available.
### Terminology
A sys is said to *fail* when it cannot meet its promises. In particular, if a DS is designed to provide its users with a number of services, the sys has failed when one or more of those services cannot be (completely) provided.

| Term                                    | Description                                                              | Example                                              |
| --------------------------------------- | ------------------------------------------------------------------------ | ---------------------------------------------------- |
| Failure                                 | A component is not living up to its<br>specifications.                   | Program crashes.                                     |
| Error                                   | Part of a component (system's state) that can lead to a failure.         | Programming Bug.                                     |
| Fault                                   | The cause of an error.                                                   | The developer.                                       |
| Fault Prevention                        | Prevent the occurrence of a fault.                                       | Thorough software verification and validation.       |
| Fault Tolerance (FT)<br>(**Important**) | Build a component such that it can mask the occurrence of a fault.       | Build each component by two independent programmers. |
| Fault Removal                           | Reduce the presence, number, or seriousness of a fault.                  | Thorough software verification<br>and validation.    |
| Fault Forecasting                       | Estimate current presence, future incidence, and consequences of faults. | Check code quality /<br>programming experience.      |
The developer is the *fault* for the *error*: bug which caused the program to crash in *failure*.
Aim: improve *fault tolerance* -> sys can provide its services *even* in the case of faults.
### Failure models

| Type of Failure                           | Description of Server’s Behaviour                                                                                              |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| Crash                                     | Halts, but is working correctly until it halts                                                                                 |
| Omission:<br> Receive<br> Send            | Fails to respond to incoming requests<br>Fails to receive incoming messages<br>Fails to send messages (response)               |
| Timing                                    | Response lies outside a specified time interval (timeout)                                                                      |
| Performance                               | Server responds too late.                                                                                                      |
| Response<br> Value<br> State-transition   | Response is incorrect e.g. 2+2=5<br>The value of the response is wrong<br>Deviates from correct flow of control (reacts wrong) |
| Arbitrary (or Byzantine)<br>**(Serious)** | May produce arbitrary responses at arbitrary<br>times that is not detected as incorrect. False info.                           |
#### Dependability vs Security (Omission versus Commission)
Arbitrary failures are sometimes qualified as malicious. 
- *Omission* failures: a component *fails to take an action* that it *should have* taken
- *Commission* failure: a component *takes an action* that it *should not have* taken. Malicious (Byzantine) failures are often commission failures.
These deliberate failures are often *security* problems. Distinguishing between *deliberate* failures and *unintentional* ones is difficult.
#### Failure Masking by Redundancy
If a sys is to be fault tolerant, the best it can do is to try to hide the occurrence of failures from other processes. The key technique for masking faults is to use redundancy:
- **Information** redundancy: Add extra bits to data units so that errors can be recovered when bits are garbled e.g., Hamming code
- **Time** redundancy: Design a sys where actions can be performed again if anything went wrong e.g., retransmission request to a server when lacking an expected response
- **Physical** redundancy: add equipment or processes in order to allow one or more components to fail. Often used in DS e.g., extra processes are added to a sys so that the sys can still function correctly if processes crash.
### Process resilience by Groups
FT can actually be achieved in DS by *protecting* against *process failures*, which is achieved by **replicating processes** into groups. 

Organise several identical processes into a group. For all groups: when a message is sent to the group itself, all members of the group receive it. In this way, if one process in a group fails, hopefully some other process can take over for it.

Process groups may be dynamic. New groups can be created and old groups can be destroyed. A process can join a group or leave one during sys operation. A process can be a member of several groups at the same time. Consequently, mechanisms are needed for managing groups and group membership.
#### Groups org
![[process-groups.png\|400]]
**(FLAT GROUP)** All processes are equal (symmetric). More robust (no single point of failure) but decision-making (e.g., voting) is slower and more complex.
**(HIERARCHICAL GROUP)** One process acts as the coordinator (leader). Efficient for decision-making but the coordinator is a single point of failure; if it crashes, a new leader must be elected.
#### Groups and Failure Masking
A sys is **k-fault tolerant** if it can survive `k` component failures and still meet its specification. The required group size depends on the failure type:

- For **Halting or Crash/Omission Failures (fail-stop):** You need **k + 1** processes. If `k` fail, the one remaining correct process provides the answer.
- For **Arbitrary (Byzantine) Failures:** You need at least **2k + 1** processes. In the worst case, `k` malicious processes could send the same bad answer. You need a majority (`k+1`) of correct processes to out-vote them and reach a consensus.
### Consensus with crash failures
For a replicated service to work correctly, all non-faulty replicas must **agree (reach consensus)** on the order and content of client commands. With only crash failures, this is simpler.

In a **fault-tolerant process group**, each nonfaulty process executes the same commands, in the same order, as every other nonfaulty process
#### Crash Failures: Flooding-based Consensus
A simple algorithm where processes exchange lists of proposed commands in rounds. Each process merges received lists and uses a **deterministic function** (same for all) to select the next command to execute.  
![[crash-ex-1.png\|400]]This approach works as long as processes do not fail. Problems start when a process $P_i$ detects, during round $r$, that, say process $P_k$ has crashed. 
- **Challenge:** If a process crashes mid-round, some members may have received its proposal and others not. Processes must wait an extra round to ensure they all have the same information before deciding, ensuring consistency.

**(EXAMPLE)** assume we have a process group of four processes {P1, . . . , P4} and that $P_1$ crashes during round $r$. Also, assume that $P_2$ receives the list of proposed commands from $P_1$ before it crashes, but that $P_3$ and $P_4$ do not (in other words, $P_1$ crashes before it got
a chance to send its list to $P_3$ and $P_4$).
![[crash-ex-1.png\|400]]
- P3 may have detected that P1 crashed, but does not know if P2 received anything, i.e., P3 cannot know if it has the same info as P2 -> cannot make decision (same holds for P4)
- P3 and P4 postpone decision to next round
- P2 makes a decision and broadcast that decision to the others
- P3 and P4 are able to make a decision at round (r+1)
- P3 and P4 decide to execute the same command selected by P2
#### Raft Consensus Algorithm
**A popular, understandable algorithm for managing a replicated log.
- Uses a fairly straightforward **leader-election** algorithm. The current leader operates during the **current term**.
- Every server (typically, five) keeps a **log** of operations, some of which have been committed. *A backup will not vote for a new leader if its own log is more up to date*.
- All committed operations have the same position in the log of each respective server.
- The leader decides which pending op to commit next ⇒ a **primary-backup approach**.
- 
**When submitting an operation**
- A client submits a request for operation $o$
- The leader appends the request $⟨o, t.k⟩$ to its own log (registering the current term $t$ and length of ), $k$ is the index of o in the leader’s log
- The log is (conceptually) broadcast to the other servers
- The others (conceptually) copy the log and acknowledge the receipt
- When a majority of ACKs arrives, the leader commits $o$.

**(EXAMPLE)**
1. A **leader** is elected for a **term**. All client commands go to the leader.
2. The leader appends the command to its log and replicates it to **followers**.
3. Once a **majority** of followers acknowledge, leader **commits** the entry and notifies them.
4. This ensures all servers have the same log of committed commands.  
![[leader-crashes-raft.png\|400]]
- **Leader Failure:** If the leader crashes, a new election is held. The new leader must have the most up-to-date log to ensure consistency. Any missing commits will eventually be sent to the other backups.

(**Note**) In practice, only updates are *broadcast*
- At the end, every server has the *same view* and knows about the $c$ committed operations
- Effectively, any information at the backups is *overwritten*
#### The Two Generals Problem
This thought experiment illustrates the fundamental difficulty of achieving **perfect reliability** over an unreliable communication channel.  
![[two-generals-problem.png\|200]]

- **Scenario:** Two armies must coordinate an attack by sending messengers through enemy lines. Messengers can be captured.
- **Dilemma:** No matter how many acknowledgements are sent, the sender can never be _100% certain_ the final acknowledgement was received, creating an infinite regress of uncertainty.
- **Implication:** In practice, DSs use timeouts and probabilistic guarantees (e.g., TCP retransmission) rather than seeking perfect certainty.
**How Should the Generals Decide?**
a) General 1 always attacks, even if no response is received?
- Send lots of messengers to increase probability that one will get through
- If all are captured, general 2 does not know about the attack, so general 1 loses
b) General 1 only attacks if positive response from general 2 is received?
- Now general 1 is safe
- But general 2 knows that general 1 will only attack if general 2's response gets through
- Now general 2 is in the same situation as general 1 in option 1
No common knowledge: the only way of knowing something is to communicate it.
### The Byzantine Generals Problem
This is the consensus problem in the presence of **arbitrary (Byzantine) failures**, where components may behave maliciously and send conflicting information.  
![[byzantine=generals-problem.png\|200]]

- **Goal:** All loyal (non-faulty) generals must agree on a common plan (e.g., Attack or Retreat) despite the presence of `f` traitors who may lie and send contradictory messages.
- **Famous Result (Lamport et al.):** To tolerate `f` Byzantine failures, the sys must have **at least `3f + 1`** total components. This means **less than one-third** can be faulty. Cryptography (digital signatures) helps but doesn't change this fundamental bound.
### Consensus with arbitrary failures
For replicas only subject to *crash failures*, a process group needs to consist of $2k + 1$ servers to survive $k$ crashed members. Here, we assume that a process does not collude with another process and that it is consistent in its messages to others. 

Now, we look at reaching consensus in a fault-tolerant process group in which k members can fail assuming arbitrary failures. We need at least $3k + 1$ members to reach consensus under these failure assumptions.

We consider process groups in which communication between processes is **inconsistent**.
![[arb-consensus-1.png\|200]]
**System model**
• Process group consisting of n members of which one is designated to be the *primary* $P$ and $n - 1$ *backups* $B1, …Bn-1$.
• A client sends a value $v \in {T\text{rue}, F\text{alse}}$ to $P$
• Messages may be lost, but this can be detected
• Messages cannot be corrupted beyond detection
• A receiver of a message can reliably detect its sender.
#### Byzantine Agreement: Requirements
BA1: Every nonfaulty backup process stores the same value
BA2: If the primary is nonfaulty then every nonfaulty backup process stores exactly what the primary had sent.
1. Primary is faulty -> BA1 says that backups may store the same, but different (and thus wrong) value than originally sent by the client.
2. Primary is not faulty -> satisfying BA2 implies that BA1 is satisfied 
#### Why having 3k processes is not enough
**(EXAMPLE)** consider the situation of tolerating the failure of a single process, that is, $k = 1$.
![[arb-consensus-2.png.png\|400]]
*Solution*: $3k+1$
- **Example (Why 4 is needed for f=1):** With 3 generals (1 faulty), the faulty one can tell different lies to the two loyal ones, leaving each loyal general with a different view and no way to determine the truth, as they cannot form a majority (2 out of 3) for a single value.  
![[arb-consensus-3.png.png.png\|400]]
- **Solution (With 4 for f=1):** With 4 generals (1 faulty), the 3 loyal generals can exchange messages. Even if the traitor tells different stories, the 3 loyal ones can compare notes and agree on the majority value, achieving consensus.
#### System Models
In reality, failures are complex, both nodes and networks may be faulty!. sys designers make explicit **assumptions (models)** about:
- **Network Behaviour:** Can messages be lost, duplicated, reordered? (e.g., Two Generals).
- **Node Behaviour:** Do nodes only crash, or can they be Byzantine?
- **Timing Behaviour:** Are there bounds on message delay (latency) and processing speed (synchronous) or not (asynchronous)?  
The chosen models for these three aspects define what is possible (e.g., consensus is solvable in a synchronous sys with crash failures, but much harder in an asynchronous sys with Byzantine failures).

**Check of Understanding**
> **Question:** You are designing a new blockchain that uses a Proof-of-Stake consensus mechanism. The network must tolerate up to `f` validators acting maliciously (Byzantine failures) and also handle validators going offline (crash failures). Using the principles from Byzantine fault tolerance, what is the _minimum_ fraction of the total stake that must be controlled by honest (non-Byzantine) validators for the network to guarantee safety (i.e., prevent conflicting blocks from being finalised)? Explain the reasoning using the concepts of `k-fault tolerance` and the Byzantine Generals bound.
> **Answer:** The honest validators must control **more than two-thirds ( > 2/3 )** of the total stake.

> **Reasoning:** This is a direct app of the Byzantine Generals solution. To tolerate `f` Byzantine failures, the sys needs **`3f + 1`** total participants (or stake-weight equivalents). Therefore, the number of honest participants needed is **`3f + 1 - f = 2f + 1`**.
> The _fraction_ of honest stake is therefore `(2f + 1) / (3f + 1)`. As `f` increases, this fraction approaches (but always remains greater than) `2/3`. For any `f`, `(2f+1)/(3f+1) > 2/3`. This ensures that the honest nodes (controlling >2/3 of the stake) always have an overwhelming majority (`2f+1`) over the colluding malicious nodes (`f`), allowing them to out-vote any conflicting proposals and maintain a single, agreed-upon chain. If honest stake fell to 2/3 or below, malicious validators could potentially form a large enough coalition (`f` could be >= `n/3`) to violate the `3f+1` bound and break consensus.
## 15. Cloud Computing
### Technology Landscape
Cloud computing is envisioned as delivering computing resources as a utility, similar to the power grid. This evolution is driven by exponential growth in processing power (exascale supercomputers), network bandwidth (terabit speeds), and storage density.

**(EVOLUTION OF DISTRIBUTED COMPUTING)** This marks the fourth major phase:
1. **Linking Machines:** The Internet (TCP/IP).
2. **Linking Documents:** The World Wide Web (HTTP, HTML).
3. **Linking Applications:** Web Services & SOA (REST).
4. **Linking Everything as a Utility:** **Cloud Computing**, enabled by **virtualisation**.
### Towards a Definition of Cloud Computing
A cloud is a massive pool of **virtualised** resources (compute, storage, network) that are:
* **On-Demand:** Provisioned automatically by users.
* **Elastic:** Can be scaled up or down dynamically to match workload.
* **Pay-Per-Use:** Billed based on consumption.
* **Broadly Accessible:** Accessed over the network via standard mechanisms.

**(CLOUD DATA CENTERS)** These are the physical foundation: warehouses containing tens to hundreds of thousands of tightly coupled servers, managed to run multiple massive-scale applications (e.g., Amazon, Google, Microsoft). Key challenges include load balancing, data management, and fault tolerance at an immense scale.
### Virtualised infrastructures
Virtualisation is the core technology enabling cloud computing. It abstracts physical hardware (CPUs, memory, storage) to create multiple, isolated virtual machines (VMs) or containers on a single physical server.
**(BENEFITS)**
* **Server Consolidation:** Run multiple workloads on fewer physical machines, reducing cost and complexity.
* **Hardware Independence:** Applications are decoupled from specific hardware, avoiding vendor lock-in.
* **Simplified Management:** Enables automation of provisioning, scaling, and recovery.
### Conceptual Cloud Architecture
Cloud architecture is commonly viewed through two complementary models: a layered service model and a conceptual stack.
![[concept-cloud-vision.png\|300]]*The cloud provides a simplified, service-oriented interface to complex, pooled resources.*
![[layered-cloud-arch.png\|300]]*The stack shows how virtualisation and management software abstract the physical infrastructure.*
### Taxonomy of cloud Models
Cloud services are offered in three main layers, forming a stack where each higher layer builds upon the capabilities of the layer below.
#### **Infrastructure as a Service (IaaS)**
*   **What it is:** Provides fundamental computing resources as virtualised services. Consumers get raw VMs, storage, and networks.
*   **Consumer Control:** Manages OS, storage, deployed applications, and some networking components.
*   **Provider Responsibility:** Physical hardware, hypervisor, and core network.
*   **Example:** **Amazon EC2**, where you rent virtual servers by the hour.
#### **Platform as a Service (PaaS)**
*   **What it is:** Provides a complete software development and deployment environment in the cloud.
*   **Consumer Control:** Deploys and manages *applications* using provider-supported programming languages, libraries, and tools.
*   **Provider Responsibility:** Underlying OS, runtime, middleware, and infrastructure.
*   **Example:** **Microsoft Azure App Services**, **Google App Engine**.
#### **Software as a Service (SaaS)**
*   **What it is:** Delivers complete, ready-to-use application software over the internet.
*   **Consumer Control:** Only application configuration and user data.
*   **Provider Responsibility:** Everything: application, data, runtime, infrastructure.
*   **Example:** **Google Workspace (Gmail, Docs)**, **Salesforce**.

**(TYPICAL CLOUD ARCHITECTURE)** The physical realisation involves massive clusters of commodity servers, interconnected via high-speed switches and housed in warehouse-scale data centers.
![[typical-cloud-arch.png\|300]]
### Virtual Infrastructure Managers (VIMs)
Managing thousands of VMs across a data center requires sophisticated orchestration software—a **Virtual Infrastructure Manager (VIM)**.

**(ROLE OF A VIM)** A VIM provides a uniform view of the resource pool and automates the **lifecycle management** of VMs: scheduling (where to place a VM), provisioning (cloning from templates), networking (assigning IPs), and monitoring.

**(OPENSTACK EXAMPLE)** A prominent open-source VIM that controls large pools of compute, storage, and networking resources.
![[openstack-ex.png\|200]]
*   **Function:** Provides APIs and dashboards for administrators and users to provision and manage cloud resources. It integrates components for compute (Nova), networking (Neutron), storage (Cinder, Swift), and identity (Keystone).
### Cloud services
Beyond traditional Virtual Machines, **container-based virtualisation** has become central to cloud-native application development.
**(VIRTUAL MACHINES VS. CONTAINERS)**

| Virtual Machine          | Container                       |
| ------------------------ | ------------------------------- |
| ![[vm-arch-ex.png\|200]] | ![[container-arch-ex.png\|200]] |
- **Virtual Machine:** Virtualises the entire hardware stack, requiring a full guest OS. Higher isolation but more overhead.
* **Container:** Virtualises the OS, sharing the host kernel. Multiple isolated user-space instances run on a single OS. More lightweight and efficient than VMs.

**(THE CASE OF CONTAINERS & MICROSERVICES)** Containers are the ideal packaging for **microservices**—small, independent services that make up an application. An application is a collection of containers, each running a single microservice.
*   **Orchestration Need:** Managing hundreds of interconnected containers requires an orchestrator like **Kubernetes**.
![[container-kubernetes-ex.png\|400]]
*   **Kubernetes Role:** Automates deployment, scaling (load balancing), and management of containerized applications. It handles scheduling containers onto nodes, health monitoring, and recovery from failures.
### Private, Public, and Hybrid Clouds
Deployment models define where the cloud infrastructure is located and who manages it.
![[public-private-hybrid-clouds.png\|400]]
*   **Public Cloud:** Owned and operated by a third-party cloud provider (e.g., AWS, Azure, GCP). Resources are shared among multiple organisations (multi-tenant). Offers the greatest elasticity and shifts capital expenditure to operational expenditure (pay-as-you-go).
*   **Private Cloud:** Cloud infrastructure operated solely for a single organisation. It may be managed by the organisation or a third party and can be located on-premises or off-premises. Offers greater control and security but requires higher capital investment and management overhead.
*   **Hybrid Cloud:** A composition of two or more distinct cloud infrastructures (private, public) that remain unique entities but are bound together by standardised technology. This allows data and applications to be shared, enabling **cloud bursting** (using public cloud to handle spikes in demand) and flexible workload placement.

**Check of Understanding**
> **Question:** A start-up is building a new mobile app. They need to store user profiles and posts, run backend application logic, and want to use a machine learning API for content recommendations. They have a small team and want to minimise operational overhead. Using the cloud service models (IaaS/PaaS/SaaS), recommend a specific approach for each of their three needs and justify why it fits the start-up's constraints.
>
> **Answer:**
> 1.  **User Data Storage:** Use a **SaaS** database service like **Amazon DynamoDB** or **Google Firestore**. The start-up avoids managing database servers, scaling, backups, and patching. They just configure the data model and use an API, perfectly matching their need to minimise overhead.
> 2.  **Backend Application Logic:** Use a **PaaS** offering like **AWS Elastic Beanstalk** or **Google App Engine**. They can simply upload their application code (e.g., in Python/Node.js). The PaaS provider automatically handles the deployment, scaling, load balancing, and runtime management. This is ideal for a small team wanting to focus on code, not infrastructure.
> 3.  **ML Recommendations:** Use a **SaaS** AI service like **Google Cloud Vision API** or **Amazon Rekognition**. They don't need to build, train, or host ML models. They call a pre-built API, which is the fastest, lowest-overhead way to add advanced functionality.
# Remaining
## 16. Distributed Systems Topics and Trends Pt.1
Clouds are the foundational platform for modern and future internet services. As the internet expands beyond people and computers to include everyday objects, a new paradigm emerges—the **Internet of Things (IoT)**. IoT applications generate vast data streams, which must be processed and stored dynamically, creating a symbiotic relationship with cloud computing.
### IoT: the Internet of Things
The **Internet of Things (IoT)** is a sys where physical objects ("things") embedded with sensors, software, and network connectivity collect and exchange data over the internet. Kevin Ashton coined the term in 1999. Predictions indicate tens of billions of connected devices, generating data on the scale of **zettabytes**.

**(INTEGRATING A DEVICE INTO IoT - THE FRIDGE EXAMPLE)** Transforming an appliance like a fridge into an IoT device requires two core components:
1.  **Computational Intelligence:** Local processing capability to interpret sensor data (e.g., cameras for inventory, weight sensors, timers for door alarms).
2.  **Network Connectivity:** The ability to send and receive data over a network, enabling remote features.

*   **Local Features (Computation):** Suggesting recipes based on inventory, sounding an alarm if the door is left open.
*   **Network-Enabled Features (Connectivity):** Sending an SMS when stock is low, autonomously ordering food from an online supermarket by interacting with web services, and managing payment.

**(EXAMPLE ARCHITECTURE)** A typical IoT architecture involves sensors/devices collecting data, communicating via a gateway (which may perform initial processing), and ultimately sending data to the cloud for storage and advanced analytics.
![[iot-arch.png\|400]]
### Edge Computing
The traditional "cloud-only" model faces significant challenges with IoT:
*   **Data Volume & Velocity:** Sending all raw sensor data to the cloud creates massive **data bottlenecks** and network **congestion**.
*   **Latency:** Many applications (e.g., autonomous vehicles, industrial automation) require **real-time responses** (sub-second to minute-level), which round-trip cloud latency cannot support.
*   **Bandwidth Cost:** Transmitting constant streams of raw data is expensive.

**(THE CLOUD TO EDGE SHIFT)** To solve this, **Edge Computing** moves computation and data storage **closer to the location where it is needed**, i.e., near the IoT devices at the "edge" of the network. This creates a multi-layered architecture:
*   **Cloud Computing:** Centralised, data-center based, for heavy batch processing, long-term storage, and complex analytics.
*   **Edge Computing:** Geographically distributed servers (e.g., base stations, micro-data centers) that handle time-sensitive processing, data filtering, and local decision-making.
![[network-edge-computing.png\|400]]
**(EXAMPLE - DRIVERLESS CARS)** An autonomous vehicle exemplifies this hybrid model. It requires:
1.  **Local (Edge) Processing:** Immediate sensor fusion (camera, LiDAR) for real-time obstacle avoidance and navigation.
2.  **Cloud Processing:** Downloading updated high-definition maps, running complex simulations for route optimization, and performing deep learning model training.
### Revisiting the Cloud Computing Stack
The classic three-layer cloud stack (IaaS, PaaS, SaaS) is being extended "downwards" to incorporate the edge and IoT layers, forming a **continuum of computing**.
![[cloud-arch-revisited.png\|400]]
This stack now includes:
*   **Sensors/Actuators Layer (Things):** The physical IoT devices.
*   **Edge Computing Layer:** Provides local compute, storage, and networking. It acts as a first line of data processing, performing monitoring, analysis, reduction (filtering), and caching before sending relevant data upstream.
### Vision for a (near) future
The future points toward an **edge-driven**, **intelligent** distributed systems paradigm.

**(EDGE-DRIVEN EXASCALE)** Next-generation **exascale supercomputing** (capable of a quintillion calculations per second) will be tightly integrated with edge computing. The edge acts as a **filter and pre-processor**, transforming I/O-bound data problems into compute-bound problems for the supercomputer by sending only critical, refined data.
*   **Efficiency:** It's more efficient to analyse data at the edge and transmit only anomalies or valuable summaries for deep analysis.

**(INTELLIGENT DISTRIBUTED SYSTEMS)** The progression is clear: the demand from new **Disruptive Applications** (IoT, AI, real-time analytics) drives the evolution of the **Supporting Infrastructure**. This infrastructure is moving towards massive **Exascale/Zetascale** centralised compute, deeply integrated with a pervasive, intelligent edge, ultimately forming **Intelligent Distributed Systems** that are adaptive, responsive, and efficient.

**Check of Understanding**
> **Question:** A city deploys a smart traffic management sys with hundreds of cameras and sensors at intersections to optimise traffic flow and detect accidents. Using the concepts of IoT, Edge, and Cloud computing, describe a feasible three-tier architecture for this system. Specify what type of processing or action should logically happen at each tier (Sensor/Edge/Cloud) and justify your design based on the core challenges each tier solves.
>
> **Answer:** A feasible three-tier architecture would be:
> 1.  **Sensor/Traffic Intersection Tier (IoT Layer):**
>     *   **Action:** Cameras and sensors **collect raw data** (vehicle count, speed, images).
>     *   **Justification:** This is the source of the IoT data. The devices have minimal processing power and are geographically distributed at each intersection.
> 2.  **District Edge Server Tier (Edge Computing Layer):**
>     *   **Action:** A server located in each city district performs **real-time, low-latency processing**. It analyses video feeds to **detect accidents or congestion immediately**, changes local traffic light patterns in response, and **filters/aggregates** data (e.g., converting 24/7 video into "congestion level: high" summaries).
>     *   **Justification:** This solves the **latency** problem (lights must change in seconds), reduces **data volume** sent to the cloud (sends summaries, not raw video), and handles **bandwidth congestion**. It enables immediate, autonomous action.
> 3.  **City Cloud Data Center Tier (Cloud Layer):**
>     *   **Action:** Receives aggregated data from all edge servers. It runs **long-term analytics and machine learning** to identify city-wide traffic patterns, optimise signal timing plans for daily routines, and predict future congestion hotspots. It also provides the **central management dashboard** for city planners.
>     *   **Justification:** The cloud provides the **massive storage** and **heavy computational power** needed for batch analytics and model training that is not time-critical. It offers a **global, consolidated view** impossible to achieve at the edge alone.
## 17. Distributed Systems Topics and Trends Pt.2
### Module themes
This module covered the foundational principles of Distributed Systems (DS), which can be organised into four key parts:

**Part 1 - Foundations**
*   **Concepts:** Definition of DS, the role of **Middleware**.
*   **Structure:** Architectural Styles (layered, object-based, microservices), sys Architectures (centralised, decentralised, hybrid).
*   **Communication & Principles:** Models (RPC, Message-Oriented), **Transparency** goals, and **Openness** through standard interfaces.

**Part 2 – Service Orientation**
*   **Evolution:** From monolithic systems to **Service-Oriented Architectures (SOA)**.
*   **Implementation:** **Web Services** and the **REST** architectural style.
*   **Modern Paradigms:** Decomposition into **Microservices**, and the event-driven, resource-efficient model of **Serverless Computing** and **Function as a Service (FaaS)**.

**Part 3 – Distributed Systems Support**
*   **Naming & Discovery:** **Flat** and **structured naming**, **Directory Services** (e.g., LDAP).
*   **Coordination:** **Clock synchronisation** algorithms (Cristian's, Berkeley, NTP), and **election algorithms** (Bully, Ring).
*   **Consistency & Resilience:** **Consistency models** (sequential, causal, client-centric), **replication** strategies, and **Fault Tolerance** techniques (process groups, consensus with Byzantine failures).

**Part 4 – Use Cases**
*   **Modern Platforms:** **Cloud Computing** service models (IaaS, PaaS, SaaS), **Edge Computing**.
*   **Applications:** The **Internet of Things (IoT)** and emerging **Disruptive Applications** driving sys evolution.

### Evolution of distributed computing
Distributed systems are scaling to unprecedented levels in both **computational power** and **user capacity**.

**(SCALABILITY IN COMPUTE - EXASCALE)** Systems like the **El Capitan** supercomputer (1.742 exaflops) represent the pinnacle of tightly-coupled, high-performance distributed computing, built from millions of coordinated CPU/GPU cores.

**(SCALABILITY IN USERS & DATA - INTERNET SCALE)** Modern platforms serve billions of users and process exabytes of data daily:
*   **YouTube:** 1 billion hours of video watched/day.
*   **Google:** 8.5 billion searches/day.
*   **Data Generation:** ~400 exabytes of new data generated globally *each day*.

These scales necessitate the foundational principles covered in the module: replication for performance, consistency trade-offs, and fault-tolerant designs.

### Most Active Topics in Distributed Systems
Current research and development are shaped by major trends in cloud/edge computing, AI, security, and sustainability.
**(1-Edge-Cloud Continuum)**
The rigid separation between cloud and edge is blurring into a **continuum of compute resources**. This is critical for IoT and latency-sensitive applications (autonomous vehicles, AR/VR).
![[edge-cloud-continuum.png\|400]]
*   **Key Requirement:** **Distributed orchestration** to seamlessly deploy and manage applications across this heterogeneous landscape, from central cloud to far edge devices.

**(2-Intelligent Distributed Systems)** A two-way relationship is defining the future:
*   **AI for Systems:** Using **Large Language Models (LLMs)** and **agentic AI** to automate and optimise the DS itself—e.g., for intelligent scheduling, anomaly detection, automatic scaling, and self-repair.
*   **Systems for AI:** Designing new distributed architectures (specialised hardware like TPUs, high-bandwidth networks) to support the massive computational demands of **distributed AI model training and inference**.
![[intelligent-ds.png\|400]]
**(3-Serverless and Event-Driven Architectures)**
**Serverless/FaaS** is evolving beyond its initial stateless constraints.
*   **Active Issues:** Managing **stateful serverless functions**, eliminating **cold-start latency**, and achieving portability across different cloud providers.
*   **Emerging Solution - WebAssembly (WASM):** A portable, lightweight binary format that executes safely at near-native speed. Its **fast startup** and **small footprint** make it ideal for secure, efficient workload deployment across the edge-cloud continuum.

**(4-Reliability, Consistency, and Correctness at Scale)**
Ensuring correctness in massively scalable, highly available systems remains a core challenge.
*   **Beyond Traditional Consensus:** New data types like **Conflict-free Replicated Data Types (CRDTs)** allow replicas to be updated independently and concurrently without coordination, guaranteeing automatic convergence—ideal for collaborative applications.
*   **Verification:** Developing techniques to formally verify the correctness of complex, interacting microservices and multi-agent systems.

**(5-Secure Large Scale Distributed Systems)**
Decentralisation and complexity introduce profound security challenges.
*   **Distributed Identity:** Managing access for autonomous **agentic AI** workloads across domains.
*   **Confidential Computing:** Using hardware-based **Trusted Execution Environments (TEEs)** to process encrypted data in memory, protecting it even from the cloud provider, which is crucial for sensitive workloads in shared environments.

**(6-Sustainability?)** Energy footprint of distributed computing is massive and cannot be ignored.
*   **The Scale of the Problem:** Streaming a single popular music video since 2016 may have consumed over **51 TWh**—more than the annual electricity consumption of a country like Morocco.
*   **Energy-Efficient Solutions:** Research focuses on optimisation at every layer:
    *   **Hardware:** Dynamic Voltage and Frequency Scaling (DVFS).
    *   **Data Center & Virtualisation:** Consolidating VMs, turning off idle servers.
    *   **Software & Protocols:** Energy-aware network protocols and application design.

**Check of Understanding**
> **Question:** A company is building a new global collaborative document editor (like Google Docs). They require high availability, real-time co-editing with minimal lag, and must work even with intermittent user connectivity. Based on the trends discussed, recommend **two specific technical approaches** from different active topic areas (e.g., 1-Edge-Cloud, 4-Consistency, 6-Sustainability) that would be particularly suitable for this application. For each, briefly explain *how* it addresses a core requirement and *why* it's a good fit compared to a more traditional alternative.
>
> **Answer:**
> 1.  **Technical Approach from Topic 4 (Consistency): Use Conflict-free Replicated Data Types (CRDTs).**
>    *   **How it helps:** CRDTs allow each user's client to edit its local replica of the document independently, even offline. When connectivity is restored, the CRDT's mathematical properties guarantee that all changes will merge **automatically and consistently** without complex conflict resolution protocols.
>    *   **Why it's a good fit vs. Traditional:** A traditional approach using a primary server with locking (e.g., remote-write protocol) would create high latency (waiting for server acknowledgement) and fail completely during network partitions. CRDTs provide the **high availability** and **seamless offline operation** that a collaborative editor needs.
> 2.  **Technical Approach from Topic 1 (Edge-Cloud): Deploy using an Edge-Cloud continuum with WebAssembly (WASM).**
>    *   **How it helps:** The core editing logic (including the CRDT merge operations) could be packaged as a lightweight **WASM module**. This module can run consistently and securely on a user's local device (far edge for zero-latency), on a nearby edge server (for session persistence), or in the cloud (for heavy indexing). This enables **low-latency** editing and efficient use of resources.
>    *   **Why it's a good fit vs. Traditional:** A traditional thick client app is heavy and platform-dependent, while a pure cloud JS app suffers from latency and offline limitations. WASM on the edge-cloud continuum offers a **portable, fast-starting, and efficient** execution environment that can be dynamically placed to optimise for both performance and resource usage, contributing to **sustainability (Topic 6)** by reducing unnecessary data center load.