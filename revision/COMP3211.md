# Structure
### Course Outline
#### Part 1
- Architectures
- Middleware
- Distributed object technology, communication
- Service Oriented Architectures and Web services (REST)
- Microservices and nanoservices
- Serverless architectures
#### Part 2
- Supporting services: naming, directory and discovery services, synchronisation, consistency, replication, fault- tolerance
#### Part 3
- Cloud computing, Virtualisation, Edge computing
## Part 1
### 1. Introduction to Distributed Systems Pt.1 - *Chapter 1*
- Definition of a DS
- Goals and challenges
- Sharing of resources
- Transparency
- Openness
- Scalability
### 2. Introduction to Distributed Systems Pt.2 - *Chapter 1* 
- Types of distributed systems
1. High performance distributed computing systems
2. Distributed information systems
3. Distributed systems for pervasive computing
- *Homework*: What is Google spanner? 
	- What consistency properties does it have?
	- How does it handle ACID transactions?
### 3.  Architectures - *Chapter 2*
- Understand the different ways on how to view the organisation of a distributed system
- Architectural styles
	- Layered
	- Object-based
	- Resource-centred
	- Event-based
- System architecture
	- Centralised
	- Decentralised
	- Hybrid
### 4. Communication - *Chapter 4*
- Foundations
	- Latency and Bandwidth
	- Layered Protocols
	- Types of communication
		- Synchronous vs asynchronous
		- Transience vs persistence
- Remote Procedure Call
- Message Oriented Middleware
	- Message oriented communication
	- Message Passing Interface
	- Message Queuing Model
### 5. Service Oriented Architectures
- Conceptual Design of Software Systems
- Architectures
	- 1-Tier
	- 2-Tier
	- 3-Tier
	- N-Tier
- Emergence of SOAs
- Vision
- Characteristics
### 6. Web Services and REST
- Why do we need Web services?
- What are web services?
- What is REST?
- What does it consist of?
- Claimed benefits
- HTTP
### 7. Programming RESTful Web Services
- REST: Quick Recap
- REST APIs: Examples
- Reference implementations:
	- Python (Flask Restful)
	- Java (Jersey)
- Data encoding and RPC
### 8. Microservices, Nanoservices and Serverless
- Recap: SOAs
- Microservices
- Nanoservices
- Serverless Computing
- Function as a Service
- Architectural Support
- Solutions
## Part 2
### 9. Naming Pt.1 - *Chapter 5*
- Fundamental concepts
- Classes of naming systems
- Namespaces
- Naming graphs
### 10. Naming Pt.2 - *Chapter 5*
- Structured naming
	- Namespaces
	- Name resolution
	- Implementation of a namespace
- Attribute-based naming
- Directory Services
### 11. Timing and Synchronisation - *Chapter 6*
- Synchronisation in a DS
- Internal and external physical clocks
- Clock synchronisation algorithms
- Election Algorithm
- Network time protocol (NTP)
### 12. Consistency and Replication Pt.1 - *Chapter 7*
- Data-centric consistency models
### 13. Consistency and Replication Pt.1 - *Chapter 7*
- Client-centric consistency models
- Replica management
- Consistency protocols
### 14. Fault Tolerance - *Chapter 8*
- Dependability, reliability and availability in a DS
- Terminology
- Failure models
- Process resilience
- Consensus with crash failures
- Consensus with arbitrary failures
- The Byzantine Generals Problem
- *Homework*: The Paxos consensus algorithm is a protocol used in distributed systems to allow a group of computers to agree on a single value despite failures. How does Paxos compare to Raft?
## Part 3
### 15. Cloud Computing
- Technology Landscape
- Towards a Definition of Cloud Computing
- Virtualised infrastructures
- Conceptual Cloud Architecture
- Taxonomy of cloud Models
- Virtual Infrastructure Managers
- Cloud services
- Types of Clouds
### 16. Distributed Systems Topics and Trends Pt.1
- IoT: the Internet of Things
- Edge Computing
- Revisiting the Cloud Computing Stack
- Vision for a (near) future
### 17. Distributed Systems Topics and Trends Pt.2
- Module themes
- Evolution of distributed computing
- Most Active Topics in Distributed Systems
# Content 
## 1. Introduction to Distributed Systems Pt.1 - *Chapter 1*
### Definition of a Distributed System
> (1) A collection of *autonomous computing elements* (nodes) that appears to its *users* as a *single coherent system*.

NODES: Hardware devices and software processes (e.g. computer, car, robot) that need to collaborate. An AUTONOMOUS node has its own *notion of time* as every node has its own clock. 
There is no GLOBAL clock which is needed for synchronisation.
An autonomous node also needs to communicate to other nodes, providing network support.

> (2) A system in which components located at *networked* computers *communicate* and coordinate their actions only by *passing messages*.

The collection of nodes as a whole operates the same –no matter *where, when or how* the interaction takes places between the user and that system.
For example: 
- An end-user cannot tell where the computation is taking place
- Where data is stored exactly should be irrelevant to an application
- Whether or not data has been replicated is completely unknown/hidden. (Distribution transparency)
#### Examples of Distributed Systems
- The internet, The World Wide Web (WWW), A cellular mobile phone network
- The cloud
Applications (apps) built on top of Distributed Systems (DSs) are called Distributed apps (DA): Netflix, Spotify, Instagram.
“You know you have a DS when the crash of a computer you’ve never heard of stops you from getting any work done” – Leslie Lamport
#### Distributed versus Decentralised Systems
There are two views of DS:
1. INTEGRATIVE view: connecting existing networked computer systems into a larger a system.
2. EXPANSIVE view: an existing networked computer system is extended with additional computers
![[centralised-decentralised-distributed-system.png\|500]]
> A DISTRIBUTED system is a networked computer system in which processes and resources are **sufficiently** spread across multiple computers. (expansive view).

> A DECENTRALISED system is a networked computer system in which processes and resources are **necessarily** spread across multiple computers. (integrative view)

Here, data is normally brought to the high-performance computers that literally train models before they can be used. But when data needs to stay within the perimeter of an organisation (e.g. security reasons), training is brought to the data. The result is known as **federated learning**.
#### Examples of Decentralised and Distributed Systems
1. **Blockchain (distributed ledger)  decentralised system**
A distributed ledger, blockchain: we need to deal with the situation that participating parties do not trust each other enough to set up simple schemes for collaboration. 
	Instead, what they do is essentially make the transactions among each other fully public (and
 verifiable) by an extend-only ledger that keeps records of those transactions. The ledger itself is fully spread across the participants, and the participants are the ones who validate transactions (of others) before admitting them to the ledger. 
	The result is a decentralised system in which processes and resources are, indeed,
necessarily spread across multiple computers, in this case due to lack of trust.
2. **Geographically dispersed  decentralised system**
Consider systems that are naturally geographically dispersed. This occurs typically with systems in which an actual location needs to be monitored, for example, in the case of a power plant, a building, a specific natural env, and so on. 
	The system, controlling the monitors and where decisions are made, may easily be placed
somewhere else than the location being monitored. 
	One obvious example is monitoring and controlling of satellites, but also more mundane 
situations as monitoring and controlling traffic, trains, etc. Here, the necessity for spreading processes and resources comes from a spatial argument.
3. **Content Delivery Networks (CDNs) DS**
The content of an actual Website, is copied and spread across various servers of the CDN. 
	When visiting a Website, the user is transparently redirected to a nearby server that holds all
or part of the content of that Website. A server is selected for which good performance in terms of latency and bandwidth can be guaranteed. 
	The CDN dynamically ensures that the selected server will have the required content readily 
available, as well as update that content when needed, or remove it from the server when there are no or very few users to service there. 
	Meanwhile, the user knows nothing about what is going on behind the scenes (which, again, 
is a form of distribution transparency). We also see that content is not copied to all servers, yet only to where it makes sense, that is, *sufficiently*, and for reasons of performance. CDNs also copy content to multiple servers to provide high levels of dependability.
4. **Network-Attached Storage (NAS) DS**
Consider a domestic-use setup based on a NAS, a typical NAS consists of 2–4 slots for internal hard disks. 
	The NAS operates as a file server: it is accessible through a (generally wireless) network for
any authorised device, and can offer services like shared storage, automated backups, streaming media, etc. 
	The NAS itself can best be seen as a single computer optimised for storing files, and offering 
the ability to easily share those files. The latter is important, and together with multiple users, we essentially have a setup of a distributed system. 
	The users will be working with a set of files that are locally (i.e., from their laptop) easily 
accessible (in fact, seemingly integrated into the local file system), while also directly accessible by and for other users. 
	Again, where and how the shared files are stored is hidden (i.e., the distribution is 
transparent). If file sharing is the goal, then we see that a NAS can provide *sufficient* spreading of processes and resources.
### Goals and challenges
A DS aims for: 1. Sharing of resources, 2. Distribution Transparency, 3. Openness, 4. Scalability,
A DS has many challenges:
• Architecture: common organisations, common styles
• Process: what kind of processes, and their relationships
• Communication: facilities for exchanging data
• Coordination: application-independent algorithms
• Naming: how do you identify resources?
• Consistency and replication: performance requires of data, which need to be the same
• Fault tolerance: keep running in the presence of partial failures
• Security: ensure authorised access to resources
### Sharing of resources
**Examples**: File sharing on P2P, shared web hosting, shared cloud-based storage
There are many reasons for wanting to share resources; its economically cheaper to have a single high-end reliable storage facility than having to buy and maintain storage for each user separately.
	Connecting users and resources also makes it easier to collaborate and exchange 
information, as is illustrated by the Internet with its simple protocols for exchanging files, mail, documents, audio, and video. 
	The connectivity of the Internet has allowed geographically widely dispersed groups of 
people to work together by all kinds of groupware, that is, software for collaborative editing, teleconferencing, and so on, as is illustrated by multinational software-development companies that have outsourced much of their code production to Asia.
### Transparency
> The phenomenon by which a DS attempts to *hide* the fact that its processes and resources are *physically distributed* across *multiple computers*, possibly separated by large distances.
	
This is handled through many different techniques in the MIDDLEWARE layer that sits between apps and OSs. E.g. Limited transparency: network services like sockets are directly visible to app dev.

Aiming for distribution transparency may be a nice goal when designing and implementing a DS, but that it should be considered together with other issues such as performance and comprehensibility. The price for achieving full transparency may be surprisingly high.
#### Middleware
![[middleware-layer-ex.png\|500]]
Its the glue *between* apps and OSs, extending over multiple machines; contains commonly used components and functions that need not be implemented by apps separately.
The aim of the middleware is to hide heterogeneity of the underlying platforms from apps.
#### Types of Transparency
| Transparency  | Description                                                           |
| ------------- | --------------------------------------------------------------------- |
| _Access_      | Hide differences in data representation and how an object is accessed |
| _Location_    | Hide where an object is physically located in the system              |
| _Migration_   | Hide that an object may move to another location                      |
| _Relocation_  | Hide that an object may be moved to another location while in use     |
| _Replication_ | Hide that an object is replicated                                     |
| _Concurrency_ | Hide that an object may be shared by several competitive users        |
| _Failure_     | Hide the failure and recovery of an object                            |
### Openness
> An OPEN DS offers components that can easily be used by, or integrated into other systems. An open DS itself will often consist of components that originate from elsewhere.

![[middleware-openness-ex.png\|500]]
They *share* the same *interface* and *communicate* with the same *common protocol* to be able to *interact with services from other open systems*, irrespective of the underlying env.

Systems should conform to well-defined *interfaces*, easily *interoperate*, support *portability* of apps and be easily *extensible*.
### Scalability Issues
A DS can be scaled in size, geographically or administratively if it remains effective after scaling.
#### Scale in Size (number of users and/or processes)
Scalability for the internet was effortless because information is organised hierarchically rather than linearly: $O(\log(n))$. Poor scalability is when the cost of supporting $n$ users is worse than $O(n)$.
#### Scale Geographically (maximum distance between nodes)
1. DSs designed for LANs are hard to scale as they are based on *synchronous communication*.
Here, a party requesting service (*client*), *blocks* until a *reply* is sent back from the *server* implementing the service e.g., a database transaction. 
	Communication between two machines in a LAN: ~few hundred microseconds ($\mu s$)
However, the *interprocess communication* in a WAN may be hundreds of milliseconds ($m s$), three orders of magnitude ($10^3$) slower; this is a LATENCY problem.
2. Communication in WANs has *limited bandwidth* and is inherently much *less reliable* than LANs. 
In a home network, ensuring a stable, fast stream of high-quality video frames from a media server to a display is simple. If you place that same server far away with a standard TCP connection to the display, it will fail due to bandwidth limitations but also maintaining due to unreliable connection. Solutions developed for LANs are not easily *ported* to a WAN.
	Furthermore, WANs have very limited facilities for multipoint communication. Separate 
services, such as naming and directory services need to be developed for queries to be sent. These services must also be scalable, for which, no obvious solutions exist. TODO.
	In contrast, LANs often support *efficient broadcasting mechanisms* which are useful for 
discovering components and services, and are desired from a management point of view. 
#### Scale Administratively (no. administrative domains, e.g. Google data centres worldwide)
To scale a DS across multiple, independent administrative domains, solve the major problem of *conflicting policies* with respect to *resource usage* (and payment), *management*, and *security*. 
**(EXAMPLE 1)** scientists want to share usage of their expensive equipment in computational grid. 
- In these grids, a global DS is constructed as a federation of local DSs, allowing a program 
running on a computer at org A to directly access resources at org B. 
- Many components of a DS that reside within a single domain can be trusted by users that operate within that same domain. 
- In such cases, system administration may have tested and certified apps, taking special measures to ensure that such components cannot be tampered with.
- So, users trust their system administrators but this trust does not expand naturally across domain boundaries.
If a DS *expands to another domain*, two types of *security measures* need to be taken. 
**(1)** The DS has to *protect* itself against malicious *attacks* from the *new domain*. 
 - Users from the new domain may have only read access to the file system in its original domain. 
 - Facilities such as expensive image setters or high-performance computers may not be made available to unauthorised users. 
**(2)** The new domain has to protect itself against malicious attacks from the DS. 
 - E.g. downloading programs such as applets in Web browsers. Basically, the new domain does not know what to expect from such foreign code. These limitations are hard to enforce.

**(EXAMPLE 2)** consider developing a radio telescope where the final system is a federated DS.
- The telescope itself may be a wireless DS developed as a grid of a thousand sensor nodes, each collecting radio signals and collaborating with neighbouring nodes to filter out relevant events. 
	- The nodes dynamically maintain a *sink tree* by which selected events are *routed to a central point* for further analysis. 
	- The central point needs to be a reasonably powerful system, capable of storing and processing the events sent to it by the sensor nodes. 
- This system is *necessarily* placed in *proximity* of the sensor nodes, but is otherwise to be considered to operate independently. 
	- it may operate as a small local DS, storing all recorded events and offering access to remote systems owned by partners in the consortium. 
- Most partners have local DS (often a cluster of computers) that they use to further process the data collected by the telescope. 
	- In this case, the local systems directly access the central point at the telescope using a standard communication protocol. 
	- Naturally, many results produced within the consortium are made available to each partner.
	- So, the complete system will cross boundaries of several administrative domains, and that special measures are needed to ensure that data that only accessible to a specific consortium of partners. 

**(EXAMPLE 3)** DSs spanning multiple administrative domains that do not suffer from administrative scalability problems like file-sharing P2P networks. 
 - In these cases, end users simply install a program implementing distributed search and download functions and within minutes can start downloading files. 
	 - Other examples include P2P apps for telephony over the Internet such as Skype, and peer-assisted audio-streaming apps such as Spotify.
 - In these DSs, the end users -not administrative entities, collaborate to keep the system running. 
	 - At best, underlying administrative orgs such as Internet Service Providers (ISPs) can police the network traffic that these P2P systems cause, but such efforts are not very effective.
### Scaling Techniques
Scalability problems in DSs usually appear as performance problems caused by limited capacity of servers and/or networks. 
> SCALING UP is a solution of capacity improvement through memory increases, CPU upgrades, or network module replacement

> SCALING OUT is a solution of increasing machine deployment via hiding communication latencies, work distribution and replication.
#### (SOLUTION 1) Hiding Communication Latencies
This approach is about making better use of response times -applicable for geographic scalability.
It between letting: **a)** a server or **b)** a client check forms as they are being filled.
![[hiding-comm-latencies-ex.png\|500]]
**(EXAMPLE 1)** When a service has been requested at a remote machine, useful *work can be done at the requester’s side* while waiting for a reply. 
- To do this, the requesting application must uses only ASYNCHRONOUS communication. 
- When a reply arrives, the application is interrupted and a special handler is called to complete the previously issued request. 
- Asynchronous communication can be used in batch-processing systems and parallel apps where independent tasks can be scheduled for execution while another task is waiting for the communication to complete. 
	- Alternatively, a new thread of control can be started to perform the request. 
	- Although, it blocks waiting for the reply, other threads in the process can continue.

**(EXAMPLE 2)** Many apps that cannot make effective use of asynchronous communication. 
When a user sends a request in an interactive app, they have nothing better to do than to wait. 
- Here, a better solution is to *reduce the overall communication*, for example, by *moving part of the computation* that is normally done at the *server* to the *client process requesting the service*. 
A typical case where this approach works is accessing databases using forms. 
- Filling in forms can be done by sending a separate message for each field and awaiting confirmation from the server; the server may check for syntax errors before accepting an entry.
- A better solution is to ship the code for filling in the form, and possibly checking the entries, to the client, and have the client return a completed form. 
 - This approach is widely supported by the Web by means of Java applets and JS.
#### (SOLUTION 2) Partitioning and Distribution (Domain Name Lookup & Addressing)
Involves taking a *component*, splitting it into *smaller parts*, and subsequently *spreading* those parts *across the system*.
**(EXAMPLE 1)** The Internet Domain Name System (DNS). The DNS name space is hierarchically organised into a tree of domains, which are divided into non-overlapping ZONES. 
	These zones allow the DNS to scale through distribution and decentralised administration, 
allowing different orgs to manage their own portions of the name space independently while maintaining a coherent global system.
- The names in each zone are handled by a single name server: Each path name being the name of a host in the Internet, and is thus associated with a network address of that host. 
![[zones-dns.png\|500]]
RESOLVING a name means returning the network address of the associated host.
E.g. `flits.cs.vu.nl` $R(\text{name}, Z_i) = \text{address}$ be the resolving of a name to a zone $i$ to an address. 
$$\begin{align} 
R(\text{flits.cs.vu.nl}, Z_1) = Z_2) \\ 
R(\text{flits.cs.vu}, Z_2) = Z_3) \\
R(\text{flits.cs}, Z_3) = \\
\text{host address} \\
\end{align}$$
- Eventually a zone will return the address of the associated host, show how the naming service, given by DNS, is distributed across several machines, thus avoiding that a single server has to deal with all requests for name resolution. 
- Hierarchical nature of DNS means that name lookup does not take twice as long when the number of machines on the Internet doubles

**(EXAMPLE 2)** the WWW is physically partitioned and distributed across a few hundred million servers, each handling a number of Web documents. 
- The name of the server handling a document is *encoded* into that document’s URL. 
- The Web only scaled to its current size due to this method of distributing of documents.
#### (SOLUTION 3) Replication
Scalability problems often appear in the form of *performance degradation*; the solution is to REPLICATE components across a DS. It *increases availability* and *balances the load* between components leading to better performance. Also, in geographically widely dispersed systems, having a copy nearby can *hide communication latency* problems.

**(DRAWBACKS)** Having multiple copies (cached or replicated), leads to *inconsistencies*: modifying one copy makes that copy different from the rest.
- Always keeping copies consistent requires *global synchronisation* on each modification.
- Global synchronisation precludes large-scale solutions.
### Pitfalls (9)
Many DSs are needlessly complex caused by mistaken assumptions:
The network is: **(1)** reliable, **(2)** secure, **(3)** homogeneous.
**(4)** Topology doesn't change. **(5)** Latency is zero. **(6)** Bandwidth is infinite.
**(7)** Transport cost is zero. **(8)** There is one administrator. **(9)** All clocks are synchronised.
	These *properties* are *unique* to DSs: reliability, security, heterogeneity, and network topology; 
latency and bandwidth; transport costs; administrative domains and clock synchronisation. 
When developing non-DAs, most of these issues will most likely not show up.
## 2. Types of Distributed Systems Pt.2 - *Chapter 1*
There are three types of DS that serve distinct purposes.
### 1. High performance distributed computing systems (HPDCS)
#### Distributed Shared Memory systems
High-performance distributed computing started with parallel computing
- Multiprocessor and multi-core versus multi-computer 
**(CONTEXT)** Distributed Shared Memory systems vs Multiprocessors
- Multiprocessors are easy to program in comparison to multi-computers, yet have problems when increasing the number of processors (or cores). 
- Distributed shared memory could never compete with that of multiprocessors, and failed to meet the expectations of programmers; they have been widely abandoned now.
**(RESULT)**  A shared-memory model on top of a multi-computer via virtual-memory techniques. 
- Map all main-memory pages (from different processors) into one single virtual address space. 
- If a process at processor A addresses a page P located at processor B, the OS at A traps and fetches P from B, just as it would if P had been located on local disk. 
#### Cluster Computing
Cluster computing systems became popular when the price/performance ratio of personal computers and workstations improved. It made economical sense to build a supercomputer using off-the-shelf technology by simply hooking up a collection of relatively simple computers in a high-speed network. 
- Mostly used for parallel programming in which a single (compute intensive) program is run in parallel on multiple machines.
- Essentially a group of high-end systems connected through a Local Area Network (LAN)
- Homogeneous: same OS, near-identical hardware
- Single managing node running the OS
**(EXAMPLE)** Linux-based Beowulf clusters
![[cluster-computing-system-ex.png\|500]]
Each cluster is a collection of mostly-identical COMPUTE nodes that are controlled and accessed by means of a single MASTER node. 
- This node handles the allocation of nodes to a particular parallel program, maintains a batch queue of submitted jobs, and provides an interface for the users of the system. 
- As such, the master node actually runs the *middleware* needed for the execution of programs and management of the cluster, while the compute nodes are equipped with a standard OS extended with typical middleware functions for communication, storage, fault tolerance, etc.
#### Grid Computing
The next step along in development is it have lots of nodes from any and everywhere.
These DSs are often constructed as a federation of computer systems, each may fall under a different administrative domain: hardware, software and network technology could differ.
- Enables flexible, secure and coordinated resource sharing among dynamic collections of individuals, institutions, and resources.
- Enable communities "Virtual Organisations" to share geographically distributed resources.

There is an emerging trend of a more hybrid architecture whereby nodes are configured specifically for certain tasks. This is even more prevalent in grid computing. 
- The processes belonging to the same virtual organization have access rights to the resources that are provided to that organization. 
- Typically, resources consist of compute servers (including supercomputers, possibly implemented as cluster computers), storage facilities, and databases. 
- In addition, special networked devices such as telescopes, sensors, etc., can be provided.

Most grid-computing software focuses on managing access to different administrative domains, and applications to users and virtual orgs. This outlines the architectural issues.
![[grid-computing-systems-proposed-arch-ex.png\|500]]
Typically the collective, connectivity, and resource layer form the heart of what could be called a GRID MIDDLEWARE LAYER, jointly providing access to and management of resources that are potentially dispersed across multiple sites; they act as a site (or administrative) unit.
	This prevalence is emphasized by the gradual shift toward a service-oriented architecture in 
which sites offer access to the various layers through a collection of Web services.
This defines an alternative architecture: Open Grid Services Architecture (OGSA), which is based upon a standardised version of the original architecture that follows Web service standards. 
#### Cloud Computing
From the perspective of grid computing, a next logical step is to simply outsource the entire infrastructure that is needed for compute-intensive applications. 
> (1) Cloud computing is an information technology infrastructure in which computing resources are *virtualised* and accessed as a *service*.

Provides the facilities to dynamically construct an infrastructure and compose what is needed from available services. Different to grid computing, which is associated with HPC.
> (2) Cloud computing is characterized by an easily usable and accessible pool of _virtualized_ resources. 

Usage of resources can be configured dynamically, providing the basis for scalability:
- if more work needs to be done, a customer can simply acquire more resources. 
- This links to utility computing because cloud computing is generally based on a pay-per-use model in which guarantees are offered by means of customized service level agreements (SLAs).
**(CONTEXT)** Researchers wanted to organize computational grids that were easily accessible and organizations running data centers wanted to open-up their resources to customers. 
	This lead to UTILITY computing: a customer could upload tasks to a data centre and be
charged on a per-resource basis, forming the basis of cloud computing.

The cloud has four layers:
![[the-organisation-of-clouds.png\|500]]

 **(1) Hardware**: The lowest layer formed to manage the necessary hardware.
- Generally implemented at data centres.
- Processors, routers, power and cooling systems- invisible to customers
**(2) Infrastructure**: Provides customer a mix of virtual storage and computing resources.
- Deploys virtualisation techniques. 
- Evolves around allocating and managing virtual storage devices and virtual servers
**(3) Platform**: Provides higher-level abstractions for storage and such. 
- An OS to app devs: easy to develop and deploy apps that need to run in a cloud. 
- In practice, an application developer is offered a vendor-specific API, which includes calls to uploading and executing a program in that vendor’s cloud. 
**(4) Application**: apps themselves run here, e.g., text processors, presentation apps.
- It is important to realize that these applications are executed in the VENDOR’s cloud.
- Comparable to the suite of apps shipped with OSes.

**(PROVISION)** These layers are provided through various interfaces (including CLI tools, programming and Web interfaces), leading to three different types of services:
- **Infrastructure-as-a-Service** (IaaS) covering the hardware and infrastructure layer
- **Platform-as-a-Service** (PaaS) covering the platform layer
- **Software-as-a-Service** (SaaS) in which their applications are covered
Cloud computing outsources local computing infrastructures but still has its obstacles including *provider lock-in*, *security* and *privacy* issues, and *dependency on the availability* of services.
### 2. Distributed information systems
#### Context
Many of the existing middleware solutions are the result of working with an infrastructure in which it was easier to integrate applications into an enterprise-wide information system. 
Organisations had *many* *networked applications* but struggled to achieve interoperability.

**(INTEGRATION)** A networked application simply of a server running that app (often including a db) and making it available to remote programs, called *clients*. 
- Clients send a request to the server for executing a specific operation and a response returned. 
- Integration at the lowest level allows clients to wrap a number of requests, possibly for different apps or servers, into a single larger request and have it executed as a distributed transaction.
- The key idea is that all, or none of the requests are executed. 
- As apps became more sophisticated and were separated into independent components (notably distinguishing db components from processing components), applications needed to communicate directly with each other, leading to *Enterprise Application Integration (EAI)*
#### Transactions
Database applications where operations are carried out in the form of TRANSACTIONS.
- In a mail system, there might be primitives to send, receive, and forward mail. 
- In an accounting system, they might be quite different. READ and WRITE are typical examples, however. Ordinary statements, procedure calls, and so on, are also allowed inside a transaction. 
- In particular, **remote procedure calls (RPCs)**, that is, procedure calls to remote servers, are often also encapsulated in a transaction, leading to what is known as a **transactional RPC**.

| Primitive           | Description                                     |
| ------------------- | ----------------------------------------------- |
| `BEGIN_TRANSACTION` | Mark the start of a transaction                 |
| `END_TRANSACTION`   | Terminate the transaction and try to commit     |
| `ABORT_TRANSACTION` | Kill the transaction and restore the old values |
| `READ`              | Read data from a file, a table, or otherwise    |
| `WRITE`             | Write data from a file, a table, or otherwise   |
The operations between them form the body of the transaction. 
- The characteristic feature of a transaction is _either all of these operations are executed or none are executed_. 
	- These may be system calls, library procedures, or bracketing statements in a language, depending on the implementation. 
- This all-or-nothing property of transactions is one of the four characteristic properties that transactions have. 

More specifically, transactions adhere to the so-called ACID properties:
**Atomic**: happens indivisibly (seemingly)
**Consistent**: does not violate system in-variants
**Isolated**: not mutual interference between concurrent transactions
**Durable**: commit means changes are permanent

**(SUB-TRANSACTION)** join to form a nested transaction. 
- The top-level transaction may fork off children that run in parallel with one another, on different machines, to gain performance or simplify programming. 
- Each child may also execute one or more sub-transactions, or fork off its own children. 
**(ISSUES)** Imagine a transaction starts several subs in parallel, and one commits, making its results visible to the parent transaction. 
- After further computation, the parent aborts, restoring the entire system to the state it had before the top-level transaction started. 
- Consequently, the results of the sub that committed must be undone. Thus, the permanence referred to above applies only to top-level transactions.

Since transactions can be nested arbitrarily deep, considerable administration is needed to get everything right. The semantics are clear, however. 
- When any transaction or subtransaction starts, it is conceptually _given a private copy of all data in the entire system for it to manipulate_ as it wishes. 
	- If it aborts, its private universe just vanishes, as if it had never existed. 
	- If it commits, its private universe replaces the parent’s universe. 
- Thus if a subtransaction commits and then later a new subtransaction is started, the second one sees the results produced by the first one. 
- Likewise, if an enclosing (higher level) transaction aborts, all its underlying subtransactions have to be aborted as well. 
- And if several transactions are started concurrently, the result is as if they ran sequentially in some unspecified order.
**(TRANSACTION PROCESSING MONITOR)**  The *data* involved in a transaction is *distributed across several servers*. 
A TP Monitor is responsible for coordinating the execution of a transaction and the commitment of subtransactions following a standard protocol known as distributed.

Nested transactions are important in DS, naturally providing a way of distributing a transaction across multiple machines, following a logical division of the work of the original transaction. 

For example, a transaction for planning a trip by which three different flights need to be reserved can be logically split up into three subtransactions. 
- Each of these subtransactions can be managed separately and independently of the other two.
![[tpm-in-a-ds.png\|500]]
Apps wanting to coordinate several subtransactions into a single transaction did not have to implement this coordination themselves because a TP monitor does this for them. 
This is exactly where MIDDLEWARE comes into play: 
- it implements services that are useful for many applications avoiding that such services have to be reimplemented over and over again by application developers.

**(Middleware and EAI)** Several types of communication middleware exist. 
With remote procedure calls (RPC), an application component can effectively send a request to another application component by doing a local procedure call, which results in the request being packaged as a message and sent to the callee. 

The result will be sent back and returned to the application as the result of the procedure call. 
Later, techniques were developed to allow calls to remote objects, leading to what is known as **remote method invocations (RMI)**. An RMI is _essentially the same as an RPC, except that it operates on objects instead of functions_.

Middleware offers communication facilities for application integration (EAIs)
![[middleware-in-eai.png\|500]]
**Remote Procedure Call (RPC):** Requests are sent through local procedure call, packaged as message, processed, responded through message, and result returned as return from call.

RPC and RMI have the _disadvantage_ that the caller and callee both need to be up and running at the time of communication. 

In addition, they need to know **exactly** how to refer to each other. This **tight coupling** is often experienced as a serious drawback, and has lead to what is known as **message-oriented middleware (MOM)**. 

Here, applications send messages to logical contact points, described by means of a subject. 
- Likewise, applications can indicate their interest for a specific type of message, after which the communication middleware will take care that those messages are delivered to those apps. 
- These so-called **publish/subscribe systems** form an important and expanding class of DS.
- Messages are sent to logical contact point (published), and forwarded to subscribed apps.
### 3. Distributed Systems for Pervasive computing (DPS)
Next for DS where nodes are small, mobile, and often embedded in a larger system, characterised by the fact that the system *naturally blends into the user's env*. Coined as the IoT.
- We often need to deal with the intricacies of wireless and mobile communication, will require special solutions to make a pervasive system as transparent or unobtrusive as possible.

They're unique because the separation between users and system components is more blurred. 
There is often no single dedicated interface, such as a screen/keyboard combination. 
Instead, a **pervasive** system is often equipped with many **sensors** that pick up various aspects of a user’s behavior. Likewise, it may have a myriad of **actuators** to provide information and feedback, often even purposefully aiming to _steer_ behavior.
#### Ubiquitous Computing Systems (UCS)
Pervasive and continuously present, i.e., a continuous interaction between system and user often with the user being unaware that the interaction is happening.
Core requirements for a UCS roughly as follows: 
1. (**Distribution**) Devices are networked, distributed, and accessible in a transparent manner 
2. (**Interaction**) Interaction between users and devices is highly unobtrusive, hiding interfaces.
Much of the interaction by humans will be **implicit**: _one that is not primarily aimed to interact with a computerized system but which such a system understands as input_.
**(Ex)** The settings of a car’s driver’s seat, steering wheel, and mirrors is fully personalized. 
If Bob takes a seat, the system will recognize that it is dealing with Bob and subsequently makes the appropriate adjustments. 
3. (**Context awareness**) The system is aware of a user’s context in order to optimize interaction.
_any information that can be used to characterize the situation of entities (i.e., whether a person, place or object) that are considered relevant to the interaction between a user and an application, including the user and the application themselves_. 

In practice, context is often characterized by location, identity, time, and activity: 
- the where, who, when, and what. 
- A system will need to have the necessary (sensory) input to determine one or several of these context types. 
- Raw data as collected by various sensors is lifted to a level of abstraction that can be used by applications. 

A concrete example is detecting where a person is, for example in terms of GPS coordinates, and subsequently mapping that information to an actual location, such as the corner of a street, or a specific shop or other known facility. 

The question is where this processing of sensory input takes place: 
- is all data collected at a central server connected to a database with detailed information on a city, or is it the user’s smartphone where the mapping is done? 
- When it comes to combining flexibility and potential distribution, so-called shared data spaces in which processes are decoupled in time and space are nice, yet, suffer from scalability problems. 
4. (**Autonomy**) Devices operate autonomously without human interven- tion, and are thus highly self-managed 
In a UCS env, the system administrator cannot keep everything up and running. 
The system should act autonomously, and automatically react to changes using many techniques:
- **Address allocation**
	- In order for networked devices to communicate, they need an IP address. Addresses can be allocated automatically using protocols like the Dynamic Host Configuration Protocol (DHCP) (which requires a server) or Zeroconf. 
- **Adding devices**
	- It should be easy to add devices to an existing system. A step towards automatic configuration is realized by the Universal Plug and Play Protocol (UPnP).
	- Using UPnP, devices can discover each other and make sure that they can set up communication channels between them. 
- **Automatic updates
	- Many devices in a UCS should be able to regularly check the Internet software updates
	- Manual intervention is to be kept to a minimum
4. (**Intelligence**) The system as a whole can handle a wide range of dynamic actions and interactions.
UCS often use methods and techniques from the field of artificial intelligence. 
In many cases, a wide range of advanced algorithms and models need to be deployed to handle incomplete input, quickly react to a changing environment, handle unexpected events, and so on. 
#### Mobile computing systems 
Pervasive, but emphasises that devices are inherently mobile.
A distributed system where devices can change physical location and network attachment points while maintaining connectivity, often forming a key part of **pervasive computing**.

* **Device Heterogeneity:** Encompasses a wide range of **IP-enabled devices** beyond smartphones/tablets (e.g., IoT sensors, wearables, vehicular systems).
* **Wireless Communication:** The primary enabler of mobility (*Mobile typically implies wireless*).
* **Dynamic Location:** A device's **network location** (IP address/point of attachment) and **physical location** change over time. This is the central challenge.
![[mobile-comp-ex.png\|500]]
**A. Service Discovery & Availability**
* **Problem:** As devices move, the set of locally available services changes.
* **Required Mechanism:** **Dynamic service discovery** (services announce presence, clients discover them). This is crucial for context-aware applications.
*   *Example:* Your smartphone automatically discovering and connecting to a nearby wireless printer when you enter an office, but not seeing it at home.
**B. Location Management**
*   **Geographical Location:** Needed for **tracking/tracing** (e.g., GPS in a delivery vehicle).
*   **Network Location:** Needed for routing. Solved by protocols like **Mobile IP**, which separates a device's permanent **home address** from its temporary **care-of address**, allowing seamless movement between networks.
**C. Communication in Dynamic Networks: MANETs & DTNs**
*   **MANET (Mobile Ad-hoc Network):** A self-configuring network of mobile devices connected wirelessly without fixed infrastructure.
    *   **Core Routing Problem:** **Static routes fail** as nodes move. Requires **proactive** (e.g., DSDV) or **reactive** (e.g., AODV) routing protocols.
*   **DTN (Disruption/Disconnection-Tolerant Network):** Assumes **intermittent connectivity** and no guaranteed end-to-end path.

DTN Routing Principles (Store-Carry-Forward).
When a continuous path cannot be established, DTNs use a **store-carry-forward** paradigm:
1.  **Store:** A node receives and **stores** a message.
2.  **Carry:** The node physically **carries** the message as it moves.
3.  **Forward:** When it finds another node, it may **forward** the message based on a chosen strategy.
_Passing messages in a (mobile) disruption-tolerant network_
![[passing-messages-disruption-tolerant-network.png\|500]]
**Routing Strategies:**
*   **Flooding-Based (e.g., Epidemic Routing):** Messages spread redundantly. High delivery chance but high resource cost.
*   **Selective Forwarding:** More efficient. Decisions based on:
    *   **Utility/Probability:** Forward to nodes with higher historical probability of contacting the destination.
    *   **Social Patterns:** Forward to nodes belonging to the same "community" (**social-based routing**).
    *   **Connectivity:** Forward to **well-connected nodes** (central hubs in the mobility pattern).
*   ***Technical Example:** A wildlife tracker on a zebra (Node A) needs to send data to the ranger station (Node D). There's no direct path. The zebra moves and encounters a vulture's tracker (Node B). Using a **utility-based** protocol, Node A forwards the data to Node B. The vulture flies and later lands near a fixed sensor node (Node C) connected to the station, finally delivering the data. The network tolerated long delays and disconnections.*
Mobile computing shifts the design focus from stable connectivity and fixed locations to **mobility management, discovery, and tolerant routing** in the face of constant change and potential disconnection. Mobile devices set up connections to stationary servers, bringing mobile computing in the position of clients of cloud-based services.
#### Sensor (and actuator) networks: 
Pervasive, with emphasis on the actual (collaborative) sensing and actuation of the env.
Here is the content reformatted as requested, with headings prepended to the paragraphs and the "Features" section integrated.

**(SENSOR NETWORKS)** They are more than just a collection of input devices; instead, sensor nodes often collaborate to efficiently process the sensed data in an application-specific manner, making them very different from, for example, traditional computer networks. A sensor network generally consists of tens to hundreds or thousands of relatively small nodes, each equipped with one or more sensing devices. In addition, nodes can often act as actuators, a typical example being the automatic activation of sprinklers when a fire has been detected.

**(KEY CHARACTERISTICS)** Many sensor networks use wireless communication, and the nodes are often battery powered. Their limited resources, restricted communication capabilities, and constrained power consumption demand that efficiency is high on the list of design criteria.

**(NODE ARCHITECTURE)** When zooming into an individual node, we see that, conceptually, they do not differ a lot from “normal” computers: above the hardware there is a software layer akin to what traditional operating systems offer, including low-level network access, access to sensors and actuators, and memory management. Normally, support for specific services is included, such as localization, local storage (think of additional flash devices), and convenient communication facilities such as messaging and routing. However, similar to other networked computer systems, additional support is needed to effectively deploy sensor network applications. In distributed systems, this takes the form of middleware.

**(PROGRAMMING MODELS & COMMUNICATION SCOPE)** For sensor networks, instead of looking at middleware, it is better to see what kind of programming support is provided. One typical aspect in programming support is the scope provided by communication primitives. This scope can vary between addressing the physical neighborhood of a node, and providing primitives for system-wide communication. In addition, it may also be possible to address a specific group of nodes. Likewise, computations may be restricted to an individual node, a group of nodes, or affect all nodes.
*   *Example (Abstract Regions):* A node can define a region of its eight nearest neighbors. It can write a sensor reading to that region, and then perform an operation like finding which node in the region has the maximum reading, enabling local, collaborative processing.

**(DATABASE VIEW & DATA ACCESS)** As another related example, consider a sensor network as implementing a distributed database. This view is common as many networks are deployed for measurement and surveillance, where an operator extracts information by issuing queries.

**(DATA PROCESSING ARCHITECTURES)** To organize a sensor network as a distributed database, there are essentially two extremes:
1.  **Centralized Processing:** Sensors send all raw data to a central operator's site. This wastes network resources and energy.
2.  **Purely Edge Processing:** Queries are sent to sensors, each computes an answer, and the operator aggregates all responses. This wastes the opportunity for sensors to aggregate data locally, reducing the volume of traffic.

**(IN-NETWORK PROCESSING)** What is needed are facilities for **in-network data processing**. One common method is to use a **routing tree**. A query is propagated down the tree to all nodes, and results are **aggregated** (e.g., summed, averaged, filtered) at intermediate nodes as they are passed back up to the root (the operator). This conserves energy and bandwidth.
*   *Example (TinyDB):* Implements a declarative SQL-like interface. It uses a tree-based routing algorithm where parent nodes schedule data collection from children and aggregate results before forwarding them upward.

**(PUBLISH/SUBSCRIBE MODEL)** An alternative to a single, query-specific tree is a **publish/subscribe** model. Nodes publish specific types of data (e.g., temperature readings) which are forwarded to designated **broker nodes**. Queries for that data type are then sent to the corresponding broker. This decouples data producers from consumers.

**(FEATURES)** Sensor networks are characterized by:
*   **Collaborative Processing:** Nodes work together to process data.
*   **Severe Resource Constraints:** Limited battery, CPU, memory, and communication bandwidth.
*   **Application-Specific Design:** The network architecture is tightly coupled to its sensing task.
*   **Wireless Communication & Ad-hoc Deployment.**
*   **In-Network Processing:** Computation occurs within the network to reduce data volume and save energy.
*   **Diverse Programming Models:** Including the database abstraction (e.g., TinyDB) and region-based programming.
**(BOTTOM LINE)** While mobile devices often act as clients to cloud services (**mobile cloud/edge computing**), sensor networks represent a different paradigm: a massively distributed, collaborative, and deeply embedded system where processing is pushed into the network itself to overcome severe resource limitations and achieve scalability.
![[cloud-edge-continuum.png\|500]]
**(WHAT IS GOOGLE SPANNER?)** Google Spanner is a globally distributed, strongly consistent, relational database service. Its unique achievement is combining the horizontal scalability of NoSQL systems with **Strong ACID transactions** and an SQL interface across data centers worldwide. It was first described in 2012 and is used internally for services like Google Ads, Gmail, and Google Photos before being offered as Cloud Spanner

**(CORE INNOVATIONS)** It works by automatically sharding data across many Paxos-based replication groups for availability. Its most significant innovation is **TrueTime**, a globally synchronized clock API that uses GPS and atomic clocks to bound clock uncertainty across servers. TrueTime is the key that enables its powerful consistency properties without sacrificing performance globally.

**(CONSISTENCY PROPERTIES)** By default, Spanner provides **External Consistency**, which is the strictest isolation level for transaction-processing systems.
It has two formal guarantees:
- **Strict Serializability**: The system behaves as if all transactions executed one at a time in some order (**serializability**), and that order respects real-time: if transaction T2 starts _after_ T1 commits, T2 must appear to execute _after_ T1 in the serial order.
- **External Consistency**: This adds a real-time ordering guarantee for transactions _observed_ to commit in sequence. If T2's commit process begins after T1's commit _returns successfully_, T2 is guaranteed to have a later commit timestamp than T1. This prevents anomalies like seeing a withdrawal before a deposit that visibly completed earlier. External consistency is a stronger property than both linearizability (for single operations) and basic strong consistency

**(HANDLING ACID TRANSACTIONS)** Spanner provides full ACID guarantees globally
This is achieved through a specific architecture:
- **Atomicity & Durability**: Uses a **Paxos-based synchronous replication** across every data shard. A write must be accepted by a majority of replicas before it is committed, ensuring durability. The two-phase commit protocol coordinates commits across different shards involved in a transaction
- **Isolation & Consistency**: Primarily uses **Multi-Version Concurrency Control (MVCC)**. Writes create new, timestamped versions of data
- TrueTime assigns globally meaningful, monotonically increasing timestamps to commits, establishing the externally consistent serial order. **Read-write transactions** (for reads and writes) use locking and get external consistency. **Read-only transactions** are executed at a chosen timestamp (system-chosen for strong reads, past for stale reads) without acquiring locks, reading a consistent snapshot.
- **Alternative Isolation Level**: Spanner also offers **Repeatable Read (Snapshot) Isolation**, which can improve performance for read-heavy, low-conflict workloads. Unlike the default, it is susceptible to **write skew**, so applications must use locking reads (`FOR UPDATE`) for correctness in critical sections.
## 3.  Architectures - *Chapter 2*
### Understand the different ways on how to view the organisation of a DS
An architecture defines the system through **components** (replaceable units with interfaces), their **connectors**, the **data** they exchange, and how they are **configured** into a whole.
![[machine-interfaces.png\|500]]
### Architectural styles
#### Layered
Components are organised in hierarchical layers. A component in layer `Lj` can make a **downcall** to a lower layer `Li` (`i<j`) and expects a response. Upcalls to higher layers are rare.
![[layered-architectures.png\|500]]
**(LAYERED COMMUNICATION PROTOCOL)** A classic example is a protocol stack (e.g., TCP/IP). Each layer offers a **service** through a specific **interface** and implements that service using a **protocol** (rules for data exchange). For instance, **TCP** offers a reliable, connection-oriented service via a `socket` interface, governed by a protocol managing connections and data ordering.
![[layered-communication-protocol-ex.png\|500]]
**(APPLICATION LAYERING: THE PAD MODEL)** Many client-server apps use three logical layers:
1.  **Presentation:** User interface.
2.  **Application:** Core processing logic.
3.  **Data:** Persistent data management.
![[pad-search-engine-ex.png\|500]]
*Example:* In a cloud search engine, a user's query (Presentation) is routed by a load balancer to an application server (Application), which processes it by querying an index (Data).
![[cloud-search-ex.png\|500]]
#### Object-based
The system is structured as a collection of loosely coupled **objects** (or **components**), where each object encapsulates its state (data) and behaviour (methods). Interaction occurs via **Remote Method Invocation (RMI)**.
Objects are accessed through interfaces, hiding their implementation. In a distributed object, the interface can be placed on a client machine via a **proxy** (client stub), while the object itself resides on a server. The proxy handles communication (marshalling/unmarshalling), forwarding requests to the server-side **skeleton**.
![[object-based-arch-remote-obj-client-side-proxy.png\|500]]
*Example:* A Java RMI application where a client-side proxy object forwards method calls to the actual object on the server.
This style, promoting clear encapsulation, forms the foundation for **Service-Oriented Architectures (SOA)**, where systems are built by composing independent, often remotely hosted, services (e.g., an online shop using a third-party payment service).
#### Resource-centred
This style, exemplified by **REST (Representational State Transfer)**, models a system as a collection of uniquely named **resources**. All services offer the same, minimal interface and interactions are **stateless**.
*   **Key Principles:**
    1.  Resources have unique identifiers (e.g., URIs).
    2.  Uniform interface (typically **CRUD** operations).
    3.  Messages are self-descriptive (e.g., HTTP headers).
    4.  Stateless execution (server forgets client after request).
*   **CRUD Operations:** `POST` (Create), `GET` (Retrieve), `PUT` (Update), `DELETE`.
*   *Example:* **Amazon S3**. A file (object) in a bucket (directory) is accessed via a URI (`http://BucketName.s3.amazonaws.com/ObjectName`). Creating the object uses `PUT` on that URI; listing bucket contents uses `GET`.

#### Event-based (Publish-Subscribe)
Components interact by generating and reacting to events, leading to **loose coupling** between producers (publishers) and consumers (subscribers).
Coordination models vary based on temporal and referential coupling:
![[event-based-vs-shared-data-space-arch-ex.png\|500]]
*   **Event-Based Coordination:** Subscribers express interest in event types (**topics** or **content**). When a publisher generates an event, the middleware (**event bus**) delivers it to all matching subscribers. Processes are **referentially decoupled** (don't know each other) but often **temporally coupled** (subscriber must be running).
*   **Shared Data Space:** Processes communicate by writing and reading **tuples** (structured data records) to/from a shared, associative store. This is both **referentially and temporally decoupled**. A process can subscribe to tuples matching a pattern and be notified when one is published.
*   *Example:* A sensor network where a motion sensor (publisher) emits an "occupancy" event. A security service (subscriber), which registered for this event, receives the notification and checks if the door lock status (another event) is "unlocked" to trigger an alert.
### System architecture
#### Centralised
Based on the **client-server model**, where **servers** (providers) and **clients** (users) of services are distinct roles, communicating via a request/reply model.
![[centralised-org-arch-ex.png\|500]]
*   **2-Tiered:** Direct client-to-server communication. The client may handle presentation and some application logic (fat client) or be very thin.
![[2-tiered-arch-ex.png\|500]]
*   **3-Tiered:** A more scalable architecture with distinct presentation, application logic, and data tiers. Servers in one tier can act as clients to the next (e.g., an application server requesting data from a database server).
![[3-tier-arch-ex.png\|500]]
#### Decentralised (Peer-to-Peer - P2P)
All processes (**peers**) are equal, acting as both client and server (**servant**). Functions are distributed across all peers.
*   **Unstructured P2P:** Peers connect in an ad-hoc manner, forming a random overlay network. Data location is non-deterministic.
    *   **Search by Flooding:** A query is sent to all neighbours, who propagate it further (limited by a TTL). High network cost but fast.
    *   **Search by Random Walk:** A query is forwarded to one randomly chosen neighbour at a time. Lower cost but slower; multiple parallel walks speed it up.
*   **Structured P2P:** Peers organise into a specific, deterministic overlay (e.g., a ring, tree, or hypercube). Data items are assigned unique **keys** via hashing and stored in a **Distributed Hash Table (DHT)**. Lookups are routed efficiently through the overlay to the peer responsible for that key.
![[p2p-arch-as-4d-hyper-cube-ex.png\|500]]
*   *Example (Structured P2P):* In the 4D hypercube above, to find data with key `14` (`1110` in binary), node `0111` would route the request through connected neighbours, following the overlay protocol, until it reaches the node responsible for key `1110`.
#### Hybrid
Combines elements of centralised and decentralised architectures.
A prime example is the **edge-server system**. Servers are placed at the network's **edge** (e.g., at an Internet Service Provider). An origin server in a central data centre replicates content to these edge servers. End-users connect to a nearby edge server for low-latency access, while the system as a whole is managed centrally.
![[internet-as-edge-servers-ex.png\|500]]
This model extends to **fog/edge computing**, where computation and storage are distributed between the central cloud, edge servers, and even end-user devices for ultra-responsive services.
## 4. Communication - *Chapter 4*
At its core, communication in a distributed system involves **processes on different machines exchanging information** via **low-level message passing** over a network. Higher-level models (like RPC) are abstractions built on this fundamental mechanism.
### Latency and Bandwidth
Two critical performance metrics for any communication channel.

**(LATENCY)** The **delay** or time it takes for a single message (or the first bit of data) to travel from source to destination. It is primarily limited by the speed of light and processing delays in network hardware.
*   *Example:* Datacenter (1ms) vs. Intercontinental (100ms) vs. "Sneakernet" in a van (1 day).

**(BANDWIDTH)** The **capacity** or volume of data that can be transmitted per unit of time (e.g., bits per second).
*   *Example:* 5G (~5-20 Gbps) vs. Home Broadband (~300 Mbps) vs. Van of Hard Drives (~1 Gbps effective).

**Key Relationship:** The total time to transfer a message is the sum of the **latency** and the time to push all the data through the pipe, which is the message size divided by the **bandwidth**.

**Check of Understanding**
> **Question:** How long does it take to send a 10 MB (80 Mbit) file over a link with 100 ms latency and 1 Gbps (1000 Mbit/s) bandwidth?
> **Answer:** Time = Latency + (Size/Bandwidth) = 0.1s + (80 Mbit / 1000 Mbit/s) = 0.1s + 0.08s = **0.18 seconds.** Here, latency is the dominant factor. For a 10 GB file, the bandwidth term would dominate.

### Layered Protocols
Communication is organised into layers, each providing a service to the layer above and using the service of the layer below. This modularity hides complexity.

The classic **OSI model** has seven layers, but practical systems like **TCP/IP** use fewer.
![[layered-protocols.png\|500]]

**(MIDDLEWARE PROTOCOLS)** This layered model is extended in distributed systems by **Middleware**, which sits between the OS/transport layers and the application. It provides common services like secure communication, data marshalling, and naming, freeing the application developer from re-implementing them.
![[middleware-protocols.png\|500]]
![[middleware-adapted-layering-scheme.png\|500]]
### Types of Communication
Communication can be characterized along two key dimensions:
**(SYNCHRONOUS vs. ASYNCHRONOUS)** This concerns the **timing and blocking behaviour** of the sender and receiver.
*   **Synchronous:** The sender blocks and waits until the receiver is ready to accept the message (or a reply is received).
*   **Asynchronous:** The sender continues execution immediately after issuing the send; the message is stored (buffered) until the receiver can process it.
![[communication-sync-ex.png\|500]]
**(TRANSIENT vs. PERSISTENT)** This concerns the **lifetime and storage** of the message in the communication system.
*   **Transient:** A message is only stored *en route* while travelling to the receiver. If it cannot be delivered immediately (e.g., the receiver is down), it is **discarded**.
*   **Persistent:** A message is stored in a **queue** (e.g., at a message server) until it can be successfully delivered to the receiver, even if the receiver is temporarily unavailable.

**Common Combinations:**
*   **Client-Server / RPC:** Typically uses **transient, synchronous** communication (request-reply). Both parties must be active.
*   **Message-Oriented Middleware:** Typically uses **persistent, asynchronous** communication. Provides decoupling and fault tolerance.

**Check of Understanding**
> **Question:** What is a major drawback of the transient, synchronous model used in classic client-server systems?
> **Answer:** It requires **both client and server to be running simultaneously**. If the server crashes after the client sends a request, the client may be left hanging (blocked) indefinitely, leading to poor reliability and scalability.

### Remote Procedure Call (RPC)
RPC is an abstraction that makes a remote function call appear like a **local procedure call** to the programmer, hiding all network complexity.

**(BASIC OPERATION)** A **client stub** on the caller's machine masquerades as the actual server procedure. It **marshals** parameters into a network message, sends it, waits for the reply, **unmarshals** the result, and returns it.
![[rpc-operation-ex.png\|500]]
*Example:*
```java
// This looks local, but `processPayment` executes on a remote server.
Result result = paymentsService.processPayment(cardDetails, amount);
```

**(PARAMETER PASSING & MARSHALLING)** A core challenge is transforming data (parameters, results) from a machine-specific in-memory format into a neutral network format. This involves:
*   **Byte Order (Endianness):** Converting between big-endian and little-endian integer representations.
*   **Data Representation:** Agreeing on formats for floats, strings (ASCII/EBCDIC), and complex structures.
*   **Serialisation:** Flattening an object's state into a byte stream.
![[parameter-passing.png\|500]]

**(ASYMPTOTOIC RPC)** To avoid clients blocking, RPC can be made asynchronous. The client sends a request and continues, later collecting the result via a separate call or a callback.
![[async-rpc-ex.png\|500]]

**(REMOTE METHOD INVOCATION - RMI)** RMI is the object-oriented extension of RPC, allowing invocation of methods on remote objects. It uses client-side **proxy objects** that represent the remote object.
![[rmi-ex.png\|500]]

**Check of Understanding**
> **Question:** What is the key technological difference between early RPC systems (e.g., SunRPC) and modern ones like gRPC?
> **Answer:** While the core concept is the same, modern RPC frameworks like **gRPC** typically use **HTTP/2** as the transport for multiplexing and efficiency, and **Protocol Buffers** (a binary, language-neutral interface definition language) for highly efficient, version-tolerant marshalling/serialisation, as opposed to older textual or more verbose formats like XML.

### Message Oriented Middleware
MOM provides **persistent, asynchronous communication** via messages, leading to loosely coupled, reliable systems.

**(MESSAGE-QUEUING MODEL)** The core abstraction is a **queue**. Senders place messages into a queue, and receivers take messages from it. Queues decouple the communicating processes in time (asynchronous) and space (they don't need to know each other's address).
![[mpi-message-queue-model.png\|500]]
![[message-queuing-system-ex.png\|500]]

**(THE MESSAGE-PASSING INTERFACE - MPI)** MPI is a standard API for **transient messaging** primarily used in High-Performance Computing (HPC). It assumes a known, running group of processes.
*   It provides a rich set of primitives for point-to-point (`MPI_Send`, `MPI_Recv`) and collective communication (broadcast, scatter/gather).
![[advanced-transient-messaging-ex.png\|500]]
*Example Primitives:*
    *   `MPI_Send(buf, count, type, dest, tag, comm)`: Sends a message.
    *   `MPI_Recv(buf, count, type, source, tag, comm, status)`: Receives a message (blocks).
    *   `MPI_Isend()` / `MPI_Irecv()`: Non-blocking versions for overlap of communication and computation.

**(ADVANCED MESSAGE QUEUING PROTOCOL - AMQP)** AMQP is an open standard application-layer protocol for MOM, defining how messages are formatted and transmitted between systems (e.g., between a publisher and a queue, or a queue and a consumer).
*   *Example Systems:* **RabbitMQ** (a widely-used AMQP broker), **Apache ActiveMQ**. They are used for enterprise integration, task distribution, and in cloud platforms.
*   *Use Case - OpenStack:* Uses a message queue (like RabbitMQ) as the central nervous system. All service requests (e.g., "launch a VM") are placed on queues, and worker processes listen to these queues and execute the tasks, enabling a scalable, decoupled control plane.

**Check of Understanding**
> **Question:** A financial trading system needs to ensure that a "sell order" message is never lost, even if the trading engine process crashes momentarily. Which communication paradigm and characteristic (transient/persistent) is essential here?
> **Answer:** This requires **Message-Oriented Middleware** with **persistent communication**. The "sell order" message would be placed into a durable queue. The trading engine, upon restarting, would retrieve the message from the queue and process it, guaranteeing "at-least-once" delivery.
## 5. Service Oriented Architectures
### Conceptual Design of Software Systems
At a conceptual level, software systems are broken into layers, leading to tiered architectures.
![[1-2-3-n-tier-architectures-ex.png\|500]]
### Architectures
#### 1-Tier
All components (presentation, logic, data) are tightly integrated into a single, monolithic application and deployment unit. Example: A standalone desktop application with its own embedded database.
#### 2-Tier
The classic client-server model. A **client** (handling presentation and some logic) communicates directly with a **server** (typically a database). Example: A desktop application that connects to a central database server.
#### 3-Tier
Introduces a dedicated middle tier, separating concerns:
*   **Presentation Tier:** User interface.
*   **Application/Logic Tier:** Core business rules and processes.
*   **Data Tier:** Persistent data storage.
This provides better scalability, security (the database is not directly exposed), and maintenance. Example: A web application (Presentation) using an application server (Logic) to query a database (Data).
#### N-Tier
A generalisation of the 3-tier model, where any tier can be subdivided or services can be integrated, creating a more complex, distributed architecture. This often arises from:
1.  Integrating full legacy systems (which are themselves 2 or 3-tier) as resources.
2.  Adding dedicated web or API gateway servers as an extra presentation tier.
**Challenge:** N-tier architectures face significant complexity in **integration** due to a lack of universal standards, requiring extensive custom middleware to connect disparate systems.
### Emergence of SOAs
SOA emerged as a direct response to the challenges of N-tier and enterprise application integration. The trend was toward integrating complex, decoupled systems, often from different organisations. SOA facilitates this by treating application **functionality as reusable, loosely-coupled services**.
### Vision
SOA is an architectural style for building applications from **independent services** that communicate via messages. It represents a paradigm shift from tightly-coupled object-oriented systems to **message-oriented**, platform-independent integration. ARCH MODEL:
Top-bottom: Services/APIs, App, App server, Managed Runtime Env, OS, Hypervisor, Storage, Network, Hardware.
The SOA vision is built on integrating **Resources** (data, compute), **Communities** (procedures), and **Technologies** through standardised **Services** and **Connectivity**.
#### What is a ‘Service’?
A service is a reusable, self-contained software component that implements a discrete business function (e.g., "get user data," "process payment"). It is accessed strictly through its defined, message-based interface. Examples include weather forecasts, credit checks, or translation engines.
#### Architecture Model
The SOA model defines three core roles and three core operations:
![[soa-architecture-ex.png\|500]]
*   **Roles:** Service Provider (hosts), Service Registry (finds), Service Consumer (uses).
*   **Operations:** Publish (provider lists service), Find (consumer locates service), Bind (consumer connects to and uses the provider).
#### Service Provision and Consumption
![[service-provision-and-consumption.png\|500]]
This illustrates the dynamic interaction: a service is implemented and published by a provider. A consumer discovers it via the registry and subsequently binds to the provider to execute it.
#### Differences between SOAs and traditional n-tier architectures
1.  **Decentralised Middleware:** In SOA, communication logic (middleware) is embedded within each service's environment, not centralised in a single application server tier.
2.  **Strong Emphasis on Loose Coupling:** Services have minimal dependencies on each other's internal implementation, communicating via standardised messages. This contrasts with the tighter integration often found in N-tier layers.
3.  **Adherence to Well-Supported Standards:** SOA relies heavily on global, platform-neutral standards (e.g., XML, SOAP, WSDL) to enable interoperability across organisational boundaries, unlike the often proprietary or bespoke interfaces in N-tier systems.
**Check of Understanding**
> **Question:** A company has a monolithic 1-tier application for customer orders. They want to expose the "check inventory" and "process payment" functions to new mobile and web apps, and also to partner companies. Explain why moving to an SOA style would be more effective than simply creating a 3-tier version of their existing system.
> **Answer:** A 3-tier architecture would primarily separate logic internally for scalability but would likely still result in a single, integrated application with a single point of access. SOA, by contrast, would decompose "check inventory" and "process payment" into **independent, reusable services**. These services could then be:
> *  **Discovered and consumed separately** by the new mobile app, web app, and partner systems.
> *   **Scaled independently** based on demand.
> *   **Updated or replaced** without affecting the entire order system, thanks to **loose coupling** via standardised interfaces. This provides the agility and openness needed for multi-channel and inter-organisational integration that a standard 3-tier refactor would not.
## 6. Web Services and REST
### Why do we need Web services?
The evolution of distributed computing has moved from linking machines (Internet/TCP/IP) to linking documents (WWW/HTTP/HTML) to linking **applications**. Web services fulfill this third stage, enabling programmatic, reliable interaction between disparate applications over networks, moving beyond unreliable "screen scraping" of websites. They support industry needs for standardised data exchange using formats like **XML** and **JSON**.
### What are web services?
A **Web Service** is a unit of business logic accessible over a network (Internet/intranet) using standard protocols (HTTP, SMTP) and data formats (XML, JSON). It provides a **coarse-grained** way to expose functionality from existing platforms (JEE, .NET) within a **Service-Oriented Architecture (SOA)**, promoting large-scale, **loosely-coupled** systems without mandating specific implementation technologies.

**(SOA ROLES & EXAMPLE)** In an SOA with web services, there are three core roles: the **Provider** (publishes the service), the **Broker** (registry/directory like a "Yellow Pages"), and the **Requestor** (finds and uses the service). This enables dynamic discovery and integration.
![[soa-web-services.png\|500]]
*Example:* An airline's booking system (on Oracle Solaris) and a car rental website (on Linux) can be integrated via web services, allowing seamless combined bookings despite different underlying hardware and software.

**(KEY FEATURES)**
*   **Standardised Data Packaging:** Uses XML/JSON over standard transports (HTTP, SMTP).
*   **Interoperability Abstraction:** Provides a web-friendly layer over traditional RPC/RMI.
*   **Self-Describing:** Described using languages like WSDL.
*   **Discoverable:** Published to and discoverable via service registries.
### What is REST?
**REST (Representational State Transfer)** is an **architectural style** (not a standard) for building distributed systems, particularly web services. It views a system as a collection of **resources** (each identified by a **URI**) and uses a **uniform interface** (HTTP verbs) to manipulate them. It is the predominant style for modern web APIs.
### What does it consist of?
**(RESOURCE-BASED ARCHITECTURE)** The core REST principles are:
1.  **Resources identified by URIs:** Every piece of data is a resource with a unique address (e.g., `/parts/00345`).
2.  **Uniform Interface:** A constrained set of well-defined operations using HTTP verbs: **POST** (Create), **GET** (Retrieve), **PUT** (Update), **DELETE** (Delete).
3.  **Self-Descriptive Messages:** Requests and responses contain all information needed to be understood (via HTTP headers and body).
4.  **Stateless Interactions:** The server does not retain client context between requests. Each request contains all necessary information.
![[rest-triangle.png\|500]]
**(REPRESENTATIONAL STATE TRANSFER)** The name derives from how clients interact with the system: a client retrieves a **representation** (e.g., HTML, JSON, XML) of a resource, which places the client in an application state. By following hyperlinks (URIs) within that representation, the client initiates a new request, transferring to a new state.
![[rest-boeing-ex.png\|500]]
**(PRACTICAL EXAMPLE)** Consider a parts depot web service.
*   **GET list of parts:** `GET http://www.parts-depot.com/parts` returns an XML list with hyperlinks to each part.
*   **GET part details:** `GET http://www.parts-depot.com/parts/00345` returns detailed XML for part 00345, which may contain further links (e.g., to its specification).
*   **Submit a Purchase Order (PO):** `POST http://www.parts-depot.com/orders` with an XML PO document in the request body creates a new order.
![[rest-purchase-order-ex.png\|500]]
![[web-service-pruchase-order.png\|500]]
### Claimed benefits
Proponents of REST highlight several advantages:
*   **Improved Performance:** Native support for HTTP caching improves response times.
*   **Scalability:** Statelessness allows requests to be handled by any server, simplifying scaling.
*   **Simplicity & Interoperability:** Relies on well-understood, ubiquitous web standards (HTTP, URI), reducing vendor lock-in.
*   **Discoverability:** Hyperlinks embedded in representations guide clients through the API, acting as a built-in discovery mechanism.
### HTTP
HTTP is the primary protocol for RESTful interactions. Understanding its mechanics is crucial.

**(ANATOMY OF HTTP REQUEST)** A client sends a request with:
*   **Request Line:** `<VERB> <URI> HTTP/<version>` (e.g., `GET /parts/00345 HTTP/1.1`).
*   **Headers:** Metadata (e.g., `Content-Type: application/json`, `Authorization: Bearer ...`).
*   **Body (optional):** The payload data (e.g., JSON or XML for POST/PUT).
![[anatomy-of-http-req.png\|500]]
![[get-post-req-ex.png\|500]]
**(ANATOMY OF HTTP RESPONSE)** The server replies with:
*   **Status Line:** `HTTP/<version> <Status Code> <Reason>` (e.g., `HTTP/1.1 200 OK`).
*   **Headers:** Metadata about the response.
*   **Body (optional):** The requested representation (e.g., HTML, JSON).
![[anatomy-of-http-resp.png\|500]]
**(HTTP STATUS CODES)** Key categories include:
*   **2xx Success:** `200 OK` (request succeeded), `201 Created` (resource created).
*   **3xx Redirection:** `301 Moved Permanently`.
*   **4xx Client Error:** `400 Bad Request`, `401 Unauthorized`, `404 Not Found`.
*   **5xx Server Error:** `500 Internal Server Error`.
**(ADDITIONAL HTTP VERBS)** Beyond CRUD, HTTP defines other verbs like `HEAD` (get headers only), `PATCH` (partial update), and `OPTIONS` (discover allowed operations).
**Check of Understanding**
> **Question:** You are designing a web API for a university's course registration system. You need operations to: a) retrieve a list of all courses for a term, b) allow a student to enroll in a specific course, and c) allow an administrator to update a course's maximum capacity. Design the RESTful endpoints (URIs and HTTP methods) for these operations, justifying your choices.
> **Answer:**
> a) **Retrieve course list:** `GET /terms/{termId}/courses`
>    *   Justification: `GET` is for safe retrieval. The URI is hierarchical, listing courses as sub-resources of a specific term.
> b) **Student enrollment:** `POST /courses/{courseId}/enrollments`
>    *   Justification: `POST` is for creating a new subordinate resource. The enrollment is created as a new sub-resource under the specific course. The student's ID would be in the request body.
> c) **Update course capacity:** `PUT /courses/{courseId}`
>    *   Justification: `PUT` is for complete replacement/update of a resource. The request body would contain the full or partial updated course representation, including the new `maxCapacity` field. Alternatively, `PATCH` could be used for a partial update.
## 7. Programming RESTful Web Services
### REST: Quick Recap
*   **Architectural Style:** Built on web standards (HTTP, URI).
*   **Core Concepts:** Everything is a **resource** identified by a **URI**. Resources are manipulated via a **uniform interface** of HTTP methods (GET, POST, PUT, DELETE).
*   **Client-Server:** REST client accesses/modifies resources on a REST server.
*   **Multiple Representations:** A single resource can have different representations (JSON, XML, text), requested via HTTP content negotiation.
*   **Stateless:** Each request contains all necessary context.
### REST APIs: Examples
Major platforms provide RESTful APIs for programmatic access:
*   **Google APIs:** Custom Search, Maps, YouTube (e.g., `GET https://www.googleapis.com/customsearch/v1?key=API_KEY&q=query`).
*   **X (Twitter) API:** For reading and writing tweet data.
*   **Amazon Web Services (AWS):** Many services like Amazon S3 expose RESTful endpoints.
*   **OpenAI API:** For accessing AI models like GPT via HTTP requests.
### Reference implementations:
#### Python (Flask Restful)
**Flask-RESTful** is a lightweight Python extension for Flask that simplifies building REST APIs.
**(BASIC EXAMPLE)** A minimal API that returns JSON.
```python
from flask import Flask
from flask_restful import Resource, Api
app = Flask(__name__)
api = Api(app)
class HelloWorld(Resource):
    def get(self):
        return {'hello': 'world'}
api.add_resource(HelloWorld, '/')
if __name__ == '__main__':
    app.run(debug=True)
```
Running this (`python api.py`) and calling `curl http://127.0.0.1:5000/` returns `{"hello": "world"}`.
**(CRUD RESOURCE EXAMPLE)** A simple in-memory "todo" API demonstrating HTTP methods.
```python
from flask_restful import Resource, reqparse
todos = {}
class TodoSimple(Resource):
    def get(self, todo_id):
        return {todo_id: todos[todo_id]} # GET to retrieve
    def put(self, todo_id):
        todos[todo_id] = request.form['data'] # PUT to create/update
        return {todo_id: todos[todo_id]}
api.add_resource(TodoSimple, '/<string:todo_id>')
```
*   **`PUT /todo1 -d "data=Remember the milk"`** creates/updates the item.
*   **`GET /todo1`** retrieves it.
#### Java (Jersey)
**Jersey** is the reference implementation for **JAX-RS (Java API for RESTful Web Services)**, a Java specification. It uses annotations to map Java classes to REST resources.
**(JAX-RS ANNOTATIONS)** Key annotations for defining REST endpoints:

| Annotation | Description |
| :--- | :--- |
| `@Path("/path")` | Defines the URI path for the resource class or method. |
| `@GET`, `@POST`, `@PUT`, `@DELETE` | Specifies the HTTP method the method responds to. |
| `@Produces(MediaType.TEXT_PLAIN)` | Declares the MIME type(s) the method returns (e.g., `"application/json"`). |
| `@Consumes(MediaType.APPLICATION_JSON)` | Declares the MIME type(s) the method accepts in the request body. |
| `@PathParam("id")` | Binds a URI path segment to a method parameter. |

**(EXAMPLE: SIMPLE CALCULATOR)** A REST service exposing `add` and `subtract` operations.
**1. Create the RESTful Web Service:**
```java
import javax.ws.rs.*;
@Path("/calc") // Base path for this resource
public class CalcREST {
    // Handles GET /calc/add/{a}/{b} and returns plain text
    @GET @Path("/add/{a}/{b}")
    @Produces(MediaType.TEXT_PLAIN)
    public String addPlainText(@PathParam("a") double a, 
                               @PathParam("b") double b) {
        return (a + b) + "";
    }
    // Handles same path but returns XML if client accepts it
    @GET @Path("/add/{a}/{b}")
    @Produces(MediaType.TEXT_XML)
    public String addXML(@PathParam("a") double a, 
                         @PathParam("b") double b) {
        return "<?xml version=\"1.0\"?><result>" + (a + b) + "</result>";
    }
    // Similar method for @Path("/sub/{a}/{b}")
}
```
**2. Publish the Service:** A main class to start an embedded HTTP server.
```java
public class CalcRESTStartUp {
    static final String BASE_URI = "http://localhost:9999/calcrest/";
    public static void main(String[] args) throws IOException {
        HttpServer server = HttpServerFactory.create(BASE_URI);
        server.start();
        System.out.println("Server running. Press Enter to stop.");
        System.in.read();
        server.stop(0);
    }
}
```
**3. Create a Client:** Java code to consume the service.
```java
Client client = Client.create();
WebResource addResource = client.resource("http://localhost:9999/calcrest")
                                 .path("calc/add/10/5");
// Request plain text
String textResult = addResource.accept(MediaType.TEXT_PLAIN).get(String.class);
// Request XML
String xmlResult = addResource.accept(MediaType.TEXT_XML).get(String.class);
```
**(OTHER FRAMEWORKS)** Other popular implementations include:
*   **Django REST Framework (Python):** A powerful, feature-rich toolkit for building Web APIs in Django.
*   **RESTEasy (Java):** A JBoss project, another full JAX-RS implementation.
### Data encoding and RPC
While REST over HTTP/JSON is dominant, other efficient data encoding formats are used, particularly in **RPC (Remote Procedure Call)** frameworks:
![[data-encoding-rpe-ex.png\|500]]
*   **Protocol Buffers (protobuf):** Google's language-neutral, platform-neutral mechanism for serializing structured data. It is smaller and faster than XML/JSON. Used extensively in **gRPC**, a modern high-performance RPC framework.
*   **Apache Thrift:** A scalable cross-language service development framework, combining a software stack with a code generation engine.
*   **Apache Avro:** A data serialization system providing rich data structures, a compact binary format, and direct code generation.
*   **Gson:** A Java library from Google to convert Java Objects into JSON and vice-versa.

**Check of Understanding**
> **Question:** You need to build a high-throughput internal microservice for real-time financial trade matching. The service requires very low latency, strict interface contracts, and must support streaming updates. Would you choose a REST/JSON approach using Flask or Jersey, or a gRPC/protobuf approach? Justify your choice based on the technical characteristics discussed.
> **Answer:** Choose **gRPC/protobuf**.
> *   **Performance & Latency:** gRPC uses **HTTP/2** (multiplexing, header compression) and **Protocol Buffers** (binary, efficient serialization), leading to much lower latency and higher throughput than REST/JSON over HTTP/1.1, which is critical for real-time trading.
> *   **Interface Contracts:** Protobuf `.proto` files provide strict, forward/backward compatible interface definitions and enable automatic, type-safe client/server code generation. REST/JSON interfaces are looser and require manual validation.
> *   **Streaming:** gRPC has first-class support for **bidirectional streaming**, perfect for pushing continuous trade updates. REST would require inefficient polling or bespoke solutions like WebSockets.
> While Flask/Jersey REST is excellent for public, web-centric APIs, gRPC/protobuf is optimized for high-performance, contract-first internal service communication.
## 8. Microservices, Nanoservices and Serverless
### Recap: SOAs
SOAs established the foundational principles of **decentralised middleware**, a **strong emphasis on loose coupling** between components, and **adherence to well-supported standards** for cross-system communication.
### Microservices
Microservices are a specific, fine-grained implementation of the SOA pattern. An application is decomposed into a suite of **small, independent, and loosely coupled services**. Each service runs in its own process, communicates via lightweight mechanisms (often HTTP/REST or messaging), and is built around a specific business capability.
![[microservices-ex.png\|500]]
**(KEY FEATURES)**
*   **Specified Functionality:** Each service has a single, well-defined responsibility.
*   **Independent Deployment:** Services can be developed, tested, and deployed in isolation, enabling continuous delivery.
*   **Organised by Business Domain:** Services are structured around business processes (e.g., "User Service," "Order Service," "Payment Service").
**(ARCHITECTURAL SHIFT)** This represents a move from a monolithic architecture, where all functionality is bundled together, to a distributed system of collaborating services.
![[monolith-to-microservices.png\|500]]
**(CONTAINERS & MICROSERVICES)** Containers (e.g., Docker) are the ideal deployment vehicle for microservices. They package an application with all its dependencies into a single, lightweight, portable artifact. This provides the isolation and environment consistency that microservices require, without the overhead of a full virtual machine.
![[container-arch-ex.png\|200]]![[vm-arch-ex.png\|200]]
![[apps-and-dependencies-deployed-as-artifact.png.png\|500]]
### Nanoservices
Nanoservices take the microservices concept to an extreme level of granularity. A nanoservice is a **single-function service**—often just a few lines of code—that performs one very specific task. While microservices are "small," nanoservices are "tiny." This extreme decomposition can lead to massive operational complexity (managing thousands of services) and is generally considered an anti-pattern unless there is a specific, compelling need for such fine granularity.
### Serverless Computing
Serverless computing is a cloud execution model where the cloud provider dynamically manages the allocation and provisioning of servers. The developer writes **code (functions)** without any concern for the underlying infrastructure. The core principle is **"No Servers to Manage."**
### Function as a Service (FaaS)
FaaS is the specific implementation paradigm at the heart of serverless computing. It allows developers to deploy **individual functions** (blocks of code) that are executed in response to events. The platform automatically scales, runs, and bills for these functions down to the millisecond of execution time.
**(EXECUTION MODEL)** When an event triggers a function, the FaaS platform checks if an instance is already running ("warm"). If not, it performs a **cold start**: loading the function code, provisioning a runtime container, and then executing it. Subsequent invocations may reuse the warm instance for much lower latency.
![[serverless-execution-model-ex.png\|500]]
### Architectural Support
Serverless architectures are inherently **event-driven**. Applications are composed of functions triggered by events from various sources (HTTP requests, database changes, message queues, timers).
![[serverless-apps-logic-flow.png\|500]]
**(KEY METRICS & CHALLENGES)**
1.  **Cold Start Latency:** The delay when instantiating a new function instance. This overhead is **not negligible** and is a critical performance consideration.
    ![[cold-vs-warm-serverless-app.png\|500]]
2.  **Statelessness:** Functions are inherently stateless. Any required state (user session, intermediate data) must be stored in external services (databases, caches), adding complexity and latency.
3.  **Function Composition:** Building complex applications requires orchestrating multiple functions, which can be done through sequential chaining, event-driven flows, or dedicated orchestration services.
**(EVOLUTION OF GRANULARITY)** The trend moves from large monoliths, to decomposed microservices, and further to ephemeral, event-triggered functions.
![[monolith-to-microservices-to-nanoservices.png\|500]]
### Solutions
**(COMMERCIAL & OPEN SOURCE FaaS PLATFORMS)**
*   **AWS Lambda:** The pioneer. Functions are triggered by events from almost any AWS service (S3, API Gateway, DynamoDB) or custom HTTP endpoints.
    ![[aws-lambda-ex.png\|500]]
![[aws-lambda-arch-ex.png\|500]]
*   **Microsoft Azure Functions:** A polyglot platform supporting multiple languages. It integrates deeply with Azure services and offers easy deployment from source control.
    ![[azure-functions-steps-ex.png\|500]]
*   **Google Cloud Functions, IBM Cloud Code Engine, Apache OpenWhisk, Knative:** Other major platforms providing similar FaaS capabilities, often with unique strengths in open-source integration or Kubernetes-native operation.

**(BENEFITS)**
*   **No Server Management:** Eliminates operational overhead.
*   **Continuous, Automatic Scaling:** Scales from zero to thousands of instances seamlessly.
*   **Pay-Per-Use Cost Model:** You only pay for the compute time your functions consume, never for idle resources.

**Check of Understanding**
> **Question:** Your team is designing a new photo-sharing application. A core feature requires generating multiple thumbnail sizes whenever a user uploads an image. Should this feature be implemented as part of a monolithic "Upload Service," as a standalone microservice, or as a serverless function? Justify your choice based on architectural concepts.
> **Answer:** The thumbnail generation is an ideal candidate for a **serverless function (FaaS)**. Justification:
> *   **Event-Driven:** It is triggered by a specific, infrequent event (image upload).
> *   **Variable, Bursty Workload:** Upload frequency is unpredictable. Serverless scales to zero when idle (cost-effective) and instantly handles sudden spikes (e.g., many users uploading at once).
> *   **Short-Running, Stateless Task:** The job is compute-intensive but short-lived. It reads from object storage (the original image) and writes back to object storage (the thumbnails), requiring no persistent internal state.
> *   **Decoupled & Focused:** Implementing it as a standalone function keeps it completely decoupled from the main upload logic, adhering to the microservices principle of single responsibility, but with the operational simplicity of serverless. A full microservice would require managing a constantly running container for a sporadic task, which is less efficient.
## 9. Naming Pt.1 - *Chapter 5*
### Fundamental concepts
**(DEFINITIONS)** In a distributed system, an **entity** (resource like a file, service, or host) must have a **name** to be accessed. A name is a string (e.g., `server42`, `/home/file.txt`). To locate and interact with an entity, its name must be **bound** to its attributes (e.g., IP address, port). The process of translating a name into these attributes is called **name resolution**.
![[name-res-ex.png\|500]]
**(ACCESS POINTS AND ADDRESSES)** An **access point** is a special entity used to operate on another entity. Its name is an **address** (e.g., `192.168.1.5:8080`). An entity can have multiple or changing access points. For flexibility, we prefer **location-independent names** (stable identifiers) rather than directly using addresses.
**(FUNCTION OF A NAMING SYSTEM)** A naming system manages **bindings** between names and entity attributes. Its primary function is **name resolution**. Secondary functions include creating/deleting bindings, listing names, and organising the **namespace** (the set of all valid names).
### Classes of naming systems
Three broad classes exist:
1.  **Flat Naming:** Uses unstructured identifiers (IDs). This lecture's focus.
2.  **Structured Naming:** Uses human-readable, hierarchical names (e.g., file paths, URLs). (Covered in Pt.2).
3.  **Attribute-Based Naming:** Entities are described/searchable by attributes (e.g., "printer, colour, floor 3"). (Covered in Pt.2).
### Namespaces
This section focuses on **flat naming systems**, where entities have non-hierarchical, often meaningless identifiers (IDs). The key challenge is **locating an entity when given only its flat ID**.

**(BROADCASTING)** One simple method is to **broadcast** a query asking "who has this ID?" across the local network.
*   *Example: Address Resolution Protocol (ARP)* resolves an IP address to a MAC address by broadcasting "Who has IP `192.168.1.10`?" on the local LAN.
![[arp-pro-ex.png\|500]]
*   **Drawback:** Does **not scale** beyond local networks; generates excessive traffic in large systems.

**(FORWARDING POINTERS)** When an entity moves, it leaves a **forwarding pointer** at its old location pointing to the new one. To locate it, a client follows the chain of pointers.
![[naming-pt1-ssp-chains-ex.png\|500]]
*   *Example in RMI:* A **client stub** can hold a forwarding pointer to the **server stub** (a **Stub-Scion Pair** or SSP). Shortcuts can be stored to improve future lookups.
![[ssp-chains-a-and-b.png\|500]]
*   **Drawbacks:** Long chains are **not fault-tolerant** (a broken pointer breaks resolution) and increase **latency**.

**(HOME-BASED APPROACH)** A central **home location** maintains the current address of a mobile entity.
*   *Example: Mobile IP.* A mobile host has a permanent **home address**. A **home agent** on its home network tracks its current **care-of-address**. Packets sent to the home address are **tunnelled** by the home agent to the current location.
![[mobile-ip-world-map-ex.png\|500]]
*   **Drawbacks:** The fixed home can become a **single point of failure** and a **performance bottleneck**, especially if the entity and client are far from the home (poor **geographical scalability**).
### Naming graphs
This metaphor describes structures for organising and resolving names on a larger scale.

**(DISTRIBUTED HASH TABLES - DHTs)** A DHT is a decentralised, structured overlay network that provides a hash-table-like interface: `put(key, value)` and `get(key)`. Entities (values) are assigned a unique **key** (via hashing). Responsibility for storing the `(key, address)` binding is distributed across all nodes in the network based on the key's value.
![[distributed-hash-table-principle.png\|500]]
![[dht-ex.png\|500]]
*   *Example:* In a Chord DHT, nodes are arranged in a logical ring. An entity with key `k` is stored on the node with the smallest ID ≥ `k` (its **successor**). Lookups are routed efficiently around the ring in `O(log N)` hops.
*   **Advantages:** Highly **scalable** and **decentralised**; no single point of failure.

**(HIERARCHICAL APPROACHES)** This method organises directory nodes into a **tree** that mirrors the network's hierarchical structure (e.g., countries, organisations, departments). Directories store location information for entities within their subtree.
![[hierarchical-tree-ex.png\|500]]
*   **(LOOKUP OPERATION)** To find an entity, start at the local leaf node. If it doesn't know the location, the request moves **up** the tree until a directory that knows about the entity is found (ultimately, the root knows all). The request then moves **down** via pointers to the leaf node holding the address.
![[lookup-operation.png\|500]]
*   **(INSERT OPERATION)** To insert a new entity address, the request moves up the tree until finding a node that already knows about the entity. A chain of **forwarding pointers** is then created back down to the leaf node storing the new address.
![[insert-operation-a-b.png\|500]]
*   **Trade-off:** More structured and scalable than broadcasting or pure home-based, but insert/lookup operations involve multiple network hops.

**Check of Understanding**
> **Question:** You are designing a peer-to-peer file-sharing system where millions of nodes join and leave frequently, and files (entities) are referenced by a unique hash of their content (a flat name). Which flat naming resolution scheme discussed would be most suitable and why? What is a key operational challenge you would need to manage?
> **Answer:** A **Distributed Hash Table (DHT)** would be most suitable.
> **Why:** The unique file hash serves as a perfect **key**. A DHT efficiently and **decentralises** the responsibility of storing the binding between this key and the current node(s) storing the file. It scales to millions of nodes and handles churn (nodes joining/leaving) gracefully through its structured overlay and replication mechanisms.
> **Key Challenge:** **Maintaining the DHT's consistency and routing tables** in the face of high churn. As nodes frequently join and leave, the system must efficiently update successor pointers and replicate data to ensure `get(key)` operations remain reliable and efficient.
## 10. Naming Pt.2 - *Chapter 5*
### Structured naming
Structured names are human-readable and often hierarchical (e.g., `/home/user/file.txt`, `www.example.com`). They are composed from simple names and organised within a **namespace**.
#### Namespaces
A namespace is the set of all valid names recognised by a system. It can be represented as a **directed graph** with two node types:
*   **Leaf Nodes:** Represent named entities (files, hosts, services). They store the entity's attributes or state.
*   **Directory Nodes:** Act as catalogs. They store a **directory table** of (`edge-label`, `node-identifier`) pairs, defining the outgoing edges.
Each path through the graph, identified by a sequence of labels (e.g., `L1, L2, ... Ln`), is a structured name.
![[naming-file-sys-ex.png\|500]]
*Example:* A filesystem namespace. A single root node is common.
![[single-root-naming-ex.png\|500]]

#### Name resolution
Name resolution is the process of traversing the naming graph to translate a name into the entity it refers to. The starting point is defined by a **closure mechanism**.
**(CLOSURE MECHANISM)** This is the rule or context that determines the **initial node** (index node) for resolution.
*   *Example (Unix Filesystem):* To resolve `/home/steen/mbox`, the closure mechanism provides access to the root directory's inode. The OS finds this via the disk's superblock.
![[closure-mechanism-unix-ex.png\|500]]
*   *Other Examples:* `www.distributed-systems.net` starts resolution at a known DNS server; `0031 20 598 7784` starts at the local telephone exchange.
**(PATHNAME RESOLUTION)** The process of walking the graph. In a Unix filesystem, every file/directory is an **inode**. A directory is a file mapping names to inode numbers. Resolution walks this chain.
![[inode-ex.png\|500]]
![[path-name-resolution-ex.png\|500]]

**(LINKING)** Creates **aliases**, allowing multiple names for one entity.
*   **Hard Link:** A direct directory entry pointing to the same inode. Multiple absolute paths refer to the same node.
*   **Symbolic (Soft) Link:** A special leaf node that stores the *pathname* of the target entity as data.
![[linking-unix-ex.png\|500]]
**(MOUNTING)** A key mechanism for **integrating different namespaces** transparently. It associates a node in the current namespace (the **mount point**) with the root (**mounting point**) of a **foreign namespace** (e.g., a remote filesystem).
![[mounting-sys-ex.png\|500]]
*Example:* Mounting a remote NFS export `/remote/vu` to the local directory `/home/steen/remote` allows accessing files via `/home/steen/remote/mbox`.

#### Implementation of a namespace
For large-scale, distributed namespaces (like DNS), the naming graph is distributed across many **name servers**. The namespace is partitioned into logical layers:
1.  **Global Layer:** Stable, high-level directory nodes (e.g., DNS root and top-level domains `.com`, `.uk`). Managed by different administrations.
2.  **Administrational Layer:** Mid-level directories for organisations (e.g., `example.com`). Each zone is managed independently.
3.  **Managerial Layer:** Low-level nodes within a single administration (e.g., `www.example.com`, `mail.example.com`). Often replicated for robustness and performance.
![[dns-partitioning-ex.png\|500]]
![[namespace-implementation-ex.png\|500]]

**(DOMAIN NAME SYSTEM - DNS)** DNS is the canonical distributed, hierarchical naming system for the internet.
*   **Zone Data:** Authoritative data for a domain portion, including records like **A** (address), **NS** (name server), **MX** (mail exchange), and **CNAME** (canonical name alias). Data is cached with a **Time-To-Live (TTL)**.
![[dns-node-info.png\|500]]
*   **Resolution Methods:**
    *   **Iterative Resolution:** The contacted server returns the **next server** to ask. The client does the legwork.
![[dns-iterative.png\|500]]
    *   **Recursive Resolution:** The contacted server **forwards the query** itself, eventually returning the final answer to the client.
![[recursive-ex.png\|500]]
*   **Trade-offs:** Recursive resolution places more load on name servers but enables more effective caching at intermediate servers. Iterative resolution is less demanding on individual servers.
![[scalability-issues.png\|500]]

### Attribute-based naming
Also known as **directory services**. Instead of looking up an entity by its precise name, you search for it using a set of **descriptive attributes**. This is like using a "yellow pages" versus a "white pages" directory.

**(LDAP - LIGHTWEIGHT DIRECTORY ACCESS PROTOCOL)** A standard protocol for directory services. Data is organised in a **Directory Information Tree (DIT)**, where each entry is a collection of (`attribute`, `value`) pairs and is uniquely named by a **Relative Distinguished Name (RDN)**.
![[ldap-ex.png\|500]]
![[dir-info-tree-ex.png\|500]]
*   *Example Search:* A query like `(C=UK)(O=Leeds University)(OU=Computing)(CN=Main Server)` would return all directory entries matching those attributes, potentially revealing multiple host servers.
*   *Example Table:* Two entries distinguished by their `HostName` RDN.

**(JAVA NAMING AND DIRECTORY INTERFACE - JNDI)** A Java API that provides a uniform interface to multiple different naming and directory services (LDAP, DNS, CORBA, RMI registry) through pluggable **service providers**.
![[jndi-arch-ex.png\|500]]
It allows applications to bind objects to names and perform lookups and searches independent of the underlying service implementation.

**Check of Understanding**
> **Question:** Your distributed application needs to find an available, colour, A3-capable printer located on the same floor as the user. The system knows the user's current floor and the printers are registered with various attributes (type, capabilities, location). Would you use DNS, the local filesystem namespace, or an LDAP-based directory service to implement this discovery? Justify your choice.
> **Answer:** You would use an **LDAP-based directory service**.
> *   **DNS** is designed for **structured name resolution** (hostname to IP address). It is poorly suited for searching by arbitrary, multi-valued attributes like "colour-capable" and "A3".
> *   The **filesystem namespace** is also for structured, hierarchical name lookup, not attribute-based querying.
> *   An **LDAP directory** is specifically built for **attribute-based naming**. Each printer could be an entry with attributes like `cn=Printer42`, `type=laser`, `capabilities=colour,A3,duplex`, `locationFloor=5`. The application can then perform a search query like `(&(type=laser)(capabilities=colour)(capabilities=A3)(locationFloor=5))` to find all matching printers. This is the exact functionality directory services provide.### Directory Services
# Remaining
## 11. Timing and Synchronisation - *Chapter 6*
### Synchronisation in a DS
In distributed systems, there is **no global agreement on time**. Each machine's internal clock (typically a **quartz crystal oscillator**) drifts at a unique rate, measured in **parts per million (ppm)**. This **clock skew** means an event that occurred later can be assigned an earlier timestamp, causing issues for logs, schedules, failure detection, and event ordering.
![[timing-clock-ex.png\|500]]
### Internal and external physical clocks
**(INTERNAL CLOCKS)** A computer's internal clock counts oscillations of a quartz crystal, triggering interrupts ("ticks") to maintain a software clock. The drift rate is bounded by a **maximum drift rate (ρ)**, meaning $1-ρ ≤ dC/dt ≤ 1+ρ$.

**(EXTERNAL STANDARDS & UTC)** The worldwide standard is **UTC (Coordinated Universal Time)**, based on atomic clocks (TAI) with leap seconds added to align with solar time. Machines need to synchronise their internal clocks with an external source like UTC.
![[utc-time-standard.png\|500]]
**(TIME REPRESENTATION)** Common formats are **Unix time** (seconds since epoch) and **ISO 8601**. A critical bug related to handling a **leap second** on 30 June 2012 caused widespread crashes, highlighting the importance of robust timekeeping.
### Clock synchronisation algorithms
Algorithms aim to minimise skew between clocks. If one machine has a UTC receiver, it acts as a **time server**.
#### Drift as a Function of UTC
• If two clocks are drifting from UTC in opposite directions, at a time dt after synchronisation they may be as much as 2$\rho$dt apart.
![[drift-as-func-utc.png\|500]]
25/02/1991. Patriot Missile Failure: Software error in the system clock. Accumulated clock drift.
**(CRISTIAN'S ALGORITHM)** A client requests the time from a server. It estimates the network delay and adjusts its clock to `server_time + (round_trip_delay / 2)`. It must apply the correction gradually to avoid time running backward.
![[christian-algo.png\|500]]
![[est-time-over-network.png\|500]]
**Round-trip network delay** $\delta = (t_4 - t_1) - (t_3 - t_2)$
**Estimated server time** when client receives response: $t_3 + \delta/2$
**Estimated clock skew**: $\delta = t_3 + \delta/2 – t_4 = (t_2 - t_1 + t_3 - t_4)/2$

**(BERKELEY ALGORITHM)** Used when no UTC source is available. A **time daemon** (coordinator) polls all machines for their time, calculates the **average** (ignoring outliers), and instructs each machine on how much to adjust its clock (forward or backward).
![[berkley-algo-time.png\|500]]
1. Time daemon clock $D$ shows **3:00**, requests network clock values.
2. Network clocks $C_n$, return their values to the daemon $C_1 =$ 2:50, $C_2 =$ 3:25.
3. Daemon calculates average: $$ \begin{align}
( \ \text{diff}(D, C_1) + \text{diff}(D, C_2)\ ) / \text{no. clocks} \\
( \ \text{diff}(\text{3:00}, \text{ 3:50}) + \text{diff}(\text{3:00}, \text{ 3:25})\ ) / 3 \\
( 0 -10 + 25 ) / 3 = +5 \text{ minutes}\\
\end{align}$$ 4. Daemon adjusts all clocks to match its own (3:00) + 5 more minutes= 3:05.
### Election Algorithm
When a coordinator (e.g., the time daemon in Berkeley) fails, an **election algorithm** selects a new leader.
**(BULLY ALGORITHM)** The process with the highest ID wins.
1.  Any process noticing coordinator failure sends **ELECTION** messages to processes with higher IDs.
2.  If it gets no **OK** response, it wins and announces **I WON** to lower-ID processes.
3.  If it gets an **OK**, it waits for the **I WON** message.
4.  A process receiving an **ELECTION** message replies **OK** and starts its own election.
![[bully-algo-ex.png\|500]]
![[bully-algo-ex-pt-2.png\|500]]
It uses `O(n²)` messages.
**a)** Process 7 has crashed and Process 4 holds an election 
**b)** Process 5 and 6 respond, telling 4 to stop
**c)** Now 5 and 6 hold elections **d)** Process 6 tells 5 to stop **e)** Process 6 wins and tells everyone.
If P7 is restarted, it will send all the others a COORDINATOR msg and bully them into submission.

**(RING ALGORITHM)** Processes are arranged in a logical ring. A process starting an election sends an **ELECTION** message with its ID to its successor. Each successor adds its own ID and forwards the message. When the message returns to the initiator, it circulates a **COORDINATOR** message with the highest ID as the new leader.
![[ring-algo.png\|500]]
Election algorithm using a ring. The solid line shows the election messages initiated by P6; the dashed one those by P3. We can see what happens if two processes, P3 and P6, discover simultaneously that the previous coordinator, process P7, has crashed. Each of these builds an ELECTION msg and each of them starts circulating its message, independent of the other one. 
	Eventually, both messages will go all the way around, and both P3 and P6 will convert them 
into COORDINATOR messages, with exactly the same members and in the same order. When both have gone around again, both will be removed. It does no harm to have extra messages circulating; at worst it consumes a little bandwidth, but this is not considered wasteful.
### Network time protocol (NTP)
**NTP** is the hierarchical, robust protocol used to synchronise clocks across the Internet to UTC.
**(STRUCTURE)** Servers are organised in **strata**.
*   **Stratum 1:** Primary servers directly connected to UTC sources (e.g., atomic clocks, GPS).
*   **Stratum 2:** Synchronise with stratum 1 servers.
*   **Stratum 3+ and clients:** Synchronise with higher-stratum servers. Accuracy decreases with stratum number.

**(CLOCK CORRECTION)** NTP calculates the offset (θ) between client and server.
Systems that rely on clock sync need to monitor clock skew!
*   **|θ| < 125 ms:** **Slew** the clock (adjust speed gradually).
*   **125 ms ≤ |θ| < 1000 ms:** **Step** the clock (jump to the correct time).
*   **|θ| ≥ 1000 ms:** **Panic** (do nothing, require manual intervention).

**Check of Understanding**
> **Question:** A distributed sensor network for a scientific experiment logs each reading with a local timestamp. During analysis, you notice events from Sensor A always appear 50ms before equivalent events from Sensor B, but you suspect they are simultaneous. You know both sensors synchronise via NTP to the same stratum-2 server. List three distinct potential causes for this consistent 50ms offset in the timestamps.
> **Answer:**
> 1.  **Asymmetric Network Delay:** The most likely cause. NTP assumes symmetric network paths. If the path from the NTP server to Sensor A has a different latency than the path to Sensor B (e.g., due to different network hops or congestion), the calculated clock offset will be inaccurate. A consistent 50ms difference suggests a fixed routing asymmetry.
> 2.  **Different NTP Strata or Servers:** If Sensor B is synchronising indirectly (e.g., getting time from Sensor A as a stratum-3 source) while Sensor A syncs directly with the stratum-2 server, Sensor B's time would have an extra hop of latency and potential error added.
> 3.  **Local Processing Delay on Sensor B:** The timestamp might be applied *after* some internal processing or queueing on Sensor B, adding a constant delay. This is a software/architectural issue, not a clock synchronisation error per se, but results in the same observable offset.
## 12. Consistency and Replication Pt.1 - *Chapter 7*
### Introduction to the problem
A fundamental issue in distributed systems is **replication**: maintaining multiple copies of the same data (e.g., a file, database record) across different machines.
![[replica-db-ex.png\|500]]
**(REASONS FOR REPLICATION)**
*   **Reliability:** If one replica crashes, the system can continue using another, increasing fault tolerance.
*   **Performance & Scalability:**
    *   **Scaling for Size:** Distributes client load across multiple servers.
    *   **Scaling Geographically:** Places data closer to users, reducing access latency (e.g., a content delivery network or CDN).

**(THE CONSISTENCY PROBLEM)** The core challenge is **keeping replicas consistent**. When one copy is updated, all other copies must eventually reflect that change. If updates aren't propagated correctly, replicas diverge, leading to stale data and incorrect results.

**(THE PERFORMANCE-SCALE DILEMMA)** Strict consistency (synchronous replication) requires that all replicas agree on the order of **conflicting operations** (Read-Write or Write-Write conflicts). Achieving this global agreement requires **synchronisation**, which is slow, communication-intensive, and hurts scalability.
*Solution:* To build scalable systems, we often **relax the consistency requirements**, accepting that replicas may be temporarily inconsistent. The specific rules for what is acceptable are defined by **consistency models**.
### Data-centric consistency models
A **consistency model** is a formal contract between a distributed data store and its client processes. It defines the possible results of **read** and **write** operations when they are executed concurrently by multiple processes, specifying what values a read operation is allowed to return.
![[data-store-ex.png\|500]]

#### Continuous Consistency
This model quantifies inconsistency, allowing applications to specify *how much* inconsistency they can tolerate, rather than requiring absolute consistency. A replica can deviate from others in three dimensions:
1.  **Numerical Deviation:** The absolute difference in value (e.g., a bank account balance on replica A is £1000, on replica B it's £950, a deviation of £50).
2.  **Staleness Deviation:** The maximum time a replica's value is older than another's.
3.  **Ordering Deviation:** The number of pending (uncommitted/not-yet-propagated) update operations.

**(CONSISTENCY UNIT - CONIT)** The data unit over which these deviations are measured is called a **Conit**. An application defines a Conit, and the system ensures deviations stay within specified bounds.

**(CONIT EXAMPLE)** A fleet manager tracks average fuel cost. The Conit consists of three variables: gallons tanked (`g`), price paid (`p`), and distance driven (`d`). These are replicated.
![[conit-replicas.png\|500]]
*   **Vector Clocks** track known operations from each replica.
*   **Order Deviation (3 for A):** A has 3 local, tentative operations pending permanent commitment.
*   **Numerical Deviation (2,482 for A):** The sum of the values from operations A has *missed* from B (e.g., two missed updates totalling £70 + £412).
*   An application could specify: "Never let the numerical deviation for the 'fuel cost' Conit exceed £1,000." The system would then trigger synchronisation before this bound is breached.
#### Sequential Consistency
This is a stricter model. The result of any execution must be equivalent to the results of some **sequential execution** of all processes' operations, where the operations *of each individual process* appear in this sequence in the **order specified by its program** (program order).

**(DEFINITION & DIAGRAM)** It does *not* require operations from different processes to happen in real-time order, only that all processes agree on a single, global sequential order of *all* operations.
![[seq-consistency.png\|500]]
In Diagram (a): P2 reads `x` as `NIL`, then later reads `a`. This is sequentially consistent because we can construct a valid sequential order: `W1(x)a` -> `R2(x)NIL` -> `R2(x)a`.

**(THE ROLE OF TIME)** Sequential consistency is about logical order, not physical time.
![[seq-consistency-no-time.png\|500]]
*   **Diagram (a) is Valid:** P3 and P4 *both* see `W2(x)b` happening before `W1(x)a`. They agree on the interleaving.
*   **Diagram (b) is a Violation:** P3 and P4 see *different* interleavings of the writes. There is no single sequential order that satisfies both views.
**(EXAMPLE WITH PROGRAMS)**
![[seq-print-ex.png\|500]]
For three processes with the above code, sequential consistency does *not* guarantee that all prints will show `1,1,1`. It only guarantees that the execution is equivalent to *some* sequential interleaving of the statements that respects each process's program order. Many interleavings are possible, leading to various `(x,y,z)` print outputs (like `1,0,0`), all of which are valid under this model.

**(THE HAPPENS-BEFORE RELATION)** This is a foundational concept for defining order in distributed systems without a global clock.
*   **Definition:** Event `a` **happens-before** event `b` (`a → b`) if:
    1.  `a` and `b` are on the same process and `a` occurs before `b`.
    2.  `a` is the sending of a message and `b` is the receipt of that *same* message.
    3.  There exists an event `c` such that `a → c` and `c → b` (transitivity).
*   If neither `a → b` nor `b → a`, the events are **concurrent** (`a || b`).
![[happens-before.png\|500]]
- a → b, c → d, and e → f due to node execution order
- b → c and d → f due to messages m1 and m2
- a → c, a → d, a → f, b → d, b → f, and c → f due to transitivity
- a ‖ e, b ‖ e, c ‖ e, and d ‖ e
#### Causal Consistency
A relaxation of sequential consistency. It only requires that **causally related** writes be seen by all processes in the *same* order. **Concurrent writes** (writes that are not causally related) may be seen in different orders by different processes.

**(DEFINITION & DIAGRAM)** A write `W2` is causally related to a previous write `W1` if `W2` could have been influenced by knowing the result of `W1` (e.g., a process read the value written by `W1` before performing `W2`).
![[causal-consistency.png\|500]]
*   **Valid:** `W1(x)c` and `W2(x)b` are concurrent (no causal link). Therefore, P2 and P3 are allowed to see them in different orders.

**(VIOLATION EXAMPLE)**
![[violated-causal-consistency.png\|500]]
*   **Diagram (a) is a Violation:** `R2(x)a` (the read of `a` by P2) creates a causal link from `W1(x)a` to `W2(x)b`. Therefore, all processes must see `W1(x)a` before `W2(x)b`. P3 sees them in reverse order, violating causal consistency.
*   **Diagram (b) is Acceptable** for causal (but *not* sequential) consistency because `W1(x)a` and `W2(x)b` are concurrent.

**Check of Understanding**
> **Question:** A social media post and its comments are stored across three geographically distributed replicas (US, EU, Asia). A user in the US posts an update (Write A). A user in Asia reads that post and immediately posts a comment (Write B). A user in Europe then reads the original post. Under a **causally consistent** model, is it permissible for the European user to see the comment (Write B) but *not* see the original post (Write A) it replied to? Explain using the happens-before relation.
> **Answer:** **No, this is not permissible under causal consistency.**
> **Explanation using happens-before:** The Asian user's read of the post establishes a causal link: `Write A → Read A (in Asia) → Write B`. Therefore, `Write A` happens-before `Write B` (`Write A → Write B`). The causal consistency model requires that all processes see causally related writes in the same order. If the European user sees `Write B` (the comment), they must also have seen `Write A` (the original post) that causally preceded it. Seeing the effect (comment) without its cause (original post) violates the contract.
## 13. Consistency and Replication Pt.1 - Chapter 7
### Client-centric consistency models
Client-centric consistency models guarantee consistency for the **operations of a single client** across a distributed data store. They do not provide guarantees about concurrent operations from different clients. These models are crucial for mobile or intermittently-connected users who access the data store from different locations, ensuring their view remains self-consistent even as they move.

**(BASIC ARCHITECTURE)** The system involves multiple local stores (e.g., `L1`, `L2`). A client process (`P`) performs read (`R`) and write (`W`) operations on a data item `x`. Different versions of `x` (e.g., `x1`, `x2`) exist across these stores.
![[client-centric-arch.png\|500]]
Notation: `W1(x1;x2)` means process P1 writes version `x2` based on the previous version `x1`. `W1(x1|x2)` indicates a concurrent write that does not follow from `x1`.
#### Monotonic Reads (MRs)
If a process reads a particular version of a data item, any future read by that same process must return that version or a **never-older** version. It prevents a user from seeing data "go back in time" when they switch servers.
**(EXAMPLE)** A user reads their unread email count (10 emails) on a server in London (`L1`). Later, they connect from a server in New York (`L2`). Monotonic Reads guarantees the count in New York is **at least 10**. It could be 12 (if new mail arrived), but never 9.
![[monotonic-read-ex-1.png\|500]]
*Diagram (b) shows a violation:* P1 reads `x1` at L1, but a later read at L2 returns `x2`, which is a version that was created *concurrently* to and does *not* include the updates seen in `x1`. This is not allowed.
#### Monotonic Writes (MWs)
Write operations performed by a single process must be **propagated to all replicas in the same order** they were issued. This ensures a user's sequence of updates is applied correctly everywhere.

**(EXAMPLE)** A user first updates a document's title (Write A), then updates its body text (Write B). Monotonic Writes ensures that no server will ever see Write B applied *before* Write A. This is critical for operations that depend on previous ones (e.g., installing software libraries in the correct order).
![[monotonic-write-ex1.png\|500]]
*Diagram (b) shows a violation:* The write that produced `x2` at L1 is propagated to L2 *after* a later write that produced `x3`. The order of writes seen at L2 is wrong.
#### Read Your Writes (RYWs)
The effects of a write operation by a process are **always visible to that process's subsequent read operations**, no matter from which location it reads. It prevents a user from seeing stale data after they have just updated it.

**(EXAMPLE)** You update your profile picture on a social media site (write). When you immediately refresh the page (read), you are guaranteed to see the new picture, not the old cached version.
![[read-your-write-consistency-1.png\|500]]
*Diagram (b) shows a violation:* P1 writes `x1` at L1. Later, when reading at L2, it sees `x2`, which is a version created *without* incorporating its own previous write (`x1`). This violates the "read your own writes" guarantee.
### Replica management
This involves deciding **where** to place replica servers, **what content** to put on them, and **how** to keep them consistent.
#### Content Replication
Replicas can be categorised by their initiation method:
*   **Permanent Replicas:** The initial, fixed set of replicas that form the core of the distributed data store.
*   **Server-Initiated Replicas:** Created dynamically by the system (owner of the data) to improve performance, typically placed near clusters of demanding clients.
*   **Client-Initiated Replicas (Caches):** Created at the request of a client to locally store data for fast access (e.g., a web browser cache).
![[replica-rings.png\|500]]
#### Server-Initiated Replica Example
A system dynamically creates temporary replicas of popular content to reduce load on the origin server and bring data closer to users. The decision is based on tracking access patterns.
**(ALGORITHM)** Servers monitor file access counts. Requests are aggregated by the server closest to the client.
![[server-initiated-replica.png\|500]]
*   If accesses for file `F` from a region exceed a **Replication threshold (R)**, a new replica of `F` is created in that region.
*   If accesses fall below a **Deletion threshold (D)**, the replica is removed.
*   If accesses are between D and R, the file might be **migrated** (moved) instead of replicated.

**(EXAMPLE)** A viral video hosted in London (`Q`) suddenly gets millions of requests from Asia. The London server sees the high access count `cntQ(P, F)` from an Asian proxy server (`P`). It then decides to create a server-initiated replica of the video on a server in Singapore to serve Asian users directly.
#### Consistency Protocols: Primary-backup Protocol with Remote Writes
This is a primary-based protocol for implementing sequential consistency. One server is designated the **primary** (or master) for a data item.

**(MECHANISM - BLOCKING)**
1.  A client sends a write request to the primary server.
2.  The primary performs the write on its local copy.
3.  The primary forwards the update to all **backup** (secondary) servers.
4.  The primary waits for acknowledgements from all backups.
5.  Only after all backups confirm, does the primary send an acknowledgement back to the client.
![[primary-bakcup-remote-write-prot.png\|500]]
**(CHARACTERISTICS & PERFORMANCE)**
*   **Strong Consistency:** Provides sequential consistency because the primary serialises all writes.
*   **Fault Tolerance:** High, as the write is confirmed on multiple servers before completion.
*   **Performance:** Write latency is high (blocking) because the client must wait for multiple network round-trips. Read performance is good (can be served locally from any replica).
#### Consistency Protocols: Primary-backup Protocol with Local Writes
A variant where the **primary role can move** to the client that wants to perform a write. This is beneficial for mobile or disconnected operation.
**(MECHANISM)**
1.  A client needing to write locates and "moves" the primary copy of the data to its local machine.
2.  The client can now perform multiple writes **locally** and very quickly.
3.  Updates are propagated to the backup servers **asynchronously** (non-blocking) after the local writes are done.
![[primary-bakcup-local-write-prot.png\|500]]
**(USE CASES & TRADE-OFFS)**
*   **Use Case 1 - Mobile Disconnected Operation:** A user takes a file primary with them on a laptop, works offline, and synchronises changes back to the network later.
*   **Use Case 2 - Performance Optimisation:** A client performing a burst of updates (e.g., a batch job) can temporarily become the primary to avoid network latency for each write.
*   **Trade-off:** Provides lower write latency for the primary holder but weakens immediate consistency guarantees for other readers until propagation is complete.
**Check of Understanding**
> **Question:** A globally distributed note-taking app uses a primary-backup protocol. A user in Tokyo starts editing a document. To minimise latency, the app makes her local device the primary for that document. She makes ten rapid edits (local writes). At the same time, a collaborator in Berlin refreshes the document.
> 1.  Under a **remote-write** protocol, what would the Berlin user see during the Tokyo edits, and what is the performance impact for the Tokyo user?
> 2.  Under the described **local-write** protocol, what is the Berlin user likely to see, and what consistency model (client-centric) is potentially violated for the Berlin user?
>
> **Answer:**
> 1.  **Remote-Write:** The Berlin user would see each edit **as soon as it is fully replicated** to the backups. The Tokyo user would experience **high write latency** for each edit, as each one would need to be sent to and acknowledged by the central primary and all backups before she could continue.
> 2.  **Local-Write:** The Berlin user is **likely to see stale data** (the old version) until the Tokyo user's device propagates its batch of updates. This scenario violates the **Monotonic Reads** guarantee for the Berlin user if he had previously seen a more recent version from another server. More generally, it highlights the trade-off for **Read Your Writes** consistency *across different users*.
## 14. Fault Tolerance - *Chapter 8*
### Dependability, reliability and availability in a DS
### Terminology
### Failure models
### Process resilience
### Consensus with crash failures
### Consensus with arbitrary failures
### The Byzantine Generals Problem

## 15. Cloud Computing
### Technology Landscape
### Towards a Definition of Cloud Computing
### Virtualised infrastructures
### Conceptual Cloud Architecture
### Taxonomy of cloud Models
### Virtual Infrastructure Managers
### Cloud services
### Types of Clouds
## 16. Distributed Systems Topics and Trends Pt.1
### IoT: the Internet of Things
### Edge Computing
### Revisiting the Cloud Computing Stack
### Vision for a (near) future
## 17. Distributed Systems Topics and Trends Pt.2
### Module themes
### Evolution of distributed computing
### Most Active Topics in Distributed Systems