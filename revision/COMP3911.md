# 00. Intro
## Security Goals & Properties
- Data confidentiality
- Data integrity: No modification, insertion, deletion or replay
- High availability: Amazon systems aer available 99.9% of the year
- Authenticity: Data sources and system users
- Access control
- Trustworthy system logs
## Attackers follow the M.O.M. acronym 
**Method**: They have the skills and tools to attack the systems and potentially have information about it.
**Opportunity**: They have a time/chance and the acces to carry out this attack e.g., systems can be accessed remotely.
**Motive**: They have a reason to attack e.g., Financial or Political.
### Assets and Adversaries 
To identify assets, you must analyse the system and the user themselves e.g., A photo of a celebrity is considered an asset, whereas a photo of a non-famous person might not be. Likewise, user data is considered an asset as it's exposure can lead to financial gain.

An adversary is the perpetrator with their eyes on the asset.
### Vulnerabilities
For attacks to be possible there must be vulnerabilities. These occur when system specifications or conditions are loosely defined e.g., buffer overflows (unspecified buffer size) and SQL Injection attacks.
### Threats
A threat is a set of circumstances that has the *potential* to cause *harm or loss*.
### Problem
A problem exists when there is both a threat and a vulnerability.
If the vulnerability can be controlled the then threat can be contained e.g., a *vulnerability* of unprotected data can be *mitigated* via the enabling of access control.
## Threats and Controls
Four main types of threats:
1. Interception: Gaining access to an asset.
2. Interruption: Preventing access to an asset.
3. Modification: Tampering of an asset.
4. Fabrication: Addition of nonexistent items.

Four main types of controls:
1. Prevention: Remove all vulnerabilities.
2. Deferral: Deal with the vulenrability.
3. Deflection: Change the nature of the asset e.g., data may be moved from on-site to cloud.
4. Detection: Controls to observe the attacks.
5. Recovery: Moving on from attacks.
## Why is security hard
- Humans make mistakes (**PEBKAC**: Problem Exists Between Keyboard and Chair)
- Assumptions are made incorrectly
- It is not easily quantified and there is no scoring system or real measure of secure.
- Emergent system properties are unpredictable.
## Hackers
- Originally defined as someone who enjoys understanding and tinkering with software or hardware.
- Different types:
	- White Hat: Uses skills ethically.
	- Black Hat: Uses skills unethically.
# 01. Threat Modelling pt. 1
Threat modelling is the *process* of *identifying* the *possible threats* to a system. 
It allows us to understand the capabilities of attackers and their threats which will endanger the security policies in place. This approach is cost-effective as it avoids mistakes/future threats that are alike.
##### Bellovin’s Threat Matrix → ![[bellovin-threat-matrix.png]]
#### Advanced Persistent Threats
- Highly skilled attackers
- Targeted and prolonged cyber-attack campaigns
- Complex, multidimensional attacks, often tightly focused and very difficult to deflect (e.g., **Stuxnet**)
- ‘Cyber-warfare’ involving ‘state actors’ (e.g., national intelligence agencies)
### Security Engineering
The design process for security follows threat modelling, security requirement building and security mechanism development.
The thread model allows security designers to estimate the stregnth of the attacker.
## Main steps for threat Modelling
### 1. System Characterisation 
Characterise the system into main modules to see the system's data flow.
To understand the system components and their interconnections, define:
- Usage scenarios
- Assumptions
- Dependencies

and model:
- Data flow diagrams
- The Network
- State machines
- At varying granularity e.g. APIs, classes or methods.
### 2. Asset and Access Point Identification 
Identify assets and access points to understand how a user would access the system.
- What are we trying to protect?
- How can we reach the system and asset?
### 3. Threat Identification 
Identify any threats. Threats to different components will differ.
Assets can be reached via access points through components and their interaconnections.
Develop a threat profile for each threat (classified as high or low risk).
#### Threat Enumeration
Threats can come from both outside and in.
For each asset, attack goals need to be made for the asset.
The adversary type can be discovered by correlating threats to assets.
#### Attack Trees
A conceptual tree structure used to show how an asset can be attacked (the root node).
Each node represents a condition/predicate/action. E.g., Pick lock (*Impossible*), Learn combination (*Possible*)->Get from staff->Blackmail, Learn combination (*P*)->Get from staff->Bribe.
## The STRIDE Threat Model Classification
Acting as a useful checklist of threats and their effects. It also assigns priority to threats
Six impact-based threat classifications:
### 1. Spoofing - masquerade e.g., unauthorised access.
E.g., email spoofing: fake sender address appears to come from a trusted source.
Website spoofing: fraudulent site masquerades as legitimate to steal credentials.
### 2. Tampering - violates data integrity.
E.g., Hacker disabling anti-virus on a machine.
**Man in the Middle Attack**: intercepts/alters communication between two parties.
### 3. Repudiation - Deny performing action.
E.g., user denies making an online transaction and the system lacks sufficient evidence, like a digital signature, to prove otherwise. This is the reason log files exist. Common in E-commerce.
### 4. Info disclosure - violate confidentiality.
E.g., Servers revealing directory structures, detailed messages exposing software/protocol versions.
### 5. Denial of Service
E.g., Mirai botnet: Consisted of large number of IoT devices such as IP cameras.
Instructed to send a large number of traffic to Dyn (DNS provider) servers. Prevent the system from providing the designated service.
### 6. Elevation of privilege - Gaining special status.
E.g., Flaw in OS (buffer overflow). Ultimately, get admin privileges.

We think of threats and attack in the terms of these classifications as well as terms of breaches of confidentiality, integrity or availability.
E.g., Data in transit from A to B, can be *tampered* with, violating *integrity*. 
To mitigate this, can we use a Secure Hashing Algorithm?
### STRIDE Per Element
Certain threats are more prevalent than others and should be focused.
## Specifying Security Requirements
These threats are translated into requirements and prioritsed (cannot handle all of them).
E.g., 
	**THREAT**: DoS attack on system
	**HOW**: Flood Network Interface or Disk Space
	**RISK**: High
	**REQUIREMENT**: DoS should not be allowed.
	**MECHANISMS**: Use firewall to drop certain packets and restrict resources used by anonymous users.
# 02. Threat Modelling pt. 2
## STRIDE Model Goals
Threat modelling can be considered from three different perspectives:
### 1. Assets
The valuables.
Tangible things which obvious for attackers e.g., user credentials, bank details.
Intangible things which need to be protected e.g., reputation, system availability.
### 2. Attackers
Consider human threat agents. Lacks structural detail to help identify an attacker's methodology.
Prrone to bias when creating an potential attacker list.
### 3. Software
Using scenario-based/user story problems to explore potential problems.
Can be highly structured by creating and using systems diagrams, making STRIDE calssifications and attack trees.
Software-focused approach is considered to be the best.
## Trust Boundaries
- These emerge when entities, with different levels of access, interact.
- TBs are drawn around user accounts, physical computers, VMs, Network segments, etc.
- They can be identified by:
	- Identify the *principals* (users)
	- Following the privelege spectrum of access e.g., User vs. Admin.
	- A new TB is added every time a principal talks to another
### Entry & Exit Points
- Places where control or data *cross* a *trust boundary*.
- Easy to overlook infrastructure entry points e.g., Config files, Windows registry, etc.
- Entry points can be layered.
### Importance
TBs delineate the attack surface between principals. 
Threats tend to cluster around the entry/exit points on TBs and can follow data flows.
So, diagramming mainly helps us to learn, in a systematic way, **where** we should look for threats.
## Attack Trees
- Root node, representing a threat
- Child nodes represent a condition that must be true for attack to succeed
- Arcs between parent and child nodes
- Path from a leaf node to root is an attack path
	- Dashed line can be used to indicate low likelihood
- Boolean relationships between siblings
	- OR is *implicit*
	- AND must be shown *explicitly*
##### Attack Tree Example ->
![[attack-tree-ex.png]]
### ### Risk Assessment: DREAD
For each threat, estimate
- **Damage** **potential**.
- **Reproducibility** (how often an attempt at exploiting a vulnerability actually works)
- **Exploitability** (effort required to exploit vulnerability).
- **Affected** **users** (proportion of install base that would be affected if an exploit became widely available).
- **Discoverability** (likelihood that a vulnerability will be found by external security researchers or black hats).
- (Use a 1–3 or 1–5 scale for each, compute *R* as a simple average)
#### Issues With DREAD as a scoring system
- Values are highly subjective
- Not all dimensions are useful – e.g., why assume that Discoverability is anything other than 100%?
- Dimensions are weighted equally but might not be considered equally.
### Common Vulnerability Scoring System (CVSS)
- Open standard: v2 and v3 in current use
- Scoring is more thorough and objective than **DREAD**
	- Uses a 0.0 – 10.0 scale
	- Provides a base score for severity, optionally modified to reflect risks in a specific environment
### Mitigation
Ideally, every threat is mitigated either by design or implementation of the system.
- Corollary: every security feature should address at least one threat from the threat model!
- Rank unmitigated threats by risk and prioritise.
![[attack-tree-paths-ex.png]]There are **four** attack paths because 1.1 and 1.2 must both be true.
1. 1.1 AND 1.2.1
2. 1.1 AND 1.2.2
3. 1.3.1
4. 1.3.2
How can we mitigate a specific condition e.g. 1.3.1. We now have three ways of attacking the system now instead of 4.
*Which condition should be mitigated first to best reduce the number of attack paths?*
**1.1** ! Because it means that we now have two attack paths.
# 03. Managing Vulnerabilities
## Exploits
Attacks that leverage a vulnerability are called *exploits* but not all vulnerabilities turn into realistic exploits.

Developers know that there will be vulnerabilities and potential exploits. After internal checks, the turn to the goodwill of the public to report on anything new, offering rewards as an incentive. It is essentially an arms race between security researchers and attackers. 
### Responsible Disclosure
Those with goodwill practise *responsible disclosure*, whereby they *inform* the software *vendor* of their find, receive their award and agree to *stay silence* for a given time.
A *Bug Bounty Program* is a structured environment where researchers investigated vulnerabilities.
### 0Day Attacks
Those who are more morally questionable, use the vulnerability to attack the system. *Exploits* that are *actively* in use *before* any responsible disclosure has taken place are called *0day attacks*.
## The Vulnerability Cycle
![[the-vulnerability-cycle.png]]
1. Vulnerability discovered and disclosed
2. Vulnerability used in an attack becoes an exploit and the search for fix begins.
3. The exploit has caught the attention of the media, bad publicity ensues.
4. Immediate diagnosis of the issue, search through logs, the patch begins development.
5. Discovery of similar vulnerability in related code.
6. If the attacker finds these first before they are patched, go to 2. Else, a tool is used to identify the exploit on unpatched systems and the cycle repeats.
### Discussion/Analysis
- Common Vulnerabilities & Exposures (**CVE**)  
	- CVE identifier scheme allows vulnerabilities to be uniquely numbered
- Common Weakness Enumeration (**CWE**)  
	- Standard for classifying vulnerabilities by type  
	- Example: CWE-120 = ‘Buffer copy without checking size of input’ 
- Scoring systems  
	- Useful for prioritising vulnerabilities  
	- Competing standards: CVSS and CWSS
### QA Techniques
- Code review
	- Hawthorne Effect (if someone's watching, you perform better)
	- Peer review, pair programming (XP)
	- External independants reviewers tied to version control (VC)
- Checklists  
	- Ensure proper consideration of security issues
	- Must be maintained to remain useful
- Static analysis tools  
#### The ‘Many Eyeballs’ Fallacy  
- Bug finding rates in Open Source Software (OSS) do not scale linearly with number of  reviewers 
- Security caveat: you need the ‘right kind of eyeballs’ to uncover security-related bugs!
#### Static Analysis Tools
Often designed for C as the language is notorious for its exploits.
- Pattern matching / lexical analysis
	- [Flawfinder](https://www.dwheeler.com/flawfinder/) (C, C++)
	- [RATS](https://www.dwheeler.com/flawfinder/) (C, Perl, PHP, Python, Ruby, OpenSSL)
- Parsing / data flow analysis
	- [Splint](https://splint.org/) – ‘Secure Programming Lint’ (C only)
	- [FindSecBugs](https://find-sec-bugs.github.io/) – plug-in for [SpotBugs](https://spotbugs.github.io/) (Java only)
	- Meta’s [Pysa](https://engineering.fb.com/2020/08/07/security/pysa/) (Python only)
- Logical reasoning about program memory usage
	- Meta’s [Infer](https://fbinfer.com) (C, Objective-C, Java)
#### Limitations of Static Analysis
- False positives reported
	- Danger of ‘flaw fatigue’
- False negatives
- Flaw database must be maintained
- Source code required
##### Testing != Security Testing
![[testing-ne-security-testing.png]]
### Whittaker’s Fault Model
##### Diagram
![[whittaker-fault-model.png]]
#### Testing
When you are left final bugs, how can you prove that the piece of software is now secure?
- Attack environmental dependencies
	- Block access to libs; manipulate Windows registry; corrupt config files; simulate lack of memory, disk, etc.
- Attack user interfaces
	- Overrun buffers; use escape characters; inject code.
- Attack the design
	- Find unprotected accounts; connect to all ports; fake sources of data; explore alternate routes to functionality.
- Attack the implementation
	- Exploit TOC (Time of Compilation)-TOU (Time of Use); force all errors; uncover test APIs; screen temporary files for sensitive information.
#### Using Threat Models
- Threat model drives the testing process.
- Each threat needs a test plan (made with STRIDE, organised by DREAD).
- STRIDE tells us what types of test to perform.
- Quantitative risk assessment (e.g., DREAD rating) can be used to prioritise test plans .
#### ‘Secure From Day One’
- Security (re)training for team members
- Threat model starts at first stage, when product is in planning; refined thereafter.
- Design process is driven by threat model and adopts standard principles such as reluctance to trust, granting of minimal privilege, etc.
- Designs undergo security reviews.
- Code QA and testing as described earlier.
- ‘Security push’ in later stages.
- Full security audit before shipping.
### Run-time Fault Injection
Behaves as a stress-test the system in the wrong states. For the system into these states.
- Can simulate faults using software that intercepts and modifies system calls e.g., *Holodeck* (Windows)
- System monitoring
- Fault injection
	- Insufficient memory, failure to lock memory
	- No disk space, too many open files, no disk in drive
	- Low bandwidth, network disconnected, no ports, missing libraries...
# 04. Security Policy Models
The overall protection of the system is made up of the Policy, Middleware and Mechanisms. These work hand-in-hand as follows:
- **Policy**: The concise, formalised set of goals.
- **Mechanisms**: The basics components needed to satisfy the policy.
- **Middleware**: How the mechanisms are enforced to satisfy the policy
#### Example
Imagine we want to protect a system from unauthorised access.
The policy here would be to set up a entry point that verifies each person on entry.
The mechanism to achieve this could be a biometric fingerprint scanner.
The middleware would be a component that enables the function of the scanner.
## Security Policy
A security policy is a high-level specificaction of security properties a system should possess. These specifications clarify the security requiremets of the system.

A policy is a set of high-level documents that state precisely what goals the protection mechanism aim to achieve.
### Policy Language
The security policy can be expressed in different ways depending on the level of granularity needed.
High-level languages are used on the outset:
- Policy constraints expressed abstractly.
- Constraints expressed independent of enforcement mechanism.
- Constraints restrict entities and actions.
- Constraints expressed unambiguously.
- Requires a precise language, usually a mathematical, logical or programming-like language.

Low-level languages are used for precision:
- Low-level of abstraction.
- Policy constraints are expressed in terms of program options, input or specific system entity characteristics.
- Set of inputs or arguments to commands.
- Check or set constraints on a system.
- Need details of system, commands.
## Policy Models
A security model represents a particular policy or set of policies.

When defining your security policies, an existing model is often followed and used as a guideline. When building a house, you do not tend to start from scratch, instead, you follow a guide. This is because many of the problems that arise have been solved before and you would only re-design something in a unique situation.

There are several existing models and you need to pick the correct one e.g. *Bell-LaPadula*, *Biba*. You cant pick a bedroom model for the kitchen.
#### Access Control
Documents (objects) are classified according to labels.
### BELL-LaPADULA (BLP) Model
This is an example of multi-level security (*MLS*). 
#### Property: SImple Security
No process can read data at a higher level (*no read up*)

Subject *s* can read object *o*, $$iff \ \ L(o) \le L(s)$$ and *s* has permission to write *o* .
This also has the *no write down* star property. Thanks to mandatory access control, information can only flow upwards. Protects against trojans.
#### Formal BLP
Subjects/objects are assigned a security label comprising a classification level and category.
We represent this as a tuple: `security_label = (class, category level)`

There is a binary relation, D (Dominates) over this set of security labels.
D is a *partial order* i.e. it is reflexive, antisymmetric and transitive

For a relation to be *reflexive*, every element must be related to itself such that every a in a relation, (a,a) exists.
For a relation to be *antisymmetric*, every element must belong to the relation and each element (a,b) must have a (b,a).
For a relation to be *transitive*, elements (a,b,c) in a relation must connect such that a connects to b and b connects to say, giving the transitive property that a connects to c as well.

This partial order is important because it provides a higher privilege to those without any but does not clear those who share classification level, regardless of category.
#### Lattices
Given an appropriate set of security labels and the auxiliary functions:
- **join()**, which returns the Lowest Upper Bound (LUB)
- **meet()**, which returns Greatest Lower Bound (GLB)

This relation forms a lattice structure for data flow.
![[img-lattice-security-model.png]]
This is useful because it allows us to identify the minimum clearances need to allow data flow within a system.

**Given two labels, *(Secret, {Crypto, Foregin})* and *(Top Secret, {Crypto})*, get the LUB and GLB.**
GLB from meet() is *(Secret, {Crypto})* because that's where their two edges meet at the bottom.
LUB from join() is *(Top Secret, {Crypto, Foregin})* because their two edges meet at the top.

**What point can be removed from the set to no longer make this a lattice, i.e., not all pair of points have a LUB and GLB?**
If we remove *(Top Secret, {Crypto, Foregin})*, then there will not be a LUB  for the two labels connected.
#### Reference Monitor
Using these definitions and data flow structure, a predicate called "exec" can be defined.
It takes a Subject s, an Object o and an Action a as arguments, i.e., $$\bra s,o,a\ket \ \in exec$$
States that a Subject can perfrom an Action on the Object. The predicate will evaluate to a boolean value based on if the security requirements are met.
### BIBA Model
BLP enforces some Mandatory Access Control (MAC), whereas BIBA is deployed more to preserve integrity, i.e., BLP upside-down.
Confidentiality is about who can **read** data; integrity is about who can **write** it.
### Chinese Wall Model
A weakness of MAC is that we have no record of the history.
Designed to prevent conflicts of interest or insider dealings.
They define three levels of abstraction:
**Objects**: Contain information about only one company.
**Company Groups**: Collect all objects concering a particular company.
**Conflict Classes**: Cluster the groups of objects for competing companies e.g., {Shell, BP}. These are who you cannot work with multiple of.

The key is to keep track of state. To do this, a 2D matrix is kept e.g., The contamination of Object o by Subject s is either true or false e.g., C[s,o] = 1 if s has accessed o.
#### Property: SImple Security
Each subject can access objects from at most one company at any time. E.g. Subject s can only acess object o if s has never dealt with a company of type o (t(o)). If s servers multiple companies of the same type, then they are all the same company.
#### Star property
s can write to o if all other objects p, that s has written to, are the same type and object or p is a sanitised object.

*Sanitsation* is the process through which conflict is removed via a trusted party. You can only have one trusted party. Similar to anonymising data.
### Resurrecting Duckling Model
Not all models can be specified through permanent access control. Here, the security policy is transient. This is a kind of "on-the-go" temporary security.
E.g. To connect some speakers with bluetooth, you need to undergo authentication for them to have a transient connection and then restart (resurrect). Followed by a clean slate (duckling).

These states are imprintable and imprinted:
1. **imprantable**: ready to trust another system.
2. **imprinted**: committed to trust that system.

To transition between them:
- 1->2, send a secret key and trust the first system.
- 2->1, death via time out, reset or end of transaction.
### Other
- Capabilities of the 2D matrix.
- Access Control Lists (ACLs)
- Role Based Access.
# 05. Command Injection and Input Validation
## Input Validation
Use prepared statements instead of filtering input data. Parameterised and reusable SQL queries are better for the developer.
### Tust Boundaries and Choke Points
![[choke-points-ex.png]]
Testing at choke points validates data throughout the structure flow.
### Phishing via Homograph Attacks
Letters used from different but seemingly identical alphabets can be used to hide false websites.
Malicious URLs often have null bytes between the user-readable part of the URL and the @ symbol.
### Malicious URLs e.g., Directory Traversal
Replacing the file dir of a dynamic page's GET req to retrieve the directory of the password file.
The solution is to use canonicalisation (aliases). User input is converted into canonical form before being used e.g.,```
File file = new File(BASE_DIR, userIn)
if ( file.getCanonicalPath().startsWith(BASE_DIR) )  { // Process file. }```
# 06. Web App Vulnerabilities
Web apps are often developed in high-level applications like Python or Java. Thanks to this, low-level vulnerabilities like buffer overflows are better protected. 
Due to the fact that web apps are accessible by absolutely anyone, the *attack surface* is very large. Again, input validation is often the issue.
## Web Attacks
Tampering with data:
- Web page defacement (e.g. different values to what you expect)
- Alteration of data in a backend DB
Information Disclosure:
- Active authenticatio tokens
- Stored user credentials
- Cred card details / sensitive info
Denial of Service.
Elevation of privilege.
### GET and POST Requests
- Both send data to the server with URL-encodings
- GET sends data as a query string
- POST sends data as a payload of the request
- POST is for operations that have side-effects, changing the app state.
	- Browsers often allow resubmissions of GET reqs via page reload but warn if you trry with POST.
### Sessions
- HTTP is *stateless*, so sessions are used to recognise user interaction sequences.
- A *session* is represented by a persistent token (the **session ID**), stored by the client as a *cookie*.
- Thee persistent tokens can be for both authenticated and unauthenticated users.
- These sessions also have an associated state, i.e., they can be stored locally or cached on the server.
#### ### Session ID Dangers
This can be predicted by the attacker and they allow them to impersonate the user.
If the browser doesn't support cookies, the web app might add the session id to the URL, exposing it entirely.
To avoid this:
- session IDs must be generated with a PRNG (Pseudo Random Number Generator)
- Never re-use
- use HTTPS protocol and enable the secure cookie option
- establish a maximum session lifetime
- Invalidate idle sessions (clean slate)
- limit session concurrency
- make it easy to log out and clear session data
- minimise the use of client-side session state storage and encrypt anything that is client-side
## Attacking the Connection
### HTTPS issues
- HTTPS is not perfect: DROWN Cross-protocol attack
#### DROWN (Decrypting RSA with Obsolete and Weakened eNcryption)
![[drown-ex.jpg]]
A server is vulnerable to DROWN if:
- It allows SSLv2 connections. This is surprisingly common, due to misconfiguration and inappropriate default settings.
- Its private key is used on _any other server_ that allows SSLv2 connections, even for another protocol. Many companies reuse the same certificate and key on their web and email servers. In this case, if the email server supports SSLv2 and the web server does not, an attacker can take advantage of the email server to break TLS connections to the web server.
### Lack of HSTS (HTTPS Strict Transport Security)
- HSTS forces use of HTTPS for all connections but still allows initial insecure requests via HTTP - risk of MITM attack.
### Miuse of TLS Certificates
### Open Redirects
Unvalidated redirects and forwards are possible when a web application accepts untrusted input that could cause the web application to redirect the request to a URL contained within untrusted input. By modifying untrusted URL input to a malicious site, an attacker may successfully launch a phishing scam and steal user credentials.

Because the server name in the modified link is identical to the original site, phishing attempts may have a more trustworthy appearance. Unvalidated redirect and forward attacks can also be used to maliciously craft a URL that would pass the application's access control check and then forward the attacker to privileged functions that they would normally not be able to access.

The solution is to avoid using user-inputs for redirect or sanitise said inputs first.
### URL Jumping
When an application like an e-commerce site has an expected page flow, the attacker can attempt to skip, say, the payment step. To fix this, keep track of the session state.
### Malicious XML Payloads
- Traditional web services involved the exchange of messages as XML documents; this document woud be sent as a request to the server and parsed.
- This parsing stage was vulnerable and gave rise to *eXponential entity eXpansions (XXE)* attacks 
- In this stage, the XML parser is using a pre-allocated amount of memory but the contents of this malicous payload contains definitions that expand the size to be larger than that memory, causing the system to crash.
- This is a *Denial of Service (DoS)* attack and was also called the *'Billion Laughs'* Attack
- To fix this, use lazy evalutation to evaluate values when needed not all at once.
## Cross-site Scripting (XSS)
- An example of an OWASP (Open Web Application Security Project) attack which refers to ==a method used to exploit a weakness in web applications== like Injection (e.g., SQL Injection, XSS), Broken Access Control, and Security Misconfigurations.
- XSS is a type of injection attack where malicious scripts are injected into trusted websites, so that when a victim visits the infected page, the malicious code is downloaded to their browser and executed.

There are two main type of XSS:
1. Reflected XSS (common)
2. Stored XSS (rare)
### Reflected XSS
Occurs when an app receive data in an HTTP request and includes that data within the immediate response in an unsafe way. 
- The emphasis is on server-side validationg of potentially dangerous client input.
```HTML
<url>https://bad.com/msg=hello%20world</url>
<p>hello world</p>

<url>https://bad.com/msg=<script>...</script></url>
<p><script>...</script></p>
```
If a form sends its data via GET, we can craft the URL to submit code via the query string e.g., 
`http://foo.com/search?name=%3Cspan%20onmouseover%3D%22alert%28%27Gotcha%2Fspan%3E`
If a form uses POST, then the *value* field is used instead of the URL. This is another example of unchecked user input.
- Requests can also be made via the img element as it will be loaded immediately.
### Stored XSS
- Relies on an injected script (JS) being stored on a vulnerable server- usually in the db.
- Targets are often forums, blogs with comments, sites that allow reviews etc.
- There is no social engineering needed as the code is delivered through normal brrowsing activity.
- Is more dangerous as malicious code can persist for a long time and affect many people.

To avoid:
- encode HTML before it is delivered to the browser e.g., & becomes &amp
- reject HTML input and only allow Markdown user input
- use the HttpOnly attribute flag with cookies to hide them from JS code.
### HTML Injection
When an app does not properly handle user input, an attacker can supply valid HTML code e.g., via a parameter value, and inject their own content into the page.
#### Frame attack
An attacker could inject a *frame* that points back to a server they control.
This woud dispay a message saying the user has been logged out and must re-supply credentials.
This is achievable within a form too.
## Cross-site Request Frogery XSRF/CSRF
Here, the client trusts the server, exploiting the fact that your broswer ssens and authentication token to a server; tricks you/your browser into authenticating a request you never intended to make.
#### Example
1. You receive an email telling you that there is suspicious activity on your banking app.
2. The email asks you to first log in to the app on your browser and then visit the following link:
```HTML
<url>https://www.bank.com@167772161/main.hmtl</url>
```
*This is a disguised URL containing the attacker's IP encoded as a 32-bit integer.*
3. You log in and begin your session, receiving an authentication token.
4. You click on the URL and watch as your token is used to perform a transaction you did not want.
#### To Avoid This
When the user logs in, generate a random *nonce* (a sequence of numbers) and store it with the user's session data - like salt with a password and used as a checksum.

This nonce is included in all pages sent back to the user's browser as a hidden form field and will be returned to the server in requests originating from those pages which is then checked against the value stored for that session.

This ensures each session is unique but what about MITM attacks? If all you need is the nonce value to mimic the session, then its an easy exploit. Consider cryptogtaphically solving this.
### Validation: Client or Server?
You cannot defend the server side of a web app by implementing defences in the client.
- The broswer is easily bypassed by attackers who will craft HTTP reqs manually.
- Client-side validation is a usability and performance aid **not** a security aid. Valiation needs to be implemented on the server side as well.
#### When to validate?
- Validate as input comes into the system; validating asap reduces the number of places where malicious content may be stored.
- On the other hand, it means that many other parts of the system have to trust that validation was done properly.
#### Comparison with older OWASP Top 10s
- Injection and auth issues are less significant
- Cryptographic failure and use of vulnerable components has become more significant.
# 07. User Authentication
## Entropy
N represents the number of items and L represents how many of those items are needed.
Entropy equation: $$ \fcolorbox{red}{black}{$L \cdot log_2N$} = log_{10}N^L \div log_{10}2 =\text{total bits} $$Total entropy:$$
\begin{rcases}
  N = 95 \\
  L = 8
\end{rcases} = 8\cdot{log_{10}95 \over log_{10}2} = 8 \cdot log_2{95} = 52.56$$ bits (2D.P.)
#### Example
What is the entropy of a 4-digit PIN (0-9)?$$
\begin{rcases}
  N = 10 \\
  L = 4
\end{rcases} = 4\cdot{log_{10}10 \over log_{10}2} = {4 \over log_{10}2} \ OR \ 4 \cdot log_2{10} = 13.29$$ bits (2D.P.)
### Key Sweeper
Looks like a phone charger but sniffs keystrokes for certain Microsoft wireless keyboards and logs them. Sends the data to the attacker vis SMS.
### Password Hashing
- Store the hash of a password, not the password itself.
- Hash functions are fast by design so brute-forcing would be efficient.
	- Avoid this by hashing them same password multiple times to slow down the method.
- Attacker could precompute a mapping of hashes onto passwords.
	- Avoid by using a random salt
#### Hash Cracking Algorithm
```python
# Loop through file data
for user in stolen_file:
	pass_hash = stolen_file.hash
	pass_salt = stolen_file.salt
	
	# Generate combinations of the current word
	for word in word_list:
		variations = generate_variations(word)
		
		# Create a hash for each word using the combination and salt
		for variation in variations:
			tmp_hash = hash(word+pass_salt)	
			
			# If the hashes are the same, that word is the password.
			if tmp_hash == pass_hash:
				return word  # the password
```
### Physical Tokens & 2FA
- Passwords are cheap and convenient but weak as authentication devices
- A simple way to improve this is to combine the password with a physical 2FA token
### Biometrics
- Most convenient
- Expensive to implement
#### The Revocation Problem
If a db of biometric credentials were compromised, what could we do?
The solution is to combine a revocable factor with this biometric factor, hash and store it.
This will allow you to create a new hash by modifying the revocable factor. Like a composite key.
# 08. DNS, ARP & Application Protocols
## Address Resolution
- **DNS** resolves human-readable **domain naimes** into numeric IP addresses for routing
- **ARP** (Address Resolution Protocol) resolves IP addresses into 48-bit **MAC Addresses** that identify specific network interfaces of hardware.
- Both processes are succeptible to spoofing attacks 
### DNS Resolvers
DNS resolvers ==provide clients with the IP address== that is associated with a *domain name*, translating human-readable website addresses like www.cloudflare.com into machine-readable IP addresses. When a user attempts to navigate to a website, their operating system sends a request to a DNS resolver. The DNS resolver responds with the IP address, and the web browser takes this address and loads the website.
##### DNS Uncached Response
![[dns-uncached.png]]
#### DNS Caching
A DNS resolver will ==save responses to IP address queries== for the duration of the designated *time to live (TTL)* associated with that IP address. In this way, the resolver can respond to future queries much more quickly, without needing to communicate with the many servers involved in the typical DNS resolution process.
##### DNS Cached Response
![[dns-cached.png]]
### DNS Cache Poisoning (Spoofing)
The process of fooling DNS software into giving an **attacker-selected address** as the IP address for a given domain (DNS).

This is done by the attacker impersonating DNS Nameserver, making a request to the DNS resolver and then ==forging the reply after the DNS Resolver queries that nameserver==. The forged IP address is then cached.

This forgery is possible becaue DNS Servers use UDP Instead of the reliable TCP and because there is no verifcation of DNS information.

With UDP, there is no guarantee that a connection is open or that the recipient is ready to receive, so ==an attacker can send a message via UDP and pretend it is a response from a legitimate server by forging the header data. ==

The DNS Server has no way of verifying this information so it accepts it and caches the data.
This is difficult. Attackers must intercept and send the forged reply before the real one arrives.

Attackers must also know the following:
- Which DNS queries are not cached so that there is time for the attacker to forge.
- The port used by the DNS Resolver
- The 16-bit request/transaction ID number
- The destination authoritative nameserver
#### Defence
DNS mapps domain names to IP addresses using UDP protocols with no verifcation checks. *DNSSEC (DNS Security Extensions)* adds a critical security layer by using digital signatures to authenticate data, preventing attacks like DNS spoofing or cache poisoning by ensuring responses come from the correct source and haven't been tampered with. 
### DNS Hijacking
If attackers gain physical access or hack a DNS Resolver, they can affects responses via the registrar.
### ARP 
Address Resolution Protocol (ARP) is a crucial network protocol that maps dynamic IP addresses (Layer 3) to fixed MAC addresses (Layer 2) on a local network (LAN), allowing devices to find each other's physical hardware address for data transmission. 

When a device needs to send data, it broadcasts an ARP request asking, "Who has this IP address?", and the device with that IP responds with its MAC address, which is then stored in an ARP cache (ARP table) for future use, bridging the logical IP world with physical Ethernet communication.
### ARP Poisoning (Spoofing)
Its the process of fooling ARP software into giving an **attacker-selected address** as the MAC address for a given IP address (ARP).
**ARP spoofing** occurs on a local area network (LAN). 

For example, Host A on a computer network wants to connect its IP address to the MAC address of Host B. Therefore, it sends an ARP request to all the other hosts on the LAN. Following this request, it receives an ARP response from Host B, with its MAC address and caches it.

ARP spoofing refers to an attacker with access to the LAN pretending to be Host B. The attacker sends messages to Host A with the goal of tricking Host A into saving the attacker’s address as Host B’s address. Host A will ultimately send communications intended for Host B to the attacker instead. Once the attacker becomes this middle man, each time Host A communicates with Host B, that host will in fact be communicating first with the attacker. Host B will typically be the default gateway, or the router.
## Remote Access
Before SSH, telnet and rlogin were used to access your machine remotely.
Here, all traffic was unencrypted. These 'r' tools recognise trusted hosts, avoiding the need for a password but these hosts can be spoofed.
### Banner Grabbing Attack
A technique used to gain information about a computer system on a network and the services running on its open ports.
==Common services on those ports== will sometimes respond with **banners**, leaking ==information about software vendors, versions, etc==. This is a threat of information disclousre.
If you have the version number, you can identify if there are any vulnerabilities.
### Sniffing a telnet session
To use telnet, you must send credentials unencrypted, this can be seen if you listen on the network:
```bash
dsniff -m

# Output
john
Leeds123
ls -l
exit```
ARP spoofing can be used to intercept these credentials.
#### Fix: Secure SHell (SSH)
Nowadays we use SSH to cryptographically login and transfer files remotely as well a secure connection tunelling.
Traffic is encrypted with a symmetric cypher, negotiated between client and server.
Public kye cryptography is used for client authentication and symmetric key exchange.
### Fix: Secure SHell (SSH)
Every host has its own SSH Key Pair.
When a client connects, the server provides its public key. This allows the client to confirm the server's identity by checking against a local db of stored public keys or by verifying a cert.
This public key is used to encrypt and authenticate data to be sent to the server.

In 2008, there was CBC mode vulnerability whereby an attacker could recover 14 bits of plaintext from arbitrary blocks of ciphertext with a probability of $2^{14}$. This was later fixed by using CTR mode not CBC.
#### Man In The Middle Attacks
Attackers intercept data by impersonating servers.
To mitigate this, maintain a local store of trusted public keys for previously-accessed servers.
Users are asked whether they trust a server on their first connect and this will be added to the list.
#### Sending emails: SMTP
Emails are a common vector for malware.
- A simple request protocol, using port 25.
- No authentication or unencryption.
- Subjects to spam, information disclosure, spoofing, etc.
#### Receiving emails
**Pop3**
- Credentials sent unencrypted. 
	- APOP ext.: server sends a timestamp and client sends an MD5 hash of this + shared secret.
- Unprotected mail.
**IMAP**
- Similar auth to POP3
- Also unprotected mail
#### SSH mitigation
SSH provides tunnelling to insecure protocols over an encrypted channel.
Nowadays, all protocols have TLS support.
##### `ssh -f -N -L 5678:localhost:110 mailhost`
![[tunnelling-ex.png]]
#### Better Approaches
Alternative ports for TLS-based services:
- SMTPS on port 587 not 25
- POP3S on post 995 not 110
- IMAPS on port 993 not 143

Consider using standard ports but add a STARTTLS to the protocols to trigger the secure handshake and connection.
### STRIPTLS Attack
MITM attack where a transparent proxy intercepts SMTP connections and removes the STARTTLS command in the header, resulting in the unencryptic communication channel being used.
# 09. Execution Monitoring (EM)
## OS-Based Software Security
A secure system satisfies security policies. These policies define execution that is deemed unacceptable for certain reasons; they are rules that must be satisfied.
The policy has middleware that enforces these security mechanisms.
#### Example
Policy: No message send after file read.
Objective: Execute the program but abort if the policy is violated.
This requires us to monitor program execution and abort at any time.

The takeaway is that ==all security policies must be *enforceable*==, like in the example above.
### Which policies are enforceable?
Can all security policies be enforced through EM?
How much does the EM mechanism know about the system and execution?
The EM can only access past events, it cannot predict/learn or attempt new -or even alternate- executions. This means that a compiler is not classified as an EM.
## Characterising EM Mechanisms
Let **T** be the target system to be monitored. 
Let **P** be a security policy to be enforced.
Let **M** be an EM mechanism (e.g. reference monitor, security kernel) to enforce the policy.

Our goal is such that: $M$ aborts $T$ if $T$ will violate $P$

As your program executes, it transitions through states after each execution or computation. 
We can think of the target system as a sequence generator comprising sequences of atomic actions, states and state/action pairs.

Let **E** be the *property of specification*, denoting a *set* of all possible (*finite and infinite*) target system execution sequences.
So executions of the target system are a proper subset of all executions:$$ E(T) \subseteq E $$
### Example 1: Mutual Exclusion of Semaphore Locks in Memory
Every specification/property is the intersection of a safety property (bad thing shouldn't happen) and a liveness (good thing must happen) property.
**Safety property**: At most one process can access the critical section.
**Liveness property**: Every requesting process is eventually granted access to the critical section.

#### Representing the states
Let r = 0,1 (not requesting, requesting)
Let c = 0,1 (not accessing, accessing)
We must satisfy that, at most, one process can access the critical section at one time.

*State Space*
<(0,0),(0,0)> -> <(0,0),(0,**1**)>, <(**1**,0),(0,0)> 
<(0,0),(0,**1**)> -> <(**1**,0),(**1**,0)>
<(**1**,0),(0,0)> -> <(**1**,0),(**1**,0)>, <(0,**1**),(0,0)> 
<(**1**,0),(**1**,0)>  -> <(0,**1**),(0,**1**)>
<(0,**1**),(0,0)>  -> <(0,**1**),(0,**1**)>

*Target System Space*
<(0,0),(0,0)> -> <(0,0),(0,**1**)>, <(**1**,0),(0,0)> 
<(0,0),(0,**1**)> -> <(**1**,0),(**1**,0)>
<(**1**,0),(0,0)> -> <(**1**,0),(**1**,0)>, <(0,**1**),(0,0)> 
~~<(**1**,0),(**1**,0)>  -> <(0,**1**),(0,**1**)>~~
~~<(0,**1**),(0,0)>  -> <(0,**1**),(0,**1**)>~~
We disable these transitions to <(0,**1**),(0,**1**)> because they don't satisfy the safety property.
### Example 2: Simple Running Program
**Safety property**: The program output is related to the input.
**Liveness property**: Successful termination.
### Defining Enforceable Policies
In this context, a security policy **P**, behaves as a predicate on the set of executions **E**.
The target system **T** satisfies this policy if and only if:$$P(\ E(T)\ ) = True$$
Meaning $P$ rules out target executions that are deem unacceptable.
==Not every security policy is a property==.

The predication on the set of executions will return true, if and only if, for every single execution in that set, your predicate applied to that execution satisfies the policy, then P is a property because the policy is true for all of the target system's set of executions.

1. Define a policy which is  a predicate on the set of executions.
2. Apply this policy to every execution in the set.
3. If all executions satisfy the policy, then the policy is cemented as a property of the target system.

Let **T** be the target system to be monitored. 
Let **E** be the *property of specification*, denoting a *set* of all possible (*finite and infinite*) target system execution sequences.
Let **e** be a single system execution in *E*.
Let **P** be a security policy to be enforced, acting as a predicate over *E*.
Let **P'** be a security policy to be enforced, acting as a predicate over *e*.
A policy is enforceable if:  
$$ \begin{align}
P(\ E(T)\ ) = \forall{e} \in E(T):P'(e) \\ 
\text{since } P'(e) \implies e \in P, P \text{ is a property}
\end{align} $$

To conceptualise this, think of different policies and decide which is a property. 
1. Amazon customer service response times are under 30 seconds.
This is likely not a property as they must always meet this requirement and its not entirely within their control.
2. The US will seek out oil where possible.
Arguably a property because it is a recurring theme and proven.
### Takeaway
Policies are ==properties that can be enforced via monitoring==.
# 10. Example Safety
Say we have the following scenario:
A system can execute the following operations on a file, depicted as states: 
```
s0: file_close  s1: file_open  s2: file_read 
s3: file_write  s4: file_sent  sn: file_delete
```
In our specification (SPEC), we have the policy: **No file send after file read**.
![[example-safety-em-state-machine.png]]
All states are such that: `s0, s1, s2, s3, s4, sn`  $\in E$.
Legal states: `s0, s1, s2, s3, sn` $\subseteq E$.
Illegal states: `s4` $\subset E$.

Violation of safety happens when an execution includes an illegal state, e.g., `<s1 … s4>`.
• Monitoring means watching the execution starting from `s0` and going through the states.
• Abort the execution to satisfy safety e.g., if the program reaches `s4` from `s3`, abort in `s3`.
# 11. Firewalls
## Definition
A firewall is a security guard placed at the point of entry between a private network and the internet. It monitors all incoming and outgoing packets by abiding by rules (policies). The order of these rules is important.

- If you have N rules, you have $!N$ combinations -a complexity of exponential order ($O(!N)$).  
- A firewall behaves like a system execution monitor except it cannot terminate your application or executions, it only controls data flow.
### Example

| Rule # |     Source     |   Destination   | Protocol | Port |     Action     |
| :----: | :------------: | :-------------: | :------: | :--: | :------------: |
|   1    |  `11.22.1.*`   |    `*.*.*.*`    |  *tcp*   | *80* |   **permit**   |
|   2    |  `11.22.2.*`   |    `*.*.*.*`    |  *tcp*   | *80* |   **permit**   |
|   3    | `11.22.3.0/16` |    `*.*.*.*`    |  *udp*   | `*`  |    **deny**    |
|   4    |   `*.*.*.*`    | `11.22.33.16/8` |  *tcp*   | *80* | **quarantine** |
#### Challenges
- Rules may conflict or be inconsistent, meaning they may be satisfied but with different outputs.
- Rules may be incomplete, making it difficult to ensure all packets are considered.
- Rules may be redundant, unnecessarily increasing complexity. 
## Model and Notation
- Treat each packet as an n-tuple, $\braket{d_1 ... d_n}$, of data.
- Field $F_i$: is a variable with a non-negative integer domain, denoted by $D(F_i)$, e.g., source IP address $[0, 2^{32} - 1]$ . E.g. the flags of the packets.
- Each data item $d_i$ in a packet is such that $d_i \in D(F_i)$.
- Let $\Sigma$ be the set of all packets over fields $F_1 ... F_n$.
- Our rule is a $\braket{\text{predicate}} { \rightarrow } \braket{\text{decision}}$.
- The predicate is a boolean expression over $d_1 ... d_n$, decision $\in \{a,d\}$.
### Mail Server Firewall Example
![[firewall-rule-ex.png]]

| Name | Meaning                                                                  |
| :--: | ------------------------------------------------------------------------ |
|  I   | **I**nterface for gateway router, 0 for internet, 1 for private network. |
|  S   | **S**ource IP Address                                                    |
|  D   | **D**estination IP Address                                               |
|  N   | Destination Port **N**umber                                              |
|  P   | **P**rotocol Type                                                        |
##### Initial Rules
$r_1: (I=0) \land (S=\text{any}) \land (D=\text{Mail Server}) \land (N=25) \land (P=\text{TCP}) \rightarrow \fcolorbox{green}{black}{accept}$.
 > Allow incoming SMTP packets from any internet host to proceed to the mail server.

$r_2: (I=0) \land (S=\text{Malicious Hosts}) \land (D=\text{any}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{red}{black}{discard}$.
 > Discard packets from known malicious internet hosts regardless of destination, port or protocol.

$r_3: (I=1) \land (S=\text{any}) \land (D=\text{any}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{green}{black}{accept}$.
 > Allow all outgoing packets from private network.

$r_4: (I=any) \land (S=\text{any}) \land (D=\text{any}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{green}{black}{accept}$.
 > *Default rule*: After previous checks, allow any incoming or outgoing packets to proceed.
#### Problems
1. **Consistency** problem
Rules $r_1$ and $r_2$ conflict since the SMTP packets from previously known malicious hosts to the mail server match both rules but their decisions differ. 
*Solution*: These rules should be swapped.

2. **Completeness** problem
To ensure that every packet has at least one matching rule in a firewall, the common practice is to make the predicate of the last rule a tautology. This is clearly not a good way to ensure the thorough consideration of all possible packets.

Due to $r_4$, *non-email packets from the outside to the mail server* and *email packets from the outside to the non-mail server hosts* are accepted by the firewall. However, these two types of traffic should be blocked as a mail server is usually dedicated to email service only.

*Solution*: Create two new rules to handle these two types of traffic, placing them after rule 1.
$r_a: (I=0) \land (S=\text{any}) \land (D=\text{Mail Server}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{red}{black}{discard}$.
 > Discard non-email packets from the internet host to the mail server. 

$r_b: (I=0) \land (S=\text{any}) \land (D=\text{any}) \land (N=25) \land (P=\text{TCP}) \rightarrow \fcolorbox{red}{black}{discard}$.
 > Discard email packets from non-mail server internet hosts.

3. **Compactness** problem
Poorly designed firewalls often have redundant rules. A rule in a firewall is redundant *iff* removing the rule does not change the function of the firewall, i.e., does not change the decision of the firewall for every packet. 

In the initial rules, $r_3$ is redundant because all the packets that match $r_3$ but not $r_1$ and $r_2$ also match $r_4$ , and both $r_3$ and  $r_4$ have the same decision. 
*Solution*: Remove rule 3.
#### Fixed Rules
$r_1: (I=0) \land (S=\text{Malicious Hosts}) \land (D=\text{any}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{red}{black}{discard}$.
 > Discard packets from known malicious internet hosts regardless of destination, port or protocol.

$r_2: (I=0) \land (S=\text{any}) \land (D=\text{Mail Server}) \land (N=25) \land (P=\text{TCP}) \rightarrow \fcolorbox{green}{black}{accept}$.
 > Allow incoming SMTP packets from any internet host to proceed to the mail server.
 
$r_3: (I=0) \land (S=\text{any}) \land (D=\text{Mail Server}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{red}{black}{discard}$.
 > Discard non-email packets from the internet host to the mail server. 

$r_4: (I=0) \land (S=\text{any}) \land (D=\text{any}) \land (N=25) \land (P=\text{TCP}) \rightarrow \fcolorbox{red}{black}{discard}$.
 > Discard email packets from non-mail server internet hosts.

$r_5: (I=any) \land (S=\text{any}) \land (D=\text{any}) \land (N=any) \land (P=\text{any}) \rightarrow \fcolorbox{green}{black}{accept}$.
 > *Default rule*: After previous checks, allow any incoming or outgoing packets to proceed.
#### Model and Notation (continued)
- A firewall is a sequence of rules $R_1 ... R_m$.
- A packet *matches* a rule $R_i$ *iff* the packet satisfies the predicate of $R_i$.
- Two rules *overlap* if there is at least one packet that can match both rules.
- Two rules *conflict* *iff* they overlap and have different decisions.
- When two rules $R_i, R_j, i<j$ conflict, the take the decision of $R_i$.
- The last rule is called the *default rule* and is usually a **tautology**, i.e. the last predicate always evaluates to *true* to ensure all packets are considered.
## Firewall Decision Diagram (FDD)
- FDD is an *acyclic*, *directed* graph defined over $F_1 ... F_n$ (the set $\Sigma$ over all packet fields) with 5 properties.
- It has exactly one root node.
- Each non-terminal (not accepting/discarding) node specifies a test for that packet field.
	- Subsequently, each outgoing edge from these non-terminal nodes corresponds to values of that field.
- Each node $v$ in FDD is labelled with a field, denoted $F(v)$, such that: 
$$F(v) \in \begin{cases}  
   \; \{F_1 ... F_n\} & \ \text{if }v \text{ is non-terminal}\\
   \; \{\text{accept}, \text{discard}\} & \ \text{if }v \text{ is terminal}
\end{cases}$$Meaning if node $v$ is a terminal or leaf node, then it will be labelled as either accept or discard, otherwise it will be labelled with $F_1 ... F_n$.

- An edge $e$ is selected when the edge label contains the value of the packet field.
	- We label it with a non-empty set of integer, such that $e \in \{...\}: \ e_i \in \mathbb{Z}^+ \cap e_i \notin \emptyset$.
	- Denoted as $I(e)$, outgoing edge nodes of $v$ are a subset of this non-negative integer domain: $I(e) \subseteq D( F(v) )$.
- The set of all outgoing edges of node $v$ is denoted by $E(v)$ and must satisfy these conditions.
	- **Consistency**: For any two distinct edges $e, e' \in E(v), \; I(e) \cap I(e') = \emptyset$.
	All edges must be distinct such that there is no overlap.
	- **Completeness**: The union of all edges is equal to the non-negative integer domain of all nodes: $\cup_{e \in E(v)} I(e) = D( F(v) )$. E.g. The domain from F1 to F2 is [1-10] 

- A directed path from root to terminal nodes is called a *decision path*.
	- It is an alternation of vertex and edge pairs where no two nodes on that path share a label.
	- We represent a decision path by $\braket{v_{1}e_{1}...v_{k}e_{k+1}}$ where 
		- $v_1$ is the root, $v_{k+1}$ is a terminal node and 
		- edge $e_i$ is a directed arc from node $v_i...v_{i+1}$
	- $$S_i = \begin{cases}  
   \; I(e_j) & \ \text{if } \exists \text{ node } v_j \text{ in path with field } F_i \\
   \; D(F_i) & \ \text{otherwise}
\end{cases}$$
The FDD ==maps each packet to a decision== by ==testing the packet== down the decision tree.
1. Incoming packet is broken down into $n$ fields.
2. Inspect the first field, look at the value and apply it to the FDD, choose which branch to follow.
3. Repeat for each field until all are accepted/discarded.
### Example
![[fdd-example-firewalls.png]]
- Packet comes in, $F_1$ has value 9, $F_2$ has value 5.
- Third edge satisfies because $9 \in \{1,2,3,4,9,10\}$.
- First edge satisfies because $5 \in \{1,2,3,4,5\}$.
- Terminal node result is **discard**.
- $S_1$ will be associated with the interval of the edge that connects to $F_2$
	- $S_1 = [1,4] \cup [9,10] \implies \in \{1,2,3,4,9,10\}$
### Results: Rules
Each decision path uses firewall rules but designed for the FDD.
The ==number of rules = number of decision paths== in FDD.
#### Theorem of FDDs
For any FDD $f$, we have:
- $f$.accept $\cap \ f$.discard $=\emptyset$. **Consistency**: No overlapping results.
- $f$.accept $\cup \ f$.discard $=\Sigma$. **Completeness**: All results must be either accept or discard.
## Reducing The Number of Rules
It takes longer for the firewall program to arrive at a decision if it must complete many rules per packet. This FDD can be further reduced. To compress the graph we use the concept of *isomorphism* and *isomorphic nodes*.
### Isomorphic Nodes
Two nodes $v$ and $v'$ in an FDD are isomorphic *iff*:
- both are terminal nodes with identical labels
- both are non-terminal nodes and there is a 1-to-1 correspondence between the outgoing edges of $v$ and $v'$, such that every pair of corresponding edges have identical labels pointing to the same node.

**Terminal nodes**
- All the accept ==terminal nodes are isomorphic to each other== because they have the same label (F2).
- All the discard nodes are also isomorphic to each other because they also share F2.

**Label nodes**
- The left-most F2 and the middle F2 ==label nodes are isomorphic to each other== because
	- both labels branch to the same accept and discard terminal nodes
	- Each branch to those terminal nodes shares the same edge values.
	- This is not true for the left-most and right-most F2 label.
#### Reduced FDD Definition
An FDD is *reduced* *iff*:
- No node in FDD has only one outgoing edge (must terminate).
- No two nodes in FDD are isomorphic to each other.
- No two node have more than one edge between them
### Algorithm 1 for Reduced FDD
#### Steps
1. If there is a node $v$ with only 1 outgoing edge $e$ pointing to $v’$, remove both $v$ and $e$, and all edges that point to $v$ now point to $v’.$ Remove outgoing edge and node, point parent to child of removed node. 
2. Two isomorphic nodes $v$ and $v’$, remove $v’$ together with all its outgoing edges, and let all edges that point to $v’$ point to $v$.
3. Two edges $e$ and $e’$ that are both between the same pair of nodes, then remove $e’$ and change label of $e$ from $I(e)$ to $I(e) \cup I(e’)$. If one node points to the same node with 2 edges, combine the edges into one.

Applied rule 2:
![[rule2-reduce-fdd.png]]
![[rule2-reduce-fdd-2.png]]
Here you can see the isomorphic label node F2 and its isomorphic terminal nodes have been merged with the input to F2 from F1 being combined to [5,8].