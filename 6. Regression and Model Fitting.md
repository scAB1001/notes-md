What will you learn?
• Define what regression is
• Fit a model
• Fit a Polynomial Curve
• Define what overfitting is
• Prevent overfitting
• Learn about Decision Trees and overfitting
• Restrict Decision Tree Complexity
• Define what Random Forest is
• Use Decision Trees for Regression
Regression
• The goal of regression is to predict the value of one or more continuous target
variables t given the value of a D-dimensional vector x of input variables.
• Regression is similar to classification: you have a number of input features, and you
want to predict an output feature. In classification, this output feature is either
binary or categorical. With regression, it is a real‐valued number.
Typically, regression algorithms model the output as a linear combination of the
inputs.

• Given the following values of X and Y, what is the value of Y when X = 5.(1,1), (2,2),
(4,4), (100,100), (20, 20)
• The answer is : 5. Not very difficult, right?
• Now, let’s take a look at different example. Say you have the following pairs of X and
Y. Can you calculate the value of Y, when X = 5?
• (1,1), (2,4), (4,16), (100,10000), (20, 400)
• The answer is : 25. Was it difficult?

• Let’s understand a bit as to what happened in the above examples. When we look
at the first example, after looking at the given pairs, one can establish that the
relationship between X and Y is Y = X. Similarly, in the second example, the
relationship is Y = X\*X.
• In these two examples, we can determine the relationship between two given
variables (X and Y) because we can easily identify the relationship between them.
• Overall, machine learning works in the same way.
• Your computer looks at some examples and then tries to identify “the most suitable” relationship
between the sets X and Y. Using this identified relationship, it will try to predict (or more) for new
examples for which you don’t know Y.
• Detecting whether a region of an image is a face or not -> Classification
• Predicting the coordinates of the bounding box around the face -> Regression

The standard way to fit a line is called least squares. In Python, it can be fit
using the Linear Regression class in the example scripts, and the fit coefficients
can be found in the following way:
Least squares works by picking the values of m and b that minimize the “penalty
function,” which adds up an error term across all of the points:
$$ L=\sum_{\substack{i}}(y_i-(mx_i+b))^2 $$
The simplest example of regression is one that you probably saw in high school:
fitting a line to data. You have a collection of x/y pairs, and you try to fit a line
to them of the form: $y = mx + b$
Plot the points out, fit a line to them by eye, trace the line with a ruler, and use that to
pull out $m$ and $b$.
Each of the four datasets has the same line of best fit and the same quality of fit.

The key thing to understand here is that this penalty function makes least squares
regression extremely sensitive to outliers in the data: three deviations of size 5 will give
a penalty of 75, but just a single larger deviation of size 10 will give the larger penalty of
size 100. Linear regression will bend the parameters so as to avoid large deviations of
even a single point, which makes it unsuitable in situations where a handful of large
deviations are to be expected.
An alternative approach that is more suitable to data with outliers is to use the penalty
function: $$ L=\sum_{\substack{i}} \lvert y_i-(mx_i+b)\rvert$$
where we just take the absolute values of the different error terms and add
them. This is called “L1 regression,” among other names. Outliers will still have
an impact, but it is not as egregious as with least squares. On the other hand,
L1 regression penalizes small deviations from expectation more harshly compared
to least squares, and it is significantly more complicated to implement
computationally

Fitting Nonlinear Curves
Fitting a curve to data is a ubiquitous problem not just in data science but in
engineering and the sciences in general. Often, there are good a priori reasons
that we expect a certain functional form, and extracting the best‐fit parameters
will tell us something very meaningful about the system we are studying. A few
Examples:
Exponential decay to some baseline. This is useful for modeling many processes
where a system starts in some kind of agitated state and decays to a
baseline: $y=ae^{bx}$

Logistic growth, which is useful in biology for modeling the population density
of organisms growing in a constrained environment that can only support
so many individuals: $y=a{e^{bx}\over c+e^{bx}}$

• Example: Polynomial Curve Fitting
• Suppose we observe a real-valued input variable x and we wish to use this
observation to predict the value of a real-valued target variable t.
• -> example using synthetically generated data
• generated from the function sin(2πx) with random noise included in the target
values

Regression
• suppose that we are given a training set comprising $N$ observations of $x$, written 
$x ≡ (x_1, . . . , x_N)^T$, together with corresponding observations of the values of t, denoted 
$t≡ (t_1, . . . , t_N)^T$.
• Note: $≡$ identical to
• a training set comprising $N = 10$ data points
• xn, for n = 1, . . . , N, spaced uniformly in range [0, 1]
• t was obtained by first computing the corresponding
values of the function sin(2πx) and then adding a small
level of random noise having a Gaussian distribution

• Our goal is to exploit this training set in order to make predictions of the value t̂
• of the target variable for some new value ̂ x of the input variable
• → discover the underlying function sin(2πx)
• → not easy task: we have to generalize from a finite
dataset

$y(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M= \sum_{\substack{j=0}}w_jx^j$
• curve fitting
• fit the data using a polynomial function of the form where M is the order of the
polynomial, and xj denotes x raised to the power of j.
• The polynomial coefficients w0, . . . , wM are collectively denoted by the vector w.
• Note that, although the polynomial function y(x,w) is a nonlinear function of x, it is
a linear function of the coefficients w
→ Functions, such as the polynomial, which are linear in the unknown parameters
have important properties and are called linear models
